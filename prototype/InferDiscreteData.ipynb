{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decide whether a sample is from a discrete or continuous distribution\n",
    "\n",
    "The code below computes the Normalized Nearest Neighbor Distribution (NNN) and then uses the Wasserstein distance to quantify how far from uniform it lives. Discrete distributions have an NNN that is much farther from uniform than continuous distributions.\n",
    "\n",
    "It also examines cases where there are multiple repeated values, and tries to decide whether they represent legitimate sampling, or whether they result from some non-sampling-like process like for example clipping, using a numerical value as a placeholder for missing data, or from e.g. attaching per-image metadata to multiple targets iwthin an image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numbers\n",
    "\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "from scipy.stats import wasserstein_distance as emd\n",
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# If the Wasserstein distance to a uniform distribution exceeds this, sample is discrete.\n",
    "DISCRETE_MIN_EMD = 0.054\n",
    "\n",
    "# samples smaller than this always look discrete, no need to test\n",
    "CONTINUOUS_MIN_SAMPLE_SIZE = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_discrete(X: NDArray, verbose=False) -> list[bool]:\n",
    "    \"\"\"Test for discreteness of a 1D sample.\n",
    "    Imagine 1D data representing a sample of a continuous distribution, e.g. event times for emissions\n",
    "    from a radioactive source. Think about the intervals between consecutive events; they have an exponential\n",
    "    distribution. The most likely interval is zero, and longer intervals are exponentially less likely;\n",
    "    the average interval is the reciprocal of the decay rate. This stands in stark contrast with the tick\n",
    "    times of a clock; the distribution of intervals between clock ticks is extremely sharply peaked; the\n",
    "    average and most likely intervals are in fact the same. Radioactive decay times and clock ticks\n",
    "    illustrate the fundamental distinction between continuous and discrete distributions.\n",
    "\n",
    "    Of course, any 1D sample can be sorted in the way that times naturally are, and so we can think\n",
    "    about the intervals between adjacent points. For a continuous distribution, a point is equally likely\n",
    "    to lie anywhere in the interval bounded by its two neighbors. Furthermore, we can put all \"between-\n",
    "    neighbor\" locations on the same scale of 0 to 1 by subtracting the smaller neighbor and dividing out\n",
    "    the length of the interval. (Duplicates are either assigned to zero or ignored, depending on context).\n",
    "    These normalized locations will be much more uniformly distributed for continuous data than for discrete,\n",
    "    and this gives us a way to distinguish them. Call this the Normalized Near Neighbor distribution (NNN),\n",
    "    defined on the interval [0,1].\n",
    "\n",
    "    The Wasserstein distance is available in scipy.stats.wasserstein_distance. We can use it to measure how close\n",
    "    the NNN is to a uniform distribution over [0,1]. We found that as long as a sample has at least 20 points, and\n",
    "    furthermore at least half as many points as there are discrete values, we can reliably distinguish\n",
    "    discrete from continuous samples by testing that the Wasserstein distance is greater or less than 0.054,\n",
    "    respectively.\n",
    "\n",
    "    Example:\n",
    "\n",
    "    >>> rng = np.random.default_rng(314159265)\n",
    "    >>> Xnorm = rng.normal(100, scale=10, size=50)\n",
    "    >>> print(f'Normal sample is discrete: {infer_discrete(Xnorm)[0]}')\n",
    "    >>> Xfish = rng.poisson(100, size=50)\n",
    "    >>> print(f'Poisson sample is discrete: {infer_discrete(Xfish)[0]}')\n",
    "    >>> ks = ks_2samp(Xnorm, Xfish)\n",
    "    >>> print(f'KS can distinguish Normal from Poisson: {ks.pvalue < 0.05}')\n",
    "    Normal sample is discrete: False\n",
    "    Poisson sample is discrete: True\n",
    "    KS can distinguish Normal from Poisson: False\n",
    "    \"\"\"\n",
    "\n",
    "    if X.ndim == 1:\n",
    "        X = np.expand_dims(X, axis=1)\n",
    "    n_examples, n_features = X.shape\n",
    "\n",
    "    if n_examples < CONTINUOUS_MIN_SAMPLE_SIZE:\n",
    "        print(\n",
    "            f\"All samples look discrete with so few data points (< {CONTINUOUS_MIN_SAMPLE_SIZE})\"\n",
    "        )\n",
    "        return [True] * n_features\n",
    "\n",
    "    shift = np.full(\n",
    "        n_features, DISCRETE_MIN_EMD + 1\n",
    "    )  # A shift of *more* than DISCRETE_MIN_EMD indicates discrete; so skipped features will be discrete\n",
    "    looks_like_noisy_discrete = np.full(n_features, False) # re-checked when there are enough repeated values, may then become True. \n",
    "    for i in range(n_features):\n",
    "        # Skip non-numerical features\n",
    "        if not all(\n",
    "            isinstance(xi, numbers.Number) for xi in X[:, i]\n",
    "        ):  # NB: np.nan *is* a number in this context.\n",
    "            continue\n",
    "\n",
    "        # Require at least 3 unique values before bothering with NNN\n",
    "        xu, nu = np.unique(X[:, i], return_counts=True, axis=None)\n",
    "        if (\n",
    "            len(xu) < 3\n",
    "        ):  # Fewer than 3 unique values should definitely be called discrete.\n",
    "            continue  # skip to next feature\n",
    "\n",
    "        Xs = np.sort(X[:, i])\n",
    "\n",
    "        # Xs may contain repeated values. If it does, these may indicate things we wish to exclude.\n",
    "        # If repeats are nothing but extremes, exclude them (clipping values).\n",
    "        repeats = xu[nu > 1]\n",
    "        if len(repeats) > 1:\n",
    "            just_clipping = all(\n",
    "                np.logical_or(np.isclose(repeats, xu[0]), np.isclose(repeats, xu[-1]))\n",
    "            )\n",
    "            if just_clipping:\n",
    "                Xs = xu[1:-1]  # Exclude the clipping values.\n",
    "        else:  # Either no repeats, or just one value is repeated, likely a \"missing data\" indicator. So exclude the repeats.\n",
    "            Xs = xu\n",
    "\n",
    "        # If there are 3 or more distinct repeated values, we need to check whether non-repeats are in fact just very near\n",
    "        #    to the repeats, but not exactly equal, due to noise. To do this, we need to bin the unique values according to\n",
    "        #    bin boundaries put midway between repeated values, then form a normalized sample of locations within each bin. Repeated\n",
    "        #    values all lie at 0.5 in such a sample, while unique values are distributed between 0 and 1.  We then compare the emd from this\n",
    "        #    normalized distribution to two hypothetical distributions: (1) all values lie at 0.5, indicating a discrete distribution,\n",
    "        #    and (2) values are uniform, indicating a continuous distribution. We choose the possibility with the smaller emd.\n",
    "        #\n",
    "        # In a case where there are many repeated values, yet the unique values appear more like a continuous sample than they appear like \n",
    "        #    noise-perturbed versions of the repeats, we conclude that the repeats are actual copies, e.g. they may be per-image\n",
    "        #    metadata that were copied to multiple objects detected in the same image. infer_discrete() is agnostic, though, about how\n",
    "        #    a sample comes to have many repeated values and yet also unique values that appear to be drawn from a continuous distribution; we are \n",
    "        #    merely asserting that data that should be treated as discrete will never have these properties.\n",
    "        #\n",
    "        if len(non_repeats:=xu[nu==1]) > 0 and n_examples - len(xu) >= 3:\n",
    "            bin_centers = copy.deepcopy(repeats)\n",
    "\n",
    "            # Most edges lie halfway between adjacent centers.\n",
    "            bin_edges = (bin_centers[1:] + bin_centers[0:-1]) / 2\n",
    "\n",
    "            # Find average point spacing and use it to position outer bin boundaries. \n",
    "            dx_avg = (xu[-1] - xu[0])/len(xu)\n",
    "            b0, b1 = xu[0] - dx_avg, xu[-1] + dx_avg\n",
    "            # Put it all together \n",
    "            bin_edges = np.concatenate((b0.reshape(1), bin_edges, b1.reshape(1)))\n",
    "\n",
    "            # generate normalized repeat values, since not all bins necessarily have same width.\n",
    "            itable = np.linspace(0, len(bin_edges) - 1, len(bin_edges))\n",
    "            rindex = np.interp(bin_centers, bin_edges, itable)\n",
    "            xindex = np.interp(non_repeats, bin_edges, itable)\n",
    "\n",
    "            rfrac, _ = np.modf(rindex) # where does rindex lie relative to bin boundaries? Middle is rfrac = 0.5\n",
    "            xint = np.floor(xindex).astype(np.intp)\n",
    "            xfrac = xindex - xint\n",
    "            rfracx = rfrac[xint]  # What is normalized x location, relative to the repeat in its bin? \n",
    "\n",
    "            denom = np.zeros_like(xindex)\n",
    "            le, gt = xfrac <= rfracx, xfrac > rfracx # check which side of its repeat each x is on. \n",
    "\n",
    "            # when rfracx = 0.5, we want a denom of 1. \n",
    "            denom[le], denom[gt] = 2*rfracx[le], 2*(1 - rfracx[gt])\n",
    "\n",
    "            xnorm = (xfrac - rfracx)/denom + 0.5\n",
    "            dist_to_uni = emd(xnorm, np.linspace(0, 1, len(xnorm)))\n",
    "            dist_to_discrete = emd(xnorm, np.zeros_like(xnorm) + 0.5)\n",
    "\n",
    "            looks_like_noisy_discrete[i] = dist_to_discrete < dist_to_uni\n",
    "            if verbose:\n",
    "                print(f'looks_like_noisy_discrete: {looks_like_noisy_discrete}')\n",
    "                print(f'\\t distance to uniform: {dist_to_uni:.3f}')\n",
    "                print(f'\\t distance to discrete:{dist_to_discrete:.3f}')\n",
    "\n",
    "        Xs = xu # NNN really only makes sense with unique values\n",
    "\n",
    "        X0, X1 = Xs[0:-2], Xs[2:]  # left and right neighbors\n",
    "\n",
    "        dx = np.zeros(len(Xs) - 2)  # no dx at end points, so len minus 2\n",
    "        gtz = (X1 - X0) > 0  # check for repeats\n",
    "        dx[np.logical_not(gtz)] = 0.0 #  set dx to zero for repeats\n",
    "\n",
    "        # Finally, the core idea: dx is NNN samples.\n",
    "        dx[gtz] = (Xs[1:-1] - X0)[gtz] / (X1 - X0)[gtz]  \n",
    "\n",
    "        # how far is dx from uniform, for this feature?\n",
    "        shift[i] = emd(\n",
    "            dx, np.linspace(0, 1, len(dx))\n",
    "        ) \n",
    "\n",
    "    looks_discrete = np.logical_or(shift > DISCRETE_MIN_EMD, looks_like_noisy_discrete)\n",
    "    return list(looks_discrete)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rng = np.random.default_rng(314159265)\n",
    "\n",
    "Xrep = rng.poisson(5, size=100)+ np.concatenate((rng.normal(scale=.11, size=50), np.zeros(50))) # 11% noise is a lot!  \n",
    "infer_discrete(Xrep, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(314159265)\n",
    "npts = 800\n",
    "loc = 100\n",
    "Xnorm = rng.normal(loc, scale=np.sqrt(loc), size=npts)\n",
    "print(f'Normal sample is discrete: {infer_discrete(Xnorm, verbose=True)[0]}')\n",
    "\n",
    "Xfish = rng.poisson(loc, size=npts)\n",
    "print(f'Poisson sample is discrete: {infer_discrete(Xfish, verbose=True)[0]}')\n",
    "\n",
    "ks = ks_2samp(Xnorm, Xfish)\n",
    "print(f'KS can distinguish Normal from Poisson: {ks.pvalue < 0.05}') # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist((Xnorm, Xfish));\n",
    "plt.legend(['normal: continuous', 'Poisson: discrete'])\n",
    "plt.title(f'Similar distributions (p = {ks.pvalue :.3f})');# type: ignore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the hist function bins both quantities and plots them as discrete. But the normal values are nevertheless from a continuous distribution.\n",
    "\n",
    "The point is, the NNN and Wasserstein distance together provide a way to reasonably infer that a feature should be handled as discrete, for functions that want to know.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's becoming apparent that we should deal with repeated values and non-repeated values separately. A sample with enough repeated values will always look discrete, even in cases where the non-repeated values are from a process that generates a continuous distribution, and the repeats are generated by something else entirely. For example, repeats can be causes by clipping, by using a numerical value for missing data, or by dealing the same value out to multiple examples, e.g. per image metadata for an image with multiple examples detected.\n",
    "\n",
    "The next set of tests generate discrete samples without repeated values, and test the sensitivity of infer_discrete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_d = np.random.permutation(2000) # no repeats\n",
    "uni = np.random.uniform(-0.5, 0.5, size = len(uni_d))\n",
    "print(len(np.unique(uni_d))/len(uni))\n",
    "\n",
    "print(infer_discrete(uni_d))\n",
    "print(infer_discrete(uni))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_value = 2000\n",
    "uni_d = np.random.permutation(top_value) # no repeats\n",
    "# grab_some = np.random.uniform(size=len(uni_d)) > 0.7 # too sloppy\n",
    "frac = 0.5\n",
    "grab_some =  int(frac*top_value)\n",
    "uni_d = uni_d[0:grab_some] # still random, but this way I know how many\n",
    "\n",
    "noise = 0.5\n",
    "uni = np.random.uniform(0, top_value, size = len(uni_d))\n",
    "\n",
    "print(f'grabbed {len(uni_d)} values out of {top_value}')\n",
    "print(f'What fraction are unique? {len(np.unique(uni_d))/len(uni_d)}')\n",
    "\n",
    "print(f'uni_d looks_discrete: {infer_discrete(uni_d)}')\n",
    "print(f'uni looks discrete: {infer_discrete(uni)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_value = 2000 # will pick a permutation of integers from 0 to top_value\n",
    "fracs = np.linspace(0.1, 0.35, 50)  # and keep this many of them to pass to infer_discrete(). \n",
    "\n",
    "ntry = 1000  # try it this many times to compute how accurate it is. \n",
    "c_acc = np.zeros(len(fracs))\n",
    "d_acc = np.zeros(len(fracs))\n",
    "# pval = np.zeros(ntry)\n",
    "\n",
    "for ifrac, frac in enumerate(fracs):\n",
    "    grab_some =  int(frac*top_value)\n",
    "\n",
    "    udd = np.full(ntry, False)\n",
    "    ud = np.full(ntry, False)\n",
    "\n",
    "    for i in range(ntry):\n",
    "        uni_d = np.random.permutation(top_value) # no repeats\n",
    "        uni_d = uni_d[0:grab_some] # still random, but this way I know how many\n",
    "\n",
    "        uni = np.random.uniform(0, top_value, size = len(uni_d))\n",
    "\n",
    "        udd[i] = 1 if infer_discrete(uni_d)[0] else 0# returns a list of length 1. \n",
    "        ud[i] = 0 if infer_discrete(uni)[0] else 1\n",
    "        # pval[i] = ks_2samp(uni, uni_d).pvalue\n",
    "\n",
    "    d_acc[ifrac] = np.sum(udd).astype(np.float16)\n",
    "    c_acc[ifrac] = np.sum(ud).astype(np.float16)\n",
    "\n",
    "d_acc /= ntry\n",
    "c_acc /= ntry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fracs, c_acc, fracs, d_acc)\n",
    "plt.title('no repeats in sample: acc vs frac')\n",
    "plt.xlabel('fraction of possible discrete values actually present in sample')\n",
    "plt.ylabel('fraction of correct calls')\n",
    "plt.legend(['continuous', 'discrete'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are the tests that Ryan suggested, which got me thinking about repeats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 4000\n",
    "objects = np.random.choice(2, 4000, p=[0.75,0.25])+1\n",
    "metadata = np.random.rand(4000) * 50\n",
    "\n",
    "# make a lot of metadata repeats. Should they be discrete? I think not! \n",
    "check = np.concatenate([np.repeat(metadata[i], objects[i]) for i in range(metadata.size)])\n",
    "num_dups = check.size - np.unique(check).size\n",
    "print(num_dups)\n",
    "\n",
    "print(f'repeated metadata are discrete: {infer_discrete(check, verbose=True)}')\n",
    "print(f'metadata are discrete: {infer_discrete(metadata, verbose=True)}')\n",
    "print(f'rounding metadata makes them discrete: {infer_discrete(np.round(metadata), verbose=True)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntests = 50\n",
    "num_samples = [2000,4000,10000]\n",
    "sample_number = np.array([[i]*ntests for i in num_samples]).flatten()[:,np.newaxis]\n",
    "results = np.zeros((len(num_samples)*ntests,5))\n",
    "for i,samples in enumerate(num_samples):\n",
    "    for j in range(ntests):\n",
    "        percent = (np.random.choice(4)+94)/100\n",
    "        leftover = (1-percent)/3\n",
    "        objects = np.random.choice(5, samples, p=[percent,0.0,leftover,leftover,leftover])+1\n",
    "        num_img_with_objects = np.nonzero(objects>1)[0].size\n",
    "        if (j+1) % 2 == 0:\n",
    "            metadata = np.random.rand(int(samples*(100-(j+1))/100)) * 100\n",
    "            icopy = np.random.choice(metadata.size, int(samples*(j+1)/100))\n",
    "            metadata = np.concatenate([metadata, metadata[icopy]])\n",
    "        else:\n",
    "            metadata = np.random.rand(samples) * 100\n",
    "\n",
    "        check = np.concatenate([np.repeat(metadata[i], objects[i]) for i in range(metadata.size)])\n",
    "        unique = np.unique(check).size\n",
    "        num_dups = check.size - unique\n",
    "\n",
    "        obj_discrete = infer_discrete(check)\n",
    "        meta_discrete = infer_discrete(metadata)\n",
    "\n",
    "        results[int(i*ntests+j),:] = [num_img_with_objects/samples*100, num_dups, unique/samples*100, obj_discrete[0], meta_discrete[0]]\n",
    "\n",
    "data = np.hstack((sample_number,results))\n",
    "df = pd.DataFrame(data, columns=['Sample Number','Object Percentage', 'Total Duplicates', 'Percent Unique', 'Object Discrete', 'Meta Discrete'])\n",
    "\n",
    "groups = df.groupby(by=['Sample Number','Meta Discrete'])\n",
    "for group in groups:\n",
    "    print(group[0], len(group[1]))\n",
    "    print(np.array(group[1]['Percent Unique']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
