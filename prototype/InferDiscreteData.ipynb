{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decide whether a sample is from a discrete or continuous distribution\n",
    "\n",
    "The code below computes the Normalized Nearest Neighbor Distribution (NNN) and then uses the Wasserstein distance to quantify how far from uniform it lives. Discrete distributions have an NNN that is much farther from uniform than continuous distributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from scipy.stats import ks_2samp\n",
    "from numpy.typing import NDArray\n",
    "from scipy.stats import wasserstein_distance as emd\n",
    "\n",
    "DISCRETE_MIN_EMD = 0.06 \n",
    "CONTINUOUS_MIN_SAMPLE_SIZE = 20\n",
    "\n",
    "def infer_discrete(X: NDArray, threshold: float = 0.05) -> NDArray:\n",
    "    \"\"\"Test for discreteness of a 1D sample.\n",
    "    Imagine 1D data representing a sample of a continuous distribution, e.g. event times for emissions\n",
    "    from a radioactive source. Think about the intervals between consecutive events; they have an exponential\n",
    "    distribution. The most likely interval is zero, and longer intervals are exponentially less likely; \n",
    "    the average interval is the reciprocal of the decay rate. This stands in stark contrast with the tick\n",
    "    times of a clock; the distribution of intervals between clock ticks is extremely sharply peaked; the \n",
    "    average and most likely intervals are in fact the same. Radioactive decay times and clock ticks \n",
    "    illustrate the fundamental distinction between continuous and discrete distributions.\n",
    "\n",
    "    Of course, any 1D sample can be sorted in the way that times naturally are, and so we can think \n",
    "    about the intervals between adjacent points. For a continuous distribution, a point is equally likely \n",
    "    to lie anywhere in the interval bounded by its two neighbors. Furthermore, we can put all \"between-\n",
    "    neighbor\" locations on the same scale of 0 to 1 by subtracting the smaller neighbor and dividing out\n",
    "    the length of the interval. (Duplicates are either assigned to zero or ignored, depending on context). \n",
    "    These normalized locations will be much more uniformly distributed for continuous data than for discrete, \n",
    "    and this gives us a way to distinguish them. Call this the Normalized Near Neighbor distribution (NNN), \n",
    "    defined on the interval [0,1]. \n",
    "\n",
    "    The Wasserstein distance is available in scipy.stats.wasserstein_distance. We can use it to measure how close \n",
    "    the NNN is to a uniform distribution over [0,1]. We found that as long as a sample has at least 20 points, and\n",
    "    furthermore at least half as many points as there are discrete values, we can reliably distinguish \n",
    "    discrete from continuous samples by testing that the Wasserstein distance is greater or less than 0.06, \n",
    "    respectively. \n",
    "\n",
    "    Example:\n",
    "\n",
    "    >>> rng = np.random.default_rng(314159265)\n",
    "    >>> Xnorm = rng.normal(100, scale=10, size=50)\n",
    "    >>> print(f'Normal sample is discrete: {infer_discrete(Xnorm)[0]}')\n",
    "    >>> Xfish = rng.poisson(100, size=50)\n",
    "    >>> print(f'Poisson sample is discrete: {infer_discrete(Xfish)[0]}')\n",
    "    >>> ks = ks_2samp(Xnorm, Xfish)\n",
    "    >>> print(f'KS can distinguish Normal from Poisson: {ks.pvalue < 0.05}')\n",
    "    Normal sample is discrete: False\n",
    "    Poisson sample is discrete: True\n",
    "    KS can distinguish Normal from Poisson: False\n",
    "    \"\"\"\n",
    "\n",
    "    if X.ndim == 1:\n",
    "        X = np.expand_dims(X, axis=1)\n",
    "    n_examples, n_features = X.shape\n",
    "\n",
    "    if n_examples < CONTINUOUS_MIN_SAMPLE_SIZE:\n",
    "        print(f'All samples look discrete with so few data points (< {CONTINUOUS_MIN_SAMPLE_SIZE})')\n",
    "        return np.array([True]*n_features)\n",
    "\n",
    "    shift = np.zeros(n_features)\n",
    "    cvm = np.zeros(n_features)\n",
    "    pval = np.zeros(n_features)\n",
    "    dx = np.zeros(n_examples - 2)\n",
    "    for i in range(n_features):\n",
    "        Xs = np.sort(X[:, i])\n",
    "        X0, X1 = Xs[0:-2], Xs[2:]\n",
    "\n",
    "        gtz = (X1 - X0) > 0\n",
    "        dx[gtz] = (Xs[1:-1] - X0)[gtz] / (X1 - X0)[gtz]\n",
    "        dx[np.logical_not(gtz)] = 0.0\n",
    "\n",
    "        shift[i] = emd(dx, np.linspace(0,1,len(dx)))\n",
    "\n",
    "    return shift > DISCRETE_MIN_EMD\n",
    "\n",
    "def _unicdf(x): # for x in range 0 to 1. \n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(314159265)\n",
    "npts = 800\n",
    "loc = 100\n",
    "Xnorm = rng.normal(loc, scale=np.sqrt(loc), size=npts)\n",
    "print(f'Normal sample is discrete: {infer_discrete(Xnorm)[0]}')\n",
    "Xfish = rng.poisson(loc, size=npts)\n",
    "print(f'Poisson sample is discrete: {infer_discrete(Xfish)[0]}')\n",
    "ks = ks_2samp(Xnorm, Xfish)\n",
    "print(f'KS can distinguish Normal from Poisson: {ks.pvalue < 0.05}') # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist((Xnorm, Xfish));\n",
    "plt.legend(['normal: continuous', 'Poisson: discrete'])\n",
    "plt.title('Similar distributions')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the hist function bins both quantities and plots them as discrete. But the normal values are nevertheless from a continuous distribution.\n",
    "\n",
    "The point is, the NNN and Wasserstein distance together provide a way to reasonably infer that a feature should be handled as discrete, for functions that want to know.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
