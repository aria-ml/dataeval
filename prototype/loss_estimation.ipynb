{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from typing import Dict, cast\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchmetrics\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms.v2 as v2\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import nannyml as nml\n",
    "from IPython.display import display\n",
    "import loss_estimation\n",
    "\n",
    "np.random.seed(0)\n",
    "np.set_printoptions(formatter={\"float\": lambda x: f\"{x:0.4f}\"})\n",
    "torch.manual_seed(0)\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "random.seed(0)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "torch._dynamo.disable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the mnist dataset\n",
    "to_tensor = v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])\n",
    "train_ds = datasets.MNIST(\"./data\", train=True, download=True, transform=to_tensor)\n",
    "test_ds = datasets.MNIST(\"./data\", train=False, download=True, transform=to_tensor)\n",
    "\n",
    "class_names = list(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the operator that will be used to generate a corrupted MNIST dataset, by adding random noise.\n",
    "\n",
    "class Corrupt(v2.Transform):\n",
    "    def _transform(self, inpt, params):\n",
    "        return self.contrast(inpt)\n",
    "\n",
    "    def contrast(self, sample):\n",
    "        x = sample\n",
    "        c = 0.3\n",
    "\n",
    "        # x = np.array(x) / 255.0\n",
    "        x = x.float() / 255.0\n",
    "        rands = torch.normal(x, std=c)\n",
    "        x = torch.clip(rands, 0, 1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "c_to_tensor = v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True), Corrupt()])\n",
    "c_contrast = v2.Compose([Corrupt()])\n",
    "\n",
    "# Sets up the corrupted test dataset, using the random-noise transform specified above\n",
    "c_test_ds = datasets.MNIST(\"./data\", train=False, download=True, transform=c_to_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a subset of 2000 training images and 500 test images,\n",
    "# so that the notebook cells can be evaluated quickly.\n",
    "train_ds = Subset(train_ds, range(2000))\n",
    "test_ds = Subset(test_ds, range(500))\n",
    "c_test_ds = Subset(c_test_ds, range(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(6400, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model = torch.compile(Net().to(device))\n",
    "\n",
    "# Type cast the model back to Net as torch.compile returns a Unknown\n",
    "# Nothing internally changes from the cast; we are simply signaling the type\n",
    "model = cast(Net, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the function we will use to train the model.\n",
    "def custom_train(model: nn.Module, dataset: Dataset):\n",
    "    # Defined only for this testing scenario\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    epochs = 10\n",
    "\n",
    "    # Define the dataloader for training\n",
    "    dataloader = DataLoader(dataset, batch_size=16)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch in dataloader:\n",
    "            # Load data/images to device\n",
    "            X = torch.Tensor(batch[0]).to(device)\n",
    "            # Load targets/labels to device\n",
    "            y = torch.Tensor(batch[1]).to(device)\n",
    "            # Zero out gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward propagation\n",
    "            outputs = model(X)\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, y)\n",
    "            # Back prop\n",
    "            loss.backward()\n",
    "            # Update weights/parameters\n",
    "            optimizer.step()\n",
    "\n",
    "def reset_parameters(model: nn.Module):\n",
    "    \"\"\"\n",
    "    Re-initializes each layer in the model using\n",
    "    the layer's defined weight_init function\n",
    "    \"\"\"\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def weight_reset(m: nn.Module):\n",
    "        # Check if the current module has reset_parameters\n",
    "        reset_parameters = getattr(m, \"reset_parameters\", None)\n",
    "        if callable(reset_parameters):\n",
    "            m.reset_parameters()  # type: ignore\n",
    "\n",
    "    # Applies fn recursively to every submodule see:\n",
    "    # https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
    "    return model.apply(fn=weight_reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model: nn.Module, dataset: Dataset, class_names, has_labels: bool = False, device=\"cuda\" if torch.cuda.is_available() else \"cpu\") -> Dict[str, list]:\n",
    "        #TODO: Let user set their own custom eval function\n",
    "        outputs = []\n",
    "        labels = []\n",
    "\n",
    "        # Set model layers into evaluation mode\n",
    "        model.eval()\n",
    "        dataloader = DataLoader(dataset, batch_size=16)\n",
    "        # Tell PyTorch to not track gradients, greatly speeds up processing\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                # Load data/images to device\n",
    "                X = torch.Tensor(batch[0]).to(device)\n",
    "                # Load targets/labels to device\n",
    "                \n",
    "                output = model(X).cpu()\n",
    "                outputs.append(output)\n",
    "                if has_labels:\n",
    "                    labels.append(batch[1])\n",
    "                \n",
    "\n",
    "        return outputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the network weights to \"create\" an untrained model\n",
    "model = reset_parameters(model)\n",
    "# Run the model with each substep of data\n",
    "# train on subset of train data\n",
    "train_kwargs = {}\n",
    "eval_kwargs = {}\n",
    "custom_train(\n",
    "    model,\n",
    "    train_ds,\n",
    "    **train_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = loss_estimation.LossEstimator(\"classification_multiclass\", [\"accuracy\"])\n",
    "\n",
    "ref_outputs, ref_labels = eval_model(model, test_ds, class_names, True)\n",
    "ref_df = loss_estimation.outputs_to_nannyml(\"classification\", ref_outputs, class_names, ref_labels)\n",
    "\n",
    "op_outputs, op_labels = eval_model(model, c_test_ds, class_names, True)\n",
    "op_df = loss_estimation.outputs_to_nannyml(\"classification\", op_outputs, class_names)\n",
    "\n",
    "op_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = estimator.evaluate(ref_df, op_df, class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_accuracy(model: nn.Module, dataset: Dataset, class_names, device=\"cuda\" if torch.cuda.is_available() else \"cpu\") -> Dict[str, list]:\n",
    "    # metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "    # result = 0\n",
    "    # batch_dicts = []\n",
    "    metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=len(class_names)).to(device)\n",
    "\n",
    "\n",
    "\n",
    "    # Set model layers into evaluation mode\n",
    "    model.eval()\n",
    "    dataloader = DataLoader(dataset, batch_size=16)\n",
    "    # Tell PyTorch to not track gradients, greatly speeds up processing\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Load data/images to device\n",
    "            X = torch.Tensor(batch[0]).to(device)\n",
    "            # Load targets/labels to device\n",
    "            y = torch.Tensor(batch[1]).int()\n",
    "            output = model(X).cpu()\n",
    "\n",
    "            metric.update(output, y)\n",
    "        result = metric.compute().cpu()\n",
    "    return {\"Accuracy\": result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_evaluated = get_accuracy(model, c_test_ds, class_names)\n",
    "\n",
    "pred_accuracy = results['Op_Predicted_accuracy']\n",
    "true_accuracy = c_evaluated[\"Accuracy\"].float()\n",
    "percent_diff = np.abs(pred_accuracy - true_accuracy)* 100\n",
    "\n",
    "print(f'Predicted accuracy on corrupted MNIST: {pred_accuracy}')\n",
    "print(f'Actual accuracy on corrupted MNIST: {true_accuracy}')\n",
    "print(f'Percentage point difference between true and predicted accuracy: {percent_diff} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
