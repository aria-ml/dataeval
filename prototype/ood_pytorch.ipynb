{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-11 19:19:10.843075: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-11 19:19:10.845955: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-11 19:19:10.851845: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731352750.862111   67505 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731352750.864986   67505 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-11 19:19:10.876011: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from dataeval.detectors.ood.ae_torch import OOD_AE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-11 18:58:16.101650: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-11 18:58:16.174901: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-11 18:58:16.231474: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1731351496.280350   64300 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1731351496.295694   64300 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-11 18:58:16.411987: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from dataeval.detectors.ood.ae_torch import OOD_AE\n",
    "from dataeval.utils.torch.datasets import MNIST\n",
    "from dataeval.utils.torch.models import AriaAutoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_encoder_net(input_shape: tuple[int, int, int], encoding_dim: int):\n",
    "    return Sequential(\n",
    "        [\n",
    "            InputLayer(input_shape=input_shape),\n",
    "            Conv2D(64, 4, strides=2, padding=\"same\", activation=relu),\n",
    "            Conv2D(128, 4, strides=2, padding=\"same\", activation=relu),\n",
    "            Conv2D(512, 4, strides=2, padding=\"same\", activation=relu),\n",
    "            Flatten(),\n",
    "            Dense(encoding_dim),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def get_default_decoder_net(input_shape: tuple[int, int, int], encoding_dim: int):\n",
    "    return Sequential(\n",
    "        [\n",
    "            InputLayer(input_shape=(encoding_dim,)),\n",
    "            Dense(4 * 4 * 128),\n",
    "            Reshape(target_shape=(4, 4, 128)),\n",
    "            Conv2DTranspose(256, 4, strides=2, padding=\"same\", activation=relu),\n",
    "            Conv2DTranspose(64, 4, strides=2, padding=\"same\", activation=relu),\n",
    "            Flatten(),\n",
    "            Dense(math.prod(input_shape)),\n",
    "            Reshape(target_shape=input_shape),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load in the training mnist dataset and use the first 4000\n",
    "train_ds = MNIST(root=\"./data/\", train=True, download=True, size=4000, unit_interval=True, channels=\"channels_first\")\n",
    "\n",
    "# Split out the images and labels\n",
    "images, labels = train_ds.data, train_ds.targets\n",
    "input_shape = images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "detectors = [\n",
    "    OOD_AE(AriaAutoencoder(channels=1)),\n",
    "]\n",
    "# detectors = [AriaAutoencoder(channels=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "new_ood = OOD_AE(AriaAutoencoder(channels=1))\n",
    "\n",
    "# model = TheModelClass(*args, **kwargs)\n",
    "# optimizer = TheOptimizerClass(*args, **kwargs)\n",
    "checkpoint_path = os.getcwd()+'/OOD_20241111_0942.pt'\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, weights_only=True)\n",
    "new_ood.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "# loss = checkpoint['loss']\n",
    "\n",
    "# new_ood.model.eval()\n",
    "detectors = [new_ood]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradients collapsing to zero....why?\n",
    "\n",
    "model params are being stepped, but steps drop to zero after 165 iterations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training OOD_AE...\n",
      "Epoch 0...\n",
      "loss: 0.0037996775936335325, |grad|: 0.09965380710799894\n",
      "loss: 0.002683198545128107, |grad|: 0.09452687117641327\n",
      "loss: 0.005245742853730917, |grad|: 0.12487432802683006\n",
      "loss: 0.005218362435698509, |grad|: 0.07444694275288781\n",
      "loss: 0.0027823697309941053, |grad|: 0.09393064317296468\n",
      "loss: 0.0035670222714543343, |grad|: 0.08593201273081992\n",
      "loss: 0.0016407860675826669, |grad|: 0.044660380380051795\n",
      "loss: 0.009273688308894634, |grad|: 0.15486524868035678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/dataeval/src/dataeval/detectors/drift/torch.py:127: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  preds_tmp = model(torch.tensor(x_batch).to(torch.float32))\n"
     ]
    }
   ],
   "source": [
    "# 50 epochs about 10 minutes\n",
    "for detector in detectors:\n",
    "    print(f\"Training {detector.__class__.__name__}...\")\n",
    "    detector.fit(images, threshold_perc=99, epochs=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# model = detectors[0].model\n",
    "# recon = model(torch.from_numpy(images).to(torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# fig, axs = plt.subplots(1,2)\n",
    "\n",
    "# i = 10\n",
    "# axs[0].imshow(images[i,0,:,:])\n",
    "# axs[1].imshow(recon[i,0,:,:].detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(recon[0,0,:,:].detach().reshape(-1) , recon[199,0,:,:].detach().reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "corruption = MNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    download=False,\n",
    "    size=2000,\n",
    "    unit_interval=True,\n",
    "    channels=\"channels_first\",\n",
    "    corruption=\"shot_noise\",\n",
    ")\n",
    "corrupted_images = corruption.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from importlib import reload\n",
    "# import dataeval.detectors.drift.torch\n",
    "\n",
    "# reload(dataeval.detectors.drift.torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('OOD_AE', 0.01)]\n",
      "[('OOD_AE', 0.887)]\n"
     ]
    }
   ],
   "source": [
    "print([(type(detector).__name__, np.mean(detector.predict(images).is_ood)) for detector in detectors])\n",
    "print([(type(detector).__name__, np.mean(detector.predict(corrupted_images).is_ood)) for detector in detectors])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(detector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch = 250\n",
    "\n",
    "# # torch.save({\n",
    "# #             'epoch': epoch,\n",
    "# #             'model_state_dict': detector.model.state_dict(),\n",
    "# #             'optimizer_state_dict': detector. optimizer.state_dict(),\n",
    "# #             'loss': loss,\n",
    "# #             ...\n",
    "# #             }, PATH)\n",
    "\n",
    "# checkpoint_path = os.getcwd()+'/OOD_20241111_0942.pt'\n",
    "\n",
    "# torch.save({\n",
    "#             'epoch': epoch,\n",
    "#             'model_state_dict': detector.model.state_dict(),\n",
    "#             }, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "new_ood = OOD_AE(AriaAutoencoder(channels=1))\n",
    "\n",
    "# model = TheModelClass(*args, **kwargs)\n",
    "# optimizer = TheOptimizerClass(*args, **kwargs)\n",
    "checkpoint_path = os.getcwd()+'/OOD_20241111_0942.pt'\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, weights_only=True)\n",
    "new_ood.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "# loss = checkpoint['loss']\n",
    "\n",
    "new_ood.model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, o in zip(new_ood.model.parameters(), detector.model.parameters()):\n",
    "    print((n==o).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detectors = [new_ood]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
