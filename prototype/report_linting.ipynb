{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    import google.colab  # noqa: F401\n",
    "\n",
    "    !pip install -q daml[torch] torchmetrics torchvision\n",
    "    !export LC_ALL=\"en_US.UTF-8\"\n",
    "    !export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
    "    !export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
    "    !ldconfig /usr/lib64-nvidia\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "!pip install -q tabulate\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "np.set_printoptions(formatter={\"float\": lambda x: f\"{x:0.4f}\"})\n",
    "torch.manual_seed(0)\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "random.seed(0)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from daml.models.tensorflow import AE, VAEGMM, create_model\n",
    "from importlib import reload\n",
    "\n",
    "import daml._internal.detectors.duplicates as duplicates\n",
    "import daml._internal.detectors.linter as linter\n",
    "import daml._internal.metrics.stats as stats\n",
    "\n",
    "reload(stats)\n",
    "reload(linter)\n",
    "reload(duplicates)\n",
    "\n",
    "# # MNIST Data\n",
    "import hashlib\n",
    "import os\n",
    "import typing\n",
    "from urllib.error import HTTPError, URLError\n",
    "from urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_mnist() -> str:\n",
    "    \"\"\"Code to download mnist originates from keras/datasets:\n",
    "\n",
    "    https://github.com/keras-team/keras/blob/v2.15.0/keras/datasets/mnist.py#L25-L86\n",
    "    \"\"\"\n",
    "    origin_folder = \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/\"\n",
    "    path = _get_file(\n",
    "        \"mnist.npz\",\n",
    "        origin=origin_folder + \"mnist.npz\",\n",
    "        file_hash=(\"731c5ac602752760c8e48fbffcf8c3b850d9dc2a2aedcf2cc48468fc17b673d1\"),\n",
    "    )\n",
    "\n",
    "    return path\n",
    "\n",
    "\n",
    "def _get_file(\n",
    "    fname: str,\n",
    "    origin: str,\n",
    "    file_hash: typing.Optional[str] = None,\n",
    "):\n",
    "    cache_dir = os.path.join(os.path.expanduser(\"~\"), \".keras\")\n",
    "    datadir_base = os.path.expanduser(cache_dir)\n",
    "    if not os.access(datadir_base, os.W_OK):\n",
    "        datadir_base = os.path.join(\"/tmp\", \".keras\")\n",
    "    datadir = os.path.join(datadir_base, \"datasets\")\n",
    "    os.makedirs(datadir, exist_ok=True)\n",
    "\n",
    "    fname = os.fspath(fname) if isinstance(fname, os.PathLike) else fname\n",
    "    fpath = os.path.join(datadir, fname)\n",
    "\n",
    "    download = False\n",
    "    if os.path.exists(fpath):\n",
    "        if file_hash is not None and not _validate_file(fpath, file_hash):\n",
    "            download = True\n",
    "    else:\n",
    "        download = True\n",
    "\n",
    "    if download:\n",
    "        try:\n",
    "            error_msg = \"URL fetch failure on {}: {} -- {}\"\n",
    "            try:\n",
    "                urlretrieve(origin, fpath)\n",
    "            except HTTPError as e:\n",
    "                raise Exception(error_msg.format(origin, e.code, e.msg)) from e\n",
    "            except URLError as e:\n",
    "                raise Exception(error_msg.format(origin, e.errno, e.reason)) from e\n",
    "        except (Exception, KeyboardInterrupt):\n",
    "            if os.path.exists(fpath):\n",
    "                os.remove(fpath)\n",
    "            raise\n",
    "\n",
    "        if os.path.exists(fpath) and file_hash is not None and not _validate_file(fpath, file_hash):\n",
    "            raise ValueError(\n",
    "                \"Incomplete or corrupted file detected. \"\n",
    "                f\"The sha256 file hash does not match the provided value \"\n",
    "                f\"of {file_hash}.\",\n",
    "            )\n",
    "    return fpath\n",
    "\n",
    "\n",
    "def _validate_file(fpath, file_hash, chunk_size=65535):\n",
    "    hasher = hashlib.sha256()\n",
    "    with open(fpath, \"rb\") as fpath_file:\n",
    "        for chunk in iter(lambda: fpath_file.read(chunk_size), b\"\"):\n",
    "            hasher.update(chunk)\n",
    "\n",
    "    return str(hasher.hexdigest()) == str(file_hash)\n",
    "\n",
    "\n",
    "mnist_path = download_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create\n",
    "rng = np.random.default_rng(33)\n",
    "size = 10000\n",
    "\n",
    "with np.load(mnist_path, allow_pickle=True) as fp:\n",
    "    test_images, labels = fp[\"x_train\"][:size], fp[\"y_train\"][:size]\n",
    "\n",
    "norm_test_imgs = np.repeat(test_images[:, np.newaxis, :, :], 3, axis=1) / 255\n",
    "jitter = rng.integers(10, size=norm_test_imgs.shape)\n",
    "norm_test_imgs += jitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 5000\n",
    "# Take 5000 images and duplicate the first 10 and triplicate the first 5\n",
    "lint = linter.Linter(norm_test_imgs[:count])\n",
    "\n",
    "dupe = duplicates.Duplicates(\n",
    "    np.concatenate((norm_test_imgs[:count], norm_test_imgs[:10], np.clip(norm_test_imgs[:5] * 1.001, 0.0, 10.0)))\n",
    ")\n",
    "results = lint.evaluate()\n",
    "dupes = dupe.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-specified inputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'brightness': 0.02}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# results.keys()\n",
    "# results[464][list(results[464].keys())[0]]\n",
    "results[464]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = {}\n",
    "num_outliers = 0\n",
    "for idx in results:\n",
    "    strs_curr = list(results[idx].keys())\n",
    "    if len(strs_curr) > 0:\n",
    "        num_outliers += 1\n",
    "\n",
    "    for outlier_metric in strs_curr:\n",
    "        if outlier_metric not in outliers:\n",
    "            outliers[outlier_metric] = [0]\n",
    "        outliers[outlier_metric][0] += 1\n",
    "\n",
    "outlier_stats = {\"total\": num_outliers, \"percent\": round(num_outliers / count, 4)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dupes = len(dupes[\"exact\"])\n",
    "percent_dupes = round(len(dupes[\"exact\"]) / count, 4)\n",
    "\n",
    "num_near = len(dupes[\"exact\"])\n",
    "percent_near = round(len(dupes[\"exact\"]) / count, 4)\n",
    "\n",
    "outlier_dict = outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary that gradient will plot as a table\n",
    "dupe_stats = {\n",
    "    \"num_dupes\": num_dupes,\n",
    "    \"percent_dupes\": percent_dupes,\n",
    "    \"num_near\": num_near,\n",
    "    \"percent_near\": percent_near,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradient.slide_deck.shapes import SubText, Table, Text, TextContent\n",
    "from gradient.slide_deck.slidedeck import (\n",
    "    DEFAULT_GRADIENT_PRESENTATION_TEMPLATE_PATH,\n",
    "    DefaultGradientSlideLayouts,\n",
    "    SlideDeck,\n",
    ")\n",
    "\n",
    "\n",
    "def generate_dupe_report_table(dupe_stats: dict) -> pd.DataFrame:\n",
    "    dupe_table = pd.DataFrame(\n",
    "        {\n",
    "            \"Number of Exact Duplicates\": [str(dupe_stats[\"num_dupes\"])],\n",
    "            \"Percent Exact Duplicates in Dataset\": [str(dupe_stats[\"percent_dupes\"])],\n",
    "            \"Number of Near Duplicates\": [str(dupe_stats[\"num_near\"])],\n",
    "            \"Percent Near Duplicates in Dataset\": [str(dupe_stats[\"percent_near\"])],\n",
    "        }\n",
    "    )\n",
    "    return dupe_table\n",
    "\n",
    "\n",
    "def generate_outlier_report_table(outlier_dict: dict) -> pd.DataFrame:\n",
    "    return pd.DataFrame.from_dict(outlier_dict)\n",
    "\n",
    "\n",
    "def generate_dupe_report_slide_kwargs(dupe_stats: dict) -> dict:\n",
    "    content = [\n",
    "        f\"{dupe_stats['percent_dupes']*100}% of the dataset is a duplicate entry.\",\n",
    "    ]\n",
    "\n",
    "    kwargs = {\n",
    "        \"title\": \"Duplicate Detection: Summary\",\n",
    "        \"layout\": DefaultGradientSlideLayouts.CONTENT_DEFAULT,\n",
    "        \"placeholder_fillings\": [TextContent(lines=[Text(content=content)])],\n",
    "        \"additional_shapes\": [\n",
    "            Table(\n",
    "                dataframe=generate_dupe_report_table(dupe_stats),\n",
    "                fontsize=16,\n",
    "                left=2.0,\n",
    "                top=2.0,\n",
    "                width=9.0,\n",
    "                height=4.0,\n",
    "            ),\n",
    "        ],\n",
    "    }\n",
    "    return kwargs\n",
    "\n",
    "\n",
    "def generate_outlier_report_slide_kwargs(outlier_dict: dict, outlier_stats: dict) -> dict:\n",
    "    content = [\n",
    "        f\"{outlier_stats['total']} images in the dataset are outliers ({100*outlier_stats['percent']}% of the dataset).\",\n",
    "    ]\n",
    "\n",
    "    kwargs = {\n",
    "        \"title\": \"Outlier Detection: Summary\",\n",
    "        \"layout\": DefaultGradientSlideLayouts.CONTENT_DEFAULT,\n",
    "        \"placeholder_fillings\": [TextContent(lines=[Text(content=content)])],\n",
    "        \"additional_shapes\": [\n",
    "            Table(\n",
    "                dataframe=generate_outlier_report_table(outlier_dict),\n",
    "                fontsize=16,\n",
    "                left=2.0,\n",
    "                top=2.0,\n",
    "                width=9.0,\n",
    "                height=4.0,\n",
    "            ),\n",
    "        ],\n",
    "    }\n",
    "    return kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "example_directory = Path.cwd() / \"report_linting_example\"\n",
    "example_directory.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and add to the slide deck\n",
    "deck = SlideDeck(presentation_template_path=DEFAULT_GRADIENT_PRESENTATION_TEMPLATE_PATH)\n",
    "\n",
    "deck.add_slide(**generate_outlier_report_slide_kwargs(outlier_dict, outlier_stats))\n",
    "deck.add_slide(**generate_dupe_report_slide_kwargs(dupe_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/workspaces/daml/prototype/report_linting_example/report_linting_example_4.pptx')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deck.save(\n",
    "    output_directory=example_directory,\n",
    "    name=\"report_linting_example\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
