{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial on associating changes in metadata with observed dataset drifts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Problem Statement_\n",
    "\n",
    "When incoming data have been found to have drifted, we may wish to understand the underlying causes of the drift. Metadata may help with this task. We can look for metadata features that are predictive of out-of-distribution examples (OOD), or, if OOD are few, we can examine the significance and magnitude of distributional shifts of metadata factors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _When to use_\n",
    "\n",
    "Once you have detected drift using the `dataeval.detectors` drift detection classes, you should employ these tools to look either for metadata features that accurately predict out-of-distribution (OOD) examples, or for significant differences in two metadata distributions, i.e. the metadata corresponding to your reference and drifted datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _What you will need_\n",
    "\n",
    "1. A reference dataset which you have used to train a model.\n",
    "2. A drifted dataset, i.e. new data for which you have detected drift (see DriftDetectionTutorial; steps reproduced here), with corresponding metadata.\n",
    "3. Metadata corresponding to each of your two datasets, OR defined methods that generate intrinsic metadata from the data examples, for each dataset.\n",
    "4. A python environment with the following packages installed:\n",
    "   - `dataeval[torch]` or `dataeval[all]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _New tools developed_\n",
    "\n",
    "1. predict_ood_mi() - a standalone function that quantifies the power of metadata features to predict OOD examples\n",
    "2. ks_compare() - a standalone function that compares incoming metadata features to a reference and reports normalized shifts and their significance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Setting up_\n",
    "\n",
    "Let's import the required libraries needed to set up a minimal working example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab  # noqa: F401\n",
    "\n",
    "    %pip install -q dataeval[torch]\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision.datasets.vision import VisionDataset\n",
    "from torchvision import datasets\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "from metadata_utils import InstanceMNIST, collate_fn_2, collate_fn_3\n",
    "from metadata_tools import predict_ood_mi, ks_compare, least_likely_features\n",
    "\n",
    "from scipy.spatial import ConvexHull\n",
    "from sklearn.feature_selection import mutual_info_classif, mutual_info_regression\n",
    "\n",
    "from dataeval.detectors.drift import (\n",
    "    DriftCVM,\n",
    "    DriftKS,\n",
    "    DriftMMD,\n",
    "    preprocess_drift,\n",
    ")\n",
    "\n",
    "from dataeval.detectors.ood import OOD_AE\n",
    "from dataeval._internal.models.tensorflow.autoencoder import AE\n",
    "from dataeval._internal.models.tensorflow.utils import create_model\n",
    "from dataeval._internal.models.pytorch.autoencoder import AETrainer, AriaAutoencoder\n",
    "\n",
    "from dataeval._internal.metrics.utils import entropy, preprocess_metadata\n",
    "\n",
    "from dataeval._internal.metrics.balance import balance\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(108)\n",
    "tf.keras.utils.set_random_seed(408)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in data\n",
    "\n",
    "Let's start by loading some TensorFlow MNIST datasets. We will then wrap them into Pytorch datasets, attach some metadata, and then examine them. The new PyTorch class [InstanceMNIST](metadata_utils.py) will store the MNIST data and compute/store intrinsic metadata for the purposes of this demo.\n",
    "\n",
    "The **init** method of InstanceMNIST takes 2 keyword args: a corruption type if any, and a split if any. Possible corruption types are listed inside the get_MNIST_data() method and also stored as a corruptions attribute in each instance. For more information see https://www.tensorflow.org/datasets/catalog/mnist_corrupted .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides storing images and labels, the wrapper class InstanceMNIST lets us add methods to generate and store whatever intrinsic metadata we want. The existing methods provide a template for doing this, see e.g. bbox(). In addition to a function that returns the \"one scalar per image\" quantity you want to compute, you need to package the quantity as a dict of lists, i.e. each metadata feature name will correspond to a dict key, and each key will refer to a list of metadata values corresponding to the examples in the dataset.\n",
    "\n",
    "InstanceMNIST also explicitly normalizes MNIST pixel values to be between 0 and 1, and casts to numpy float32.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate a reference dataset and some corrupted datasets\n",
    "\n",
    "We can then use these in our experiments below. The corruptions are intended to simulate drifts that we might observe in practice.\n",
    "\n",
    "2024-10-22 Need to change InstanceMNIST so that it grabs some of each of a list of corruptions. Then get an actual dataset for a particualr corruption with a method call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corruption_list = ['identity', 'translate', 'shot_noise', 'motion_blur', 'scale']\n",
    "\n",
    "mnist = InstanceMNIST(corruption_list, size=8000)\n",
    "mnist_val = InstanceMNIST('identity', train=False, size=8000)\n",
    "\n",
    "refdata = mnist.identity\n",
    "valdata = mnist_val.identity\n",
    "shiftdata = mnist.translate\n",
    "spikydata = mnist.shot_noise\n",
    "blurdata = mnist.motion_blur\n",
    "scaledata = mnist.scale\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = np.zeros(10, dtype=np.int32)\n",
    "for i in range(10):\n",
    "    count[i] = np.sum(refdata.labels==i)\n",
    "    print(f'{i}: {count[i]}')\n",
    "print(np.std(count, ddof=1))\n",
    "print(np.sqrt(800*0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the drift detectors\n",
    "\n",
    "In order to reduce the dimensionality of the data, we can pass a simple Autoencoder to the drift detectors using the the `preprocess_fn` keyword arg. While this is not crucial for the MNIST data set, it is highly recommended for datasets that have higher dimensionality, to reduce the number of comparisons made.\n",
    "\n",
    "For the purposes of the tutorial, we will use 3 forms of drift detectors: Maximum Mean Discrepancy (MMD), CramÃ©r-von Mises (CVM), and Kolmogorov-Smirnov (KS). These detectors are built using data_reference and thus measure drifts relative to that dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AriaAutoencoder(channels=1)\n",
    "trainer = AETrainer(model, device=device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train the autoencoder to reconstruct examples contained in the refdata dataset. (Since our corruptions here are so heavy-handed and easily detected, we included the option to skip training the autoencoder, but left in the snippet that shows how to do so.)\n",
    "\n",
    "Then, we use the encoder part of the autoencoder (trained or not) to generate projections into a latent space, i.e. embeddings, and we use the embeddings to decide whether or not new incoming datasets have drifted relative to the reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_the_autoencoder = True\n",
    "if train_the_autoencoder:\n",
    "    print(f\"Training {model.__class__.__name__}...\")\n",
    "    training_loss = trainer.train(refdata, epochs=10)\n",
    "else:\n",
    "    print('NOT TRAINING AUTOENCODER!')\n",
    "    \n",
    "encoder_net = model.encoder.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build the drift detectors using the first 2000 images from refdata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reference = refdata.images[0:2000]\n",
    "\n",
    "# define preprocessing function\n",
    "preprocess_fn = partial(preprocess_drift, model=encoder_net, batch_size=64, device=device)\n",
    "\n",
    "# initialise drift detectors\n",
    "drift_detectors = [detector(data_reference, preprocess_fn=preprocess_fn) for detector in [DriftMMD, DriftCVM, DriftKS]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test reference against control\n",
    "\n",
    "Let's check for drift between the first 2000 images and the second 2000 images from this sample. The drift detector should not detect any drift.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_control = valdata.images[2000:4000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the test by calling the predict() method on each detector, with data_control as an argument. Then examine the is_drift attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test two samples from same dataset for drift:')\n",
    "[(type(detector).__name__, detector.predict(data_control).is_drift) for detector in drift_detectors]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we assess, for all drift detectors, that there is no significant drift between these two MNIST subsets, as expected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look for drift with translated data\n",
    "\n",
    "The translate corruption moves each digit within its image, towards a randomly selected corner, by a few pixels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupt_images = shiftdata.images[0:2000]\n",
    "print('Test corrupted image sample for drift:')\n",
    "[(type(detector).__name__, detector.predict(corrupt_images).is_drift) for detector in drift_detectors]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the translate corruption indeed leads to a measurable drift. Such a drift might be the footprint of data examples in the test set which are OOD for the reference set. We will make a data loader that grabs a batch of 2000 and use it to look for OOD examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the dataloader \"refbb\": it will grab a big batch of reference data. After getting a batch, we need to adjust the shape of its images, for compatibility with the OOD detectors. We will do the same for a second dataloader that will hold images for validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_batch_size = 2000\n",
    "collate_fn = collate_fn_2\n",
    "\n",
    "\n",
    "refbb = DataLoader(refdata, collate_fn=collate_fn, batch_size=big_batch_size)\n",
    "valbb = DataLoader(valdata, collate_fn=collate_fn, batch_size=big_batch_size)\n",
    "\n",
    "for train_images, _, _ in refbb:\n",
    "    break\n",
    "for val_images, _, _ in valbb:\n",
    "    break\n",
    "\n",
    "\n",
    "# Now adjust shape of images array, i.e. append a 1 to the shape. \n",
    "input_shape = (*train_images[0].shape, 1)\n",
    "# bbshape = (*train_images.shape,1)  # adjust the batch shape\n",
    "\n",
    "# train_images = train_images.reshape(bbshape).detach().numpy()\n",
    "# val_images = val_images.reshape(bbshape).detach().numpy()\n",
    "\n",
    "bbshape = train_images.shape  # DON'T adjust the batch shape\n",
    "\n",
    "train_images = train_images.reshape(bbshape).detach().numpy()\n",
    "val_images = val_images.reshape(bbshape).detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the adjusted input shape to instantiate OOD detectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OOD_detectors = [\n",
    "#     OOD_AE(create_model(AE, input_shape))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell trains the OOD detectors, which will take a few minutes, but you only need to do it once for each reference dataset. After training an OOD detector, you can test as many incoming datasets as you care to for OOD examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for detector in OOD_detectors:\n",
    "#     print(f\"Training {detector.__class__.__name__}...\")\n",
    "#     detector.fit(train_images, threshold_perc=99, epochs=12, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataeval.utils.torch.datasets import MNIST\n",
    "\n",
    "# Load in the training mnist dataset and use the first 4000\n",
    "train_ds = MNIST(root=\"./data/\", train=True, download=True, size=4000, unit_interval=True, channels=\"channels_first\")\n",
    "\n",
    "# Split out the images and labels\n",
    "images, labels = train_ds.data, train_ds.targets\n",
    "input_shape = images[0].shape\n",
    "\n",
    "OOD_detectors = [OOD_AE(create_model(AE, input_shape))]\n",
    "\n",
    "for detector in OOD_detectors:\n",
    "    print(f\"Training {detector.__class__.__name__}...\")\n",
    "    detector.fit(images, threshold_perc=99, epochs=10, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now make a few data loaders for corrupted images. These have bb in their names, for \"big batch\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr1bb = DataLoader(shiftdata, collate_fn=collate_fn, batch_size=big_batch_size)\n",
    "corr2bb = DataLoader(spikydata, collate_fn=collate_fn, batch_size=big_batch_size)\n",
    "corr3bb = DataLoader(blurdata, collate_fn=collate_fn, batch_size=big_batch_size)\n",
    "corr4bb = DataLoader(scaledata, collate_fn=collate_fn, batch_size=big_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will choose the translate corruption and run the OOD detectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(images.shape)\n",
    "\n",
    "corrbb = corr1bb # corr1bb is translate corruption\n",
    "\n",
    "for corrimages, corrlabels, corrmetadata in corrbb:\n",
    "    break\n",
    "\n",
    "print(corrimages.shape)\n",
    "# corrimages = corrimages.reshape((*corrimages.shape,1))\n",
    "# print(corrimages.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrbb = corr1bb # corr1bb is translate corruption\n",
    "\n",
    "for corrimages, corrlabels, corrmetadata in corrbb:\n",
    "    break\n",
    "\n",
    "# corrimages = corrimages.reshape((*corrimages.shape,1))\n",
    "\n",
    "print('What fraction of reference images have OOD examples?')\n",
    "print([(type(detector).__name__, np.mean(detector.predict(train_images).is_ood)) for detector in OOD_detectors])\n",
    "print('\\nWhat fraction of images from drifted dataset have OOD examples?')\n",
    "print([(type(detector).__name__, np.mean(detector.predict(corrimages).is_ood)) for detector in OOD_detectors])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And indeed, our OOD_AE detector reports that most images in the mnist-translate dataset are OOD relative to MNIST itself. We can now ask if any available metadata features are predictive of OOD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_ae = OOD_detectors[0]\n",
    "is_ood = ood_ae.predict(corrimages).is_ood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_images.shape, val_images.shape, corrimages.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loss and validation loss should be comparable. If validation loss is much greater than training loss, it means we trained the OOD model for too long. (Another symptom of overtraining, here in this notebook, is an excess of OOD detections on other samples of the reference dataset, i.e. the \"identity\" corruption will show a high rate of detections, when it should actaully be near zero.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = ood_ae.predict(train_images)\n",
    "train_loss = np.mean(train_pred.instance_score)\n",
    "\n",
    "val_pred = ood_ae.predict(val_images)\n",
    "val_loss = np.mean(val_pred.instance_score)\n",
    "\n",
    "print(f'training loss reached {train_loss:0.4f}.')\n",
    "print(f'validation loss is {val_loss:0.4f}.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function [predict_ood_mi](metadata_tools.py#predict_ood_mi) takes big batch dataloaders and an ood detector, and returns a list of metadata factors, sorted in decreasing order of the mutual information they share with the OOD flag.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test predict_ood_mi() on our 3 corrupted datasets. First there is translate, in which digits have been moved toward a randomly selected corner of their images.\n",
    "\n",
    "Note that I have added a random feature to the metadata. This can easily be done with any dataset, just pair a random value with every data example. Doing so provides a point of reference\n",
    "for evaluating whether any metadata association test we might perform is meaningful or not.\n",
    "\n",
    "You can specify whether your metadata features are continuous or discrete, through a keyword arg. Here all the features are continuous, so I will set discrete_features to False. If you have a mixture of continuous and discrete, you could have one bool for each feature, e.g. you have 3 features and only the first is discrete --> discrete_features = [True, False, False].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('translate')\n",
    "predict_ood_mi(refbb, corr1bb, ood_ae, discrete_features=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The centers of mass and the centers of the bboxes are the most predictive metadata, as we might expect; in fact they carry most of the bit that the OOD flag represents.\n",
    "\n",
    "Next we try the shot_noise corruption, which adds random values to the nonzero pixels; we might expect the spikiness measure to associate with this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('shot noise')\n",
    "predict_ood_mi(refbb, corr2bb, ood_ae, discrete_features=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the scale corruption (usually) has somewhat weaker associations between metadata and OOD, relative to the translate corruption. Spikiness is oddly strong; perhaps the scaling operation also smooths adjacent pixel values as a side effect, so that the **absence** of spikiness predicts OOD to some small degree; mutual information can detect this also.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('scale')\n",
    "predict_ood_mi(refbb, corr4bb, ood_ae, discrete_features=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore causes of drift with scaled data\n",
    "\n",
    "In cases where the metadata are more weakly associated with OOD examples, we might dig deeper for an explanation of the drift. We will try this with scale.\n",
    "\n",
    "First confirm that scale data indeed results in detection of drift.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupt_images = scaledata.images[:2000]\n",
    "print('Test scaled digits for dataset drift:')\n",
    "[(type(detector).__name__, detector.predict(corrupt_images).is_drift) for detector in drift_detectors]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the scale corruption also generates data drift. Can the drift be attributed to OOD examples?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrbb = DataLoader(scaledata, collate_fn=collate_fn, batch_size=big_batch_size)\n",
    "\n",
    "for corrimages, corrlabels, corrmetadata in corrbb:\n",
    "    break\n",
    "\n",
    "# corrimages = corrimages.reshape((*corrimages.shape,1))\n",
    "\n",
    "print('What fraction of scaled digits are OOD?')\n",
    "print([(type(detector).__name__, np.mean(detector.predict(corrimages).is_ood)) for detector in OOD_detectors])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can of course run the OOD detector on images made with each the other corruptions, to see which has the most OOD examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corruptions = mnist.corruptions\n",
    "mnist_all = InstanceMNIST(mnist.corruptions, size=3750)\n",
    "\n",
    "c_ood_dict = {}\n",
    "for c in corruptions:\n",
    "    # print(f'processing {c} data...')\n",
    "    i0 = np.random.randint(0, len(refdata)-big_batch_size)\n",
    "    i1 = i0 + big_batch_size\n",
    "    split=\"train[\" + str(i0) + \":\" + str(i1)+\"]\"\n",
    "    cdata = getattr(mnist_all, c)\n",
    "\n",
    "    cbb = DataLoader(cdata, collate_fn=collate_fn, batch_size=big_batch_size)\n",
    "    for corrimages, _, _ in cbb:\n",
    "        break\n",
    "    # corrimages = corrimages.reshape((*corrimages.shape,1))\n",
    "\n",
    "    # print(c, [np.mean(detector.predict(corrimages).is_ood) for detector in OOD_detectors])\n",
    "    c_ood_dict.update({c: [np.mean(detector.predict(corrimages).is_ood) for detector in OOD_detectors]})\n",
    "\n",
    "ood_frac = [v[0] for v in c_ood_dict.values()]\n",
    "iord = np.argsort(ood_frac)\n",
    "names = [k for k in c_ood_dict]\n",
    "maxlen = max([len(name) for name in names])\n",
    "\n",
    "hdr = 'corruption'\n",
    "print(f'{hdr:{maxlen}} |  ood fraction')\n",
    "print('='*(maxlen+15))\n",
    "for i in iord:\n",
    "        print(f'{names[i]:{maxlen+1}}:      {ood_frac[i]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the identity corruption, which is actually not a corruption at all, can nevertheless yield some OOD detections. One can evaluate the true prevalance of OOD by comparing the rate of these false detections with other putative detections. In other words, we can sometimes have an overly sensitive OOD detector, in which case we might get a lot of detections even when testing the reference distribution against itself. In the case shown above, we can see that the scale, glass_blur, and motion_blur corruptions have essentially no examples detected as OOD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare metadata features using KS two-sample test.\n",
    "\n",
    "In cases where OOD examples are relatively few, we might want to ask if there is anything in the metadata that has undergone a distributional change that might be contributing to the observed drift.\n",
    "\n",
    "The function [ks_compare()](metadata_tools.py#ks_compare) does a feature-wise KS test on the drifted metadata, relative to reference metadata. It lumps batches together until the Kolmogorov-Smirnov test statistic has reached a stable value. Thus, small batch dataloaders can be passed to ks_compare() without leading to underpowered statistical tests.\n",
    "\n",
    "ks_compare returns a dict containing ks_2samp results for each metadata feature. It also prints the p-value for each feature in ascending order, i.e. most significant first.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make some new dataloaders with more typical batch sizes of 11, using the same datasets created above, and use them to look for metadata distributional shifts using ks_compare().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = collate_fn_2\n",
    "batch_size = 11\n",
    "refdl = DataLoader(refdata, collate_fn=collate_fn, batch_size=batch_size)\n",
    "valdl = DataLoader(valdata, collate_fn=collate_fn, batch_size=batch_size)\n",
    "refdlc1 = DataLoader(shiftdata, collate_fn=collate_fn, batch_size=batch_size)\n",
    "refdlc2 = DataLoader(spikydata, collate_fn=collate_fn, batch_size=batch_size)\n",
    "refdlc3 = DataLoader(blurdata, collate_fn=collate_fn, batch_size=batch_size)\n",
    "refdlc4 = DataLoader(scaledata, collate_fn=collate_fn, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we want to know if a drift of any metadata features might explain the drift observed for scaled data. (The MNIST corruptions are quite dramatic and it is fairly obvious what is leading to the detected drift. But it is possible to detect drift in cases where the OOD detector fails to see OOD examples; in such cases one would use ks_compare to find subtle distributional shifts in metadata factors.)\n",
    "\n",
    "First compare metadata factors from the training and validation datasets. We expect to see mostly high p-values for all metadata features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_compare(refdl, valdl);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about for the scale corruption, which yielded few OOD detections?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('scale')\n",
    "res = ks_compare(refdl, refdlc4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the available metadata factors have undergone significant distributional changes, as seen by the small p-values from the KS test. The shift metric shows how far the distribution has shifted on average, in units of the width of the reference distribution. The values are sorted from greatest to least statistical significance (increasing p-values).\n",
    "\n",
    "We can of course also run ks_compare on other corrupted datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('translate')\n",
    "res1 = ks_compare(refdl, refdlc1)\n",
    "print('\\n\\nshot_noise')\n",
    "res2 = ks_compare(refdl, refdlc2)\n",
    "print('\\n\\nmotion_blur')\n",
    "res3 = ks_compare(refdl, refdlc3)\n",
    "print('\\n\\nscale')\n",
    "res4 = ks_compare(refdl, refdlc4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Summary_\n",
    "\n",
    "We demonstrate a method for associating OOD examples with metadata features, in cases where dataset drift has been observed, using mutual information. By including a random metadata feature, we can evaluate whether a given association, however weak, could be considered significant or not.\n",
    "\n",
    "We also demonstrate a method for deciding whether a set of detected OOD examples is mostly real or not: by running the OOD detector on an uncorrupted version of the reference distribution.\n",
    "\n",
    "We also demonstrate a method for finding significant distributional shifts in metadata features, and display the p-values of these shifts. Here again, including a random metadata feature provides a sanity check. We also compute a measure of the magnitudes of distributional shifts, relative to the width of their reference distributions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
