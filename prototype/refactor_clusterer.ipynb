{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Clustering Code 5/10/24\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code here is for testing the integrated clusterer with the original clusterer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from typing import Any, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import sklearn.datasets as dsets\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "import daml._internal.metrics.clustering as cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_condensed_distance_array(data):\n",
    "    return pdist(data, metric=\"euclidean\")\n",
    "\n",
    "\n",
    "def get_square_distance_matrix(condensed_distance_array):\n",
    "    return squareform(condensed_distance_array)\n",
    "\n",
    "\n",
    "def get_linkage_arr(condensed_distance_array):\n",
    "    return linkage(condensed_distance_array, method=\"single\")\n",
    "\n",
    "\n",
    "def extend_linkage(link_arr):\n",
    "    \"\"\"\n",
    "    Adds a column to the linkage matrix Z that tracks the new id assigned\n",
    "    to each row\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    Z\n",
    "        linkage matrix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    arr\n",
    "        linkage matrix with adjusted shape, new shape (Z.shape[0], Z.shape[1]+1)\n",
    "    \"\"\"\n",
    "    # Adjusting linkage matrix to accommodate renumbering\n",
    "    rows, cols = link_arr.shape\n",
    "    arr = np.zeros((rows, cols + 1))\n",
    "    arr[:, :-1] = link_arr\n",
    "    arr[:, -1] = np.arange(rows + 1, 2 * rows + 1)\n",
    "\n",
    "    return arr\n",
    "\n",
    "\n",
    "def get_extended_linkage(condensed_distance_array):\n",
    "    link_arr = get_linkage_arr(condensed_distance_array)\n",
    "    return extend_linkage(link_arr)\n",
    "\n",
    "\n",
    "def fill_missing_cluster_level(left_id, right_id, level, clusters):\n",
    "    if left_id:\n",
    "        left_level = left_id[0] + 1\n",
    "        left_cluster = left_id[1]\n",
    "        if level != left_level:\n",
    "            for level_id in range(level - 1, left_level - 2, -1):\n",
    "                if left_cluster not in clusters[level_id]:\n",
    "                    clusters[level_id][left_cluster] = {\n",
    "                        \"cluster_merged\": False,\n",
    "                        \"count\": clusters[left_level - 1][left_cluster][\"count\"],\n",
    "                        \"avg_dist\": clusters[left_level - 1][left_cluster][\"avg_dist\"],\n",
    "                        \"dist_std\": clusters[left_level - 1][left_cluster][\"dist_std\"],\n",
    "                        \"samples\": clusters[left_level - 1][left_cluster][\"samples\"],\n",
    "                        \"sample_dist\": clusters[left_level - 1][left_cluster][\"sample_dist\"],\n",
    "                        \"outside_1-std\": False,\n",
    "                        \"outside_2-std\": False,\n",
    "                    }\n",
    "    if right_id:\n",
    "        right_level = right_id[0] + 1\n",
    "        right_cluster = right_id[1]\n",
    "        if level != right_level:\n",
    "            for level_id in range(level - 1, right_level - 2, -1):\n",
    "                if right_cluster not in clusters[level_id]:\n",
    "                    clusters[level_id][right_cluster] = {\n",
    "                        \"cluster_merged\": False,\n",
    "                        \"count\": clusters[right_level - 1][right_cluster][\"count\"],\n",
    "                        \"avg_dist\": clusters[right_level - 1][right_cluster][\"avg_dist\"],\n",
    "                        \"dist_std\": clusters[right_level - 1][right_cluster][\"dist_std\"],\n",
    "                        \"samples\": clusters[right_level - 1][right_cluster][\"samples\"],\n",
    "                        \"sample_dist\": clusters[right_level - 1][right_cluster][\"sample_dist\"],\n",
    "                        \"outside_1-std\": False,\n",
    "                        \"outside_2-std\": False,\n",
    "                    }\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clusterer:\n",
    "    def __init__(self, dataset: np.ndarray, min_num_samples_per_cluster: Optional[int] = None):\n",
    "        self.min_cluster_size: Optional[int] = min_num_samples_per_cluster\n",
    "        self._on_init(dataset)\n",
    "\n",
    "    def _on_init(self, x):\n",
    "        self._data: np.ndarray = x\n",
    "        self.num_samples = len(x)\n",
    "        self.darr: np.ndarray = get_condensed_distance_array(x)\n",
    "        self.sqdmat: np.ndarray = get_square_distance_matrix(self.darr)\n",
    "        self.larr: np.ndarray = get_extended_linkage(self.darr)\n",
    "        self.max_clusters: int = np.count_nonzero(self.larr[:, 3] == 2)\n",
    "        self.last_merge_level: int = 1\n",
    "        self.min_num_samples_per_cluster: int = (\n",
    "            int(min(100, max(2, self.num_samples * 0.05))) if not self.min_cluster_size else self.min_cluster_size\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def data(self):\n",
    "        return self._data\n",
    "\n",
    "    @data.setter\n",
    "    def data(self, x: np.ndarray):\n",
    "        self._on_init(x)\n",
    "\n",
    "    def create_clusters(self) -> Dict[int, Any]:\n",
    "        \"\"\"Generates clusters based on linkage matrix\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict[str, Any]\n",
    "            Cluster information\n",
    "        \"\"\"\n",
    "        cluster_num = 0\n",
    "        cluster_tracking = 0\n",
    "        clusters = {}  # Dictionary to store clusters\n",
    "        tracking = {}  # Dictionary to associate new cluster ids with actual clusters\n",
    "\n",
    "        # Walking through the linkage array to generate clusters\n",
    "        for arr_i in self.larr:\n",
    "            level = 0\n",
    "\n",
    "            left_count = 0\n",
    "            right_count = 0\n",
    "            merged = False\n",
    "\n",
    "            arr_0 = int(arr_i[0])  # Grabbing the left id\n",
    "            arr_1 = int(arr_i[1])  # Grabbing the right id\n",
    "            dist = arr_i[2]  # Getting the distance between the left and right ids\n",
    "\n",
    "            new_sample = []\n",
    "            sample_dist = np.array([dist], dtype=np.float16)\n",
    "\n",
    "            # Linkage matrix first column id\n",
    "            left_id = tracking.get(arr_0)  # Determining if the id is already associated with a cluster\n",
    "            if left_id is None:\n",
    "                new_sample.append(arr_0)\n",
    "            else:\n",
    "                left_cluster = left_id[1]\n",
    "                left_level = left_id[0] + 1\n",
    "                left_count = clusters[left_id[0]][left_cluster][\"count\"]\n",
    "                left_sample = clusters[left_id[0]][left_cluster][\"samples\"]\n",
    "                sample_dist = np.concatenate([clusters[left_id[0]][left_cluster][\"sample_dist\"], sample_dist])\n",
    "\n",
    "            # Linkage matrix second column id\n",
    "            right_id = tracking.get(arr_1)  # Determining if the id is already associated with a cluster\n",
    "            if right_id is None:\n",
    "                new_sample.append(arr_1)\n",
    "            else:\n",
    "                right_cluster = right_id[1]\n",
    "                right_level = right_id[0] + 1\n",
    "                right_count = clusters[right_id[0]][right_cluster][\"count\"]\n",
    "                right_sample = clusters[right_id[0]][right_cluster][\"samples\"]\n",
    "                sample_dist = np.concatenate([clusters[right_id[0]][right_cluster][\"sample_dist\"], sample_dist])\n",
    "\n",
    "            # Aggregate samples, determine cluster number, and get the level\n",
    "            if left_id and right_id:\n",
    "                if left_count > right_count:\n",
    "                    samples = np.concatenate([left_sample, right_sample])\n",
    "                else:\n",
    "                    samples = np.concatenate([right_sample, left_sample])\n",
    "                cluster_num = min([left_cluster, right_cluster])\n",
    "                merged = max([left_cluster, right_cluster])\n",
    "                level = max([left_level, right_level])\n",
    "            elif left_id:\n",
    "                samples = np.concatenate([left_sample, new_sample])\n",
    "                cluster_num = left_cluster\n",
    "                level = left_level\n",
    "            elif right_id:\n",
    "                samples = np.concatenate([right_sample, new_sample])\n",
    "                cluster_num = right_cluster\n",
    "                level = right_level\n",
    "            else:\n",
    "                samples = np.array(new_sample, dtype=np.int32)\n",
    "                cluster_num = cluster_tracking\n",
    "\n",
    "            dist_avg = np.mean(sample_dist)\n",
    "            dist_std = np.std(sample_dist) if sample_dist.shape[0] > 1 else 1e-5\n",
    "\n",
    "            out1 = dist_avg + dist_std\n",
    "            out2 = out1 + dist_std\n",
    "\n",
    "            # Initialize the structure if not present\n",
    "            if level not in clusters:\n",
    "                clusters[level] = {\n",
    "                    cluster_num: {\n",
    "                        \"cluster_merged\": merged,\n",
    "                        \"count\": samples.shape[0],\n",
    "                        \"avg_dist\": dist_avg,\n",
    "                        \"dist_std\": dist_std,\n",
    "                        \"samples\": samples,\n",
    "                        \"sample_dist\": sample_dist,\n",
    "                        \"outside_1-std\": dist > out1,\n",
    "                        \"outside_2-std\": dist > out2,\n",
    "                    }\n",
    "                }\n",
    "            else:\n",
    "                clusters[level][cluster_num] = {\n",
    "                    \"cluster_merged\": merged,\n",
    "                    \"count\": samples.shape[0],\n",
    "                    \"avg_dist\": dist_avg,\n",
    "                    \"dist_std\": dist_std,\n",
    "                    \"samples\": samples,\n",
    "                    \"sample_dist\": sample_dist,\n",
    "                    \"outside_1-std\": dist > out1,\n",
    "                    \"outside_2-std\": dist > out2,\n",
    "                }\n",
    "\n",
    "            tracking[int(arr_i[-1])] = (level, cluster_num)  # Associates the new linkage id with the correct cluster\n",
    "\n",
    "            if not left_id and not right_id:\n",
    "                # Making sure that new clusters get unique numbers\n",
    "                cluster_tracking += 1\n",
    "            else:\n",
    "                # Fill missing cluster levels for continuity.\n",
    "                # Ensures all levels have consistent information across cluster changes.\n",
    "                clusters = fill_missing_cluster_level(left_id, right_id, level, clusters)\n",
    "\n",
    "            # Only tracking the levels in which clusters merge for the cluster distance matrix\n",
    "            if merged:\n",
    "                self.last_merge_level = max(self.last_merge_level, level + 1)\n",
    "\n",
    "        return clusters\n",
    "\n",
    "    def get_cluster_distances(self, clusters):\n",
    "        # this is the cluster distance matrix\n",
    "        cluster_matrix = np.full((self.last_merge_level, self.max_clusters, self.max_clusters), -1.0, dtype=np.float32)\n",
    "\n",
    "        for level, cluster_set in clusters.items():\n",
    "            if level < self.last_merge_level:\n",
    "                cluster_ids = sorted(cluster_set.keys())\n",
    "                for i, cluster_id in enumerate(cluster_ids):\n",
    "                    cluster_matrix[level, cluster_id, cluster_id] = clusters[level][cluster_id][\"avg_dist\"]\n",
    "                    for int_id in range(i + 1, len(cluster_ids)):\n",
    "                        compare_id = cluster_ids[int_id]\n",
    "                        sample_a = clusters[level][cluster_id][\"samples\"]\n",
    "                        sample_b = clusters[level][compare_id][\"samples\"]\n",
    "                        min_mat = self.sqdmat[np.ix_(sample_a, sample_b)].min()\n",
    "                        cluster_matrix[level, cluster_id, compare_id] = min_mat\n",
    "                        cluster_matrix[level, compare_id, cluster_id] = min_mat\n",
    "\n",
    "        return cluster_matrix\n",
    "\n",
    "    def get_merge_levels(self, clusters):\n",
    "        \"\"\"\n",
    "        Runs through the clusters dictionary determining when clusters merge,\n",
    "        and how close are those clusters when they merge.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        clusters:\n",
    "            A dictionary containing the original clusters information.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        merge_clusters:\n",
    "            A dictionary with each clusters merge history\n",
    "        \"\"\"\n",
    "\n",
    "        merge_clusters = {\"merge\": {}, \"likely_merge\": {}, \"no_merge\": {}}\n",
    "\n",
    "        for level, cluster_set in clusters.items():\n",
    "            cluster_ids = sorted(cluster_set.keys())\n",
    "            for i, cluster_id in enumerate(cluster_ids):\n",
    "                # Extract necessary information\n",
    "                samples = clusters[level][cluster_id][\"samples\"]\n",
    "                merged = clusters[level][cluster_id][\"cluster_merged\"]\n",
    "                out1 = clusters[level][cluster_id][\"outside_1-std\"]\n",
    "                out2 = clusters[level][cluster_id][\"outside_2-std\"]\n",
    "\n",
    "                if merged:\n",
    "                    if out2:\n",
    "                        if len(samples) < self.min_num_samples_per_cluster:\n",
    "                            if cluster_id not in merge_clusters[\"likely_merge\"]:\n",
    "                                merge_clusters[\"likely_merge\"][cluster_id] = {level: [merged, \"low\"]}\n",
    "                            if level not in merge_clusters[\"likely_merge\"][cluster_id]:\n",
    "                                merge_clusters[\"likely_merge\"][cluster_id][level] = [merged, \"low\"]\n",
    "                        else:\n",
    "                            if cluster_id not in merge_clusters[\"no_merge\"]:\n",
    "                                merge_clusters[\"no_merge\"][cluster_id] = {level: [merged]}\n",
    "                            if level not in merge_clusters[\"no_merge\"][cluster_id]:\n",
    "                                merge_clusters[\"no_merge\"][cluster_id][level] = [merged]\n",
    "\n",
    "                    elif out1 and len(samples) >= self.min_num_samples_per_cluster:\n",
    "                        if cluster_id not in merge_clusters[\"likely_merge\"]:\n",
    "                            merge_clusters[\"likely_merge\"][cluster_id] = {level: [merged]}\n",
    "                        if level not in merge_clusters[\"likely_merge\"][cluster_id]:\n",
    "                            merge_clusters[\"likely_merge\"][cluster_id][level] = [merged]\n",
    "\n",
    "                    else:\n",
    "                        if cluster_id not in merge_clusters[\"merge\"]:\n",
    "                            merge_clusters[\"merge\"][cluster_id] = {level: [merged]}\n",
    "                        if level not in merge_clusters[\"merge\"][cluster_id]:\n",
    "                            merge_clusters[\"merge\"][cluster_id][level] = [merged]\n",
    "\n",
    "        return merge_clusters\n",
    "\n",
    "    def generate_merge_list(self, cluster_merges, cluster_matrix):\n",
    "        merge_list, merge_mean, intra_max = self.cluster_merging(cluster_merges, cluster_matrix)\n",
    "        desired_merge, merge = self.get_desired_merge(merge_mean, intra_max)\n",
    "\n",
    "        j = 0\n",
    "        for i, select in enumerate(desired_merge):\n",
    "            if select:\n",
    "                merge_list[i].append(\"merge\")\n",
    "            else:\n",
    "                if merge[j]:\n",
    "                    merge_list[i].append(\"merge\")\n",
    "                else:\n",
    "                    merge_list[i].append(\"no-merge\")\n",
    "                j += 1\n",
    "\n",
    "        merge_list = sorted(merge_list, reverse=True)\n",
    "        return merge_list\n",
    "\n",
    "    def cluster_merging(self, cluster_merges, cluster_matrix):\n",
    "        intra_max = []\n",
    "        merge_mean = []\n",
    "        merge_list = []\n",
    "        # Process each merge type\n",
    "        for merge_type, merge_clusters in cluster_merges.items():\n",
    "            for outer_cluster, inner_clusters in merge_clusters.items():\n",
    "                for level, cluster_list in inner_clusters.items():\n",
    "                    inner_cluster = cluster_list[0]\n",
    "\n",
    "                    # Get the slice of the distance matrix up to the level before merging\n",
    "                    distances = cluster_matrix[:level, outer_cluster, inner_cluster]\n",
    "                    intra_distance = cluster_matrix[:, outer_cluster, outer_cluster]\n",
    "                    mask = intra_distance >= 0\n",
    "                    intra_filtered = intra_distance[mask]\n",
    "                    intra_max.append(np.max(intra_filtered))\n",
    "\n",
    "                    # Grabbing the corresponding desired values\n",
    "                    if merge_type == \"merge\":\n",
    "                        merge_mean.append(np.max(distances))\n",
    "                    else:\n",
    "                        merge_mean.append(np.mean(distances))\n",
    "\n",
    "                    merge_list.append([level, outer_cluster, inner_cluster])\n",
    "\n",
    "        return merge_list, merge_mean, intra_max\n",
    "\n",
    "    def get_desired_merge(self, merge_mean, intra_max):\n",
    "        intra_max = np.unique(intra_max)\n",
    "        intra_value = np.log(intra_max)\n",
    "        intra_value = intra_value.mean() + 2 * intra_value.std()\n",
    "        merge_value = np.log(merge_mean)\n",
    "        desired_merge = merge_value < intra_value\n",
    "\n",
    "        check = merge_value[~desired_merge]\n",
    "        check = np.abs((check - intra_value) / intra_value)\n",
    "        mask = check < 1\n",
    "        good = check[mask].mean() + check[mask].std()\n",
    "        merge = check < good\n",
    "        return desired_merge, merge\n",
    "\n",
    "    def get_last_merge_levels(self, merge_list):\n",
    "        last_good_merge_levels = {}\n",
    "        for entry in merge_list:\n",
    "            level, outer_cluster, inner_cluster, status = entry\n",
    "            if status == \"no-merge\":\n",
    "                if outer_cluster not in last_good_merge_levels:\n",
    "                    last_good_merge_levels[outer_cluster] = 1\n",
    "                if inner_cluster not in last_good_merge_levels:\n",
    "                    last_good_merge_levels[inner_cluster] = 1\n",
    "                if last_good_merge_levels[outer_cluster] > level:\n",
    "                    last_good_merge_levels[outer_cluster] = level - 1\n",
    "            else:\n",
    "                if outer_cluster in last_good_merge_levels:\n",
    "                    last_good_merge_levels[outer_cluster] = max(last_good_merge_levels[outer_cluster], level)\n",
    "        return last_good_merge_levels\n",
    "\n",
    "    def find_duplicates(self, dedup_std_list):\n",
    "        diag_mask = np.ones(self.sqdmat.shape, dtype=bool)\n",
    "        np.fill_diagonal(diag_mask, 0)\n",
    "        diag_mask = np.triu(diag_mask)\n",
    "\n",
    "        exact_mask = self.sqdmat < (np.mean(dedup_std_list) / 100)\n",
    "        exact_indices = np.nonzero(exact_mask & diag_mask)\n",
    "        exact_dedup = list(zip(exact_indices[0], exact_indices[1]))\n",
    "\n",
    "        possible_mask = self.sqdmat < np.mean(dedup_std_list)\n",
    "        possible_indices = np.nonzero(possible_mask & diag_mask & ~exact_mask)\n",
    "        possible_dedup = list(zip(possible_indices[0], possible_indices[1]))\n",
    "\n",
    "        return exact_dedup, possible_dedup\n",
    "\n",
    "    def find_outliers(self, clusters, last_merge_levels):\n",
    "        \"\"\"\n",
    "        The clusters dictionary contains whether the added sample/cluster\n",
    "        was outside 1 standard deviation or outside 2 standard deviations.\n",
    "        last_merge_levels contains the last good merge for each cluster we care about\n",
    "        Using this information to determine when the sample was added to the cluster\n",
    "        and how far it was from the cluster when it was added\n",
    "\n",
    "        \"\"\"\n",
    "        outliers = []\n",
    "        possible_outliers = []\n",
    "\n",
    "        for level, cluster_set in clusters.items():\n",
    "            cluster_ids = sorted(cluster_set.keys())\n",
    "            for cluster_id in cluster_ids:\n",
    "                # Extract necessary information\n",
    "                samples = clusters[level][cluster_id][\"samples\"]\n",
    "                merged = clusters[level][cluster_id][\"cluster_merged\"]\n",
    "                out1 = clusters[level][cluster_id][\"outside_1-std\"]\n",
    "                out2 = clusters[level][cluster_id][\"outside_2-std\"]\n",
    "\n",
    "                if cluster_id in last_merge_levels and not merged:\n",
    "                    if level > last_merge_levels[cluster_id] and out2:\n",
    "                        outliers.append(samples[-1])\n",
    "                    elif (\n",
    "                        level > last_merge_levels[cluster_id]\n",
    "                        and out1\n",
    "                        and len(samples) >= self.min_num_samples_per_cluster\n",
    "                    ):\n",
    "                        possible_outliers.append(samples[-1])\n",
    "        return outliers, possible_outliers\n",
    "\n",
    "    def run(self):\n",
    "        sample_info = self.create_clusters()\n",
    "\n",
    "        if self.max_clusters > 1:\n",
    "            cluster_matrix = self.get_cluster_distances(sample_info)\n",
    "            merge_levels = self.get_merge_levels(sample_info)\n",
    "            merge_list = self.generate_merge_list(merge_levels, cluster_matrix)\n",
    "            last_merge_levels = self.get_last_merge_levels(merge_list)\n",
    "        else:\n",
    "            last_merge_levels = {0: int(max(self.num_samples * 0.1, self.min_num_samples_per_cluster))}\n",
    "\n",
    "        outliers, potential_outliers = self.find_outliers(sample_info, last_merge_levels)\n",
    "\n",
    "        dedup_std = []\n",
    "        for cluster, level in last_merge_levels.items():\n",
    "            level_cluster = sample_info[level][cluster]\n",
    "            samples = level_cluster[\"samples\"]\n",
    "            if samples.shape[0] < self.min_num_samples_per_cluster:\n",
    "                outliers.extend(samples.tolist())\n",
    "            else:\n",
    "                dedup_std.append(level_cluster[\"dist_std\"])\n",
    "\n",
    "        duplicates, near_duplicates = self.find_duplicates(dedup_std)\n",
    "\n",
    "        ret = {\n",
    "            \"outliers\": outliers,\n",
    "            \"potential_outliers\": potential_outliers,\n",
    "            \"duplicates\": duplicates,\n",
    "            \"near_duplicates\": near_duplicates,\n",
    "        }\n",
    "\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kwds = {\"alpha\": 0.5, \"s\": 50, \"linewidths\": 0}\n",
    "\n",
    "# moons, _ = dsets.make_moons(n_samples=50, noise=0.1)\n",
    "blobs, _ = dsets.make_blobs(  # type: ignore\n",
    "    n_samples=100,\n",
    "    centers=[(-1.5, 1.8), (-1, 3), (0.8, 2.1), (2.8, 1.5), (2.5, 3.5)],  # type: ignore\n",
    "    cluster_std=0.3,\n",
    "    random_state=33,\n",
    ")\n",
    "# test_data = np.vstack([moons, blobs])\n",
    "test_data = blobs\n",
    "test_data[79] = test_data[24]\n",
    "test_data[63] = test_data[58] + 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shaun's code for testing the clusterer\n",
    "def test_outliers(x):\n",
    "    assert len(x) == 6\n",
    "    for val in x:\n",
    "        assert val in [21, 6, 4, 71, 38, 11]\n",
    "    print(\"Passed\")\n",
    "\n",
    "\n",
    "def test_potential_outliers(x):\n",
    "    assert len(x) == 5\n",
    "    for val in x:\n",
    "        assert val in [42, 48, 9, 1, 43]\n",
    "    print(\"Passed\")\n",
    "\n",
    "\n",
    "def test_duplicates(x):\n",
    "    assert x == [(24, 79), (58, 63)]\n",
    "    print(\"Passed\")\n",
    "\n",
    "\n",
    "def test_duplicates_new(x):\n",
    "    \"\"\"The new clusterer uses List of lists instead of List of sets\"\"\"\n",
    "    assert x == [[24, 79], [58, 63]]\n",
    "    print(\"Passed\")\n",
    "\n",
    "\n",
    "def test_near_duplicates(x):\n",
    "    assert x == [\n",
    "        (8, 27),\n",
    "        (10, 65),\n",
    "        (16, 99),\n",
    "        (19, 64),\n",
    "        (22, 87),\n",
    "        (27, 29),\n",
    "        (33, 76),\n",
    "        (39, 55),\n",
    "        (40, 72),\n",
    "        (41, 62),\n",
    "        (80, 81),\n",
    "        (80, 93),\n",
    "        (81, 93),\n",
    "        (87, 95),\n",
    "    ]\n",
    "    print(\"Passed\")\n",
    "\n",
    "\n",
    "def test_near_duplicates_new(x):\n",
    "    \"\"\"The new clusterer groups overlapping indices\"\"\"\n",
    "    assert x == [\n",
    "        [8, 27, 29],\n",
    "        [10, 65],\n",
    "        [16, 99],\n",
    "        [19, 64],\n",
    "        [22, 87, 95],\n",
    "        [33, 76],\n",
    "        [39, 55],\n",
    "        [40, 72],\n",
    "        [41, 62],\n",
    "        [80, 81, 93],\n",
    "    ]\n",
    "    print(\"Passed\")\n",
    "\n",
    "\n",
    "def run_tests_original(x):\n",
    "    test_outliers(x[\"outliers\"])\n",
    "    test_potential_outliers(x[\"potential_outliers\"])\n",
    "    test_duplicates(x[\"duplicates\"])\n",
    "    test_near_duplicates(x[\"near_duplicates\"])\n",
    "\n",
    "\n",
    "def run_tests_new(x):\n",
    "    test_outliers(x[\"outliers\"])\n",
    "    test_potential_outliers(x[\"potential_outliers\"])\n",
    "    test_duplicates_new(x[\"duplicates\"])\n",
    "    test_near_duplicates_new(x[\"near_duplicates\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passed\n",
      "Passed\n",
      "Passed\n",
      "Passed\n",
      "Passed\n",
      "Passed\n",
      "Passed\n",
      "Passed\n"
     ]
    }
   ],
   "source": [
    "reload(cl)\n",
    "\n",
    "clusterer = Clusterer(test_data)\n",
    "clusterer2 = cl.Clusterer(test_data)\n",
    "\n",
    "x1 = clusterer.run()\n",
    "x2 = clusterer2.evaluate()\n",
    "\n",
    "# Test they give the same outcomes\n",
    "run_tests_original(x1)\n",
    "run_tests_new(x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timing Similar Processes\n",
    "\n",
    "A process is determined by the IO for a function or group of functions which are shared between the two Clusterer classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since timeit does give outputs, some outputs must be pre-calculated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = Clusterer(test_data)\n",
    "clusterer2 = cl.Clusterer(test_data)\n",
    "\n",
    "# Clusterer pre-computed outputs\n",
    "info = clusterer.create_clusters()\n",
    "cluster_matrix = clusterer.get_cluster_distances(info)\n",
    "merge_levels = clusterer.get_merge_levels(info)\n",
    "merge_list = clusterer.generate_merge_list(merge_levels, cluster_matrix)\n",
    "last_merge_levels = clusterer.get_last_merge_levels(merge_list)\n",
    "dedup_std = []\n",
    "for cluster, level in last_merge_levels.items():\n",
    "    level_cluster = info[level][cluster]\n",
    "    samples = level_cluster[\"samples\"]\n",
    "    if samples.shape[0] >= clusterer.min_num_samples_per_cluster:\n",
    "        dedup_std.append(level_cluster[\"dist_std\"])\n",
    "\n",
    "# Integrated Clusterer pre-computed outputs\n",
    "last_merge_levels2 = clusterer2.last_good_merge_levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Individual Timings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>\tCreate Clusters\t<<<<<\n",
      "7 ms ± 229 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
      "864 ns ± 25.4 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n",
      ">>>>>\tOriginal Clusterer Last Merge Levels\t<<<<<\n",
      "31.2 ms ± 2.51 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "162 µs ± 10.1 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "919 µs ± 21.8 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "13.2 µs ± 180 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n",
      ">>>>>\tIntegrated Clusterer Last Merge Levels\t<<<<<\n",
      "30.3 ms ± 888 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      ">>>>>\tOutliers\t<<<<<\n",
      "132 µs ± 2.48 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "57.9 µs ± 1.06 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      ">>>>>\tDuplicates\t<<<<<\n",
      "156 µs ± 2.58 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "233 µs ± 7.3 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "print(\">>>>>\\tCreate Clusters\\t<<<<<\")\n",
    "%timeit clusterer.create_clusters()\n",
    "%timeit clusterer2.clusters\n",
    "print(\">>>>>\\tOriginal Clusterer Last Merge Levels\\t<<<<<\")\n",
    "%timeit clusterer.get_cluster_distances(info)\n",
    "%timeit clusterer.get_merge_levels(info)\n",
    "%timeit clusterer.generate_merge_list(merge_levels, cluster_matrix)\n",
    "%timeit clusterer.get_last_merge_levels(merge_list)\n",
    "print(\">>>>>\\tIntegrated Clusterer Last Merge Levels\\t<<<<<\")\n",
    "%timeit clusterer2._get_last_merge_levels()\n",
    "print(\">>>>>\\tOutliers\\t<<<<<\")\n",
    "%timeit clusterer.find_outliers(info, last_merge_levels)\n",
    "%timeit clusterer2.find_outliers(last_merge_levels2)\n",
    "print(\">>>>>\\tDuplicates\\t<<<<<\")\n",
    "%timeit clusterer.find_duplicates(dedup_std)\n",
    "%timeit clusterer2.find_duplicates(last_merge_levels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timing Clusterer Init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123 µs ± 2.25 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "122 µs ± 3.36 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Test the __init__ times\n",
    "%timeit Clusterer(test_data)\n",
    "%timeit cl.Clusterer(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timing Full Clusterer Workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original clusterer workflow\n",
      "21.2 ms ± 1.73 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "Integrated clusterer workflow\n",
      "20.3 ms ± 750 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "print(\"Original clusterer workflow\")\n",
    "%timeit Clusterer(test_data).run()\n",
    "print(\"Integrated clusterer workflow\")\n",
    "%timeit cl.Clusterer(test_data).evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
