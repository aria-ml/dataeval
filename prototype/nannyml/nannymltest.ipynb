{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Messy test code based on https://nannyml.readthedocs.io/en/stable/quick.html\n",
    "\n",
    "import nannyml as nml\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_df, analysis_df, _ = nml.load_us_census_ma_employment_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(reference_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(analysis_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = nml.CBPE(\n",
    "    problem_type=\"classification_binary\",\n",
    "    y_pred_proba=\"predicted_probability\",\n",
    "    y_pred=\"prediction\",\n",
    "    y_true=\"employed\",\n",
    "    metrics=[\"roc_auc\"],\n",
    "    chunk_size=chunk_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = estimator.fit(reference_df)\n",
    "estimated_performance = estimator.estimate(analysis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = estimated_performance.plot()\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "probs = np.random.rand(100)\n",
    "preds = np.round(probs)\n",
    "truth = np.array([0, 1] * 50)\n",
    "test_dict = {\"id\": list(range(100)), \"prediction\": preds, \"predicted_probability\": probs, \"employed\": truth}\n",
    "test_df = pd.DataFrame(test_dict)\n",
    "\n",
    "new_probs = np.random.rand(100)\n",
    "new_preds = np.round(probs)\n",
    "new_dict = {\"id\": list(range(100)), \"prediction\": new_preds, \"predicted_probability\": new_probs}\n",
    "new_df = pd.DataFrame(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_base = nml.CBPE(\n",
    "    problem_type=\"classification_binary\",\n",
    "    y_pred_proba=\"predicted_probability\",\n",
    "    y_pred=\"prediction\",\n",
    "    y_true=\"employed\",\n",
    "    metrics=[\"accuracy\"],\n",
    "    chunk_size=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e2 = estimator_base.fit(test_df)\n",
    "ep2 = e2.estimate(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = ep2.plot()\n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edf = ep2.to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(edf[\"accuracy\"][\"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from typing import Dict, cast\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms.v2 as v2\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "\n",
    "np.random.seed(0)\n",
    "np.set_printoptions(formatter={\"float\": lambda x: f\"{x:0.4f}\"})\n",
    "torch.manual_seed(0)\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "random.seed(0)\n",
    "torch.use_deterministic_algorithms(True)\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "torch._dynamo.disable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the mnist dataset and preview the images\n",
    "to_tensor = v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])\n",
    "train_ds = datasets.MNIST(\"./data\", train=True, download=True, transform=to_tensor)\n",
    "test_ds = datasets.MNIST(\"./data\", train=False, download=True, transform=to_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 3))\n",
    "\n",
    "for lbl in range(10):\n",
    "    i = (train_ds.targets == lbl).nonzero()[0][0]\n",
    "    img = train_ds.data[i]\n",
    "    ax = fig.add_subplot(2, 5, lbl + 1)\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.yaxis.set_visible(False)\n",
    "    ax.imshow(img, cmap=\"gray_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Contrast(v2.Transform):\n",
    "    # def __init__(self, severity=4):\n",
    "    #    self.severity = severity\n",
    "\n",
    "    def _transform(self, inpt, params):\n",
    "        # return F.to_image(inpt)\n",
    "        return self.contrast(inpt)\n",
    "\n",
    "    def contrast(self, sample):\n",
    "        severity = 4\n",
    "        x = sample  # , landmarks = sample[\"image\"], sample[\"landmarks\"]\n",
    "        # x = x * 0\n",
    "        # return x\n",
    "        # c = [0.4, 0.3, 0.2, 0.1, 0.05][severity - 1]\n",
    "        c = 0.3#100\n",
    "\n",
    "        # x = np.array(x) / 255.0\n",
    "        x = x.float() / 255.0\n",
    "        means = torch.mean(x, axis=(0, 1), keepdims=True)\n",
    "        #x = torch.clip((x - means) * c + means, 0, 1) * 255\n",
    "        #x = torch.clip(x + x * torch.normal(size=x.shape, scale=c), 0, 1) * 255\n",
    "        rands = torch.normal(x, std=c)\n",
    "        x = torch.clip(rands, 0, 1)\n",
    "        # return {\"image\": x, \"landmarks\": landmarks}\n",
    "        return x\n",
    "\n",
    "\n",
    "c_to_tensor = v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True), Contrast()])\n",
    "c_contrast = v2.Compose([Contrast()])\n",
    "\n",
    "c_test_ds = datasets.MNIST(\"./data\", train=False, download=True, transform=c_to_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 3))\n",
    "\n",
    "c_loader = DataLoader(c_test_ds, batch_size=len(c_test_ds), shuffle=False)\n",
    "\n",
    "# for c_batch in c_loader:\n",
    "for lbl in range(10):\n",
    "    # data, targets = c_batch\n",
    "    i = (c_test_ds.targets == lbl).nonzero()[0][0]\n",
    "    img = c_contrast(c_test_ds.data[i])\n",
    "    # i = (targets == lbl).nonzero()[0][0]\n",
    "    # img = data[i]\n",
    "    ax = fig.add_subplot(2, 5, lbl + 1)\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.yaxis.set_visible(False)\n",
    "    ax.imshow(img, cmap=\"gray_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a subset of 2000 training images and 500 test images\n",
    "train_ds = Subset(train_ds, range(2000))\n",
    "test_ds = Subset(test_ds, range(500))\n",
    "c_test_ds = Subset(c_test_ds, range(500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(6400, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model = torch.compile(Net().to(device))\n",
    "\n",
    "# Type cast the model back to Net as torch.compile returns a Unknown\n",
    "# Nothing internally changes from the cast; we are simply signaling the type\n",
    "model = cast(Net, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_train(model: nn.Module, dataset: Dataset):\n",
    "    # Defined only for this testing scenario\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    epochs = 10\n",
    "\n",
    "    # Define the dataloader for training\n",
    "    dataloader = DataLoader(dataset, batch_size=16)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch in dataloader:\n",
    "            # Load data/images to device\n",
    "            X = torch.Tensor(batch[0]).to(device)\n",
    "            # Load targets/labels to device\n",
    "            y = torch.Tensor(batch[1]).to(device)\n",
    "            # Zero out gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward propagation\n",
    "            outputs = model(X)\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, y)\n",
    "            # Back prop\n",
    "            loss.backward()\n",
    "            # Update weights/parameters\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "def custom_eval(model: nn.Module, dataset: Dataset) -> Dict[str, list]:\n",
    "    # metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "    # result = 0\n",
    "    # batch_dicts = []\n",
    "    # metric = torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(device)\n",
    "\n",
    "    # dict_out = {\"conf\": np.zeros(0), \"preds\": np.zeros(0), \"ground_truth\": np.zeros(0)}\n",
    "    dict_out = {\"y_pred\": np.zeros(0, dtype=int), \"y\": np.zeros(0, dtype=int)}\n",
    "    for i in range(10):\n",
    "        dict_out[f\"y_pred_proba_{i}\"] = np.zeros(0)\n",
    "\n",
    "    # Set model layers into evaluation mode\n",
    "    model.eval()\n",
    "    dataloader = DataLoader(dataset, batch_size=16)\n",
    "    # Tell PyTorch to not track gradients, greatly speeds up processing\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Load data/images to device\n",
    "            X = torch.Tensor(batch[0]).to(device)\n",
    "            # Load targets/labels to device\n",
    "            y = torch.Tensor(batch[1]).int()\n",
    "            output = model(X).cpu()\n",
    "            processed_output = torch.max(output, dim=1)\n",
    "            confs = processed_output[0]\n",
    "            preds = np.int64(processed_output[1])\n",
    "\n",
    "            # batch_dict = {\"conf\": confs, \"preds\": preds, \"ground_truth\": y}\n",
    "            # dict_out[\"conf\"] = np.concatenate((dict_out[\"conf\"], confs))\n",
    "            dict_out[\"y_pred\"] = np.concatenate((dict_out[\"y_pred\"], preds), dtype=int)\n",
    "            dict_out[\"y\"] = np.concatenate((dict_out[\"y\"], y), dtype=int)\n",
    "            for i in range(10):\n",
    "                key = f\"y_pred_proba_{i}\"\n",
    "                dict_out[key] = np.concatenate((dict_out[key], output[:, i]))\n",
    "\n",
    "            # metric.update(preds, y)\n",
    "        # result = metric.compute().cpu()\n",
    "    # return {\"Accuracy\": result}\n",
    "    return dict_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_parameters(model: nn.Module):\n",
    "    \"\"\"\n",
    "    Re-initializes each layer in the model using\n",
    "    the layer's defined weight_init function\n",
    "    \"\"\"\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def weight_reset(m: nn.Module):\n",
    "        # Check if the current module has reset_parameters\n",
    "        reset_parameters = getattr(m, \"reset_parameters\", None)\n",
    "        if callable(reset_parameters):\n",
    "            m.reset_parameters()  # type: ignore\n",
    "\n",
    "    # Applies fn recursively to every submodule see:\n",
    "    # https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n",
    "    return model.apply(fn=weight_reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the network weights to \"create\" an untrained model\n",
    "model = reset_parameters(model)\n",
    "# Run the model with each substep of data\n",
    "# train on subset of train data\n",
    "train_kwargs = {}\n",
    "eval_kwargs = {}\n",
    "custom_train(\n",
    "    model,\n",
    "    train_ds,\n",
    "    **train_kwargs,\n",
    ")\n",
    "\n",
    "# evaluate on test data\n",
    "train_dict = custom_eval(model, train_ds, **eval_kwargs)\n",
    "train_df = pd.DataFrame(train_dict)\n",
    "\n",
    "test_dict = custom_eval(model, test_ds, **eval_kwargs)\n",
    "test_df = pd.DataFrame(test_dict)\n",
    "\n",
    "c_test_dict = custom_eval(model, c_test_ds, **eval_kwargs)\n",
    "c_test_df = pd.DataFrame(c_test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(c_test_df[\"y_pred\"] == c_test_df[\"y\"])/500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nannyml as nml\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_keys = {}\n",
    "for i in range(10):\n",
    "    y_pred_keys[i] = f\"y_pred_proba_{i}\"\n",
    "print(y_pred_keys)\n",
    "\n",
    "estimator_base = nml.CBPE(\n",
    "    problem_type=\"classification_multiclass\",\n",
    "    y_pred_proba=y_pred_keys,\n",
    "    y_pred=\"y_pred\",\n",
    "    y_true=\"y\",\n",
    "    metrics=[\"accuracy\"],\n",
    "    chunk_size=50,  # 100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator_base.fit(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = estimator_base.estimate(c_test_df)  # change to c_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results.filter(period=\"analysis\").to_df()\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_accuracy = np.mean(results_df['accuracy']['value'])\n",
    "alert = np.any(results_df['accuracy']['alert'])\n",
    "\n",
    "print(f\"Predicted accuracy: {pred_accuracy}\")\n",
    "print(f\"Action recommended: {'yes' if alert else 'no'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
