{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MNIST Data\n",
    "import hashlib\n",
    "import os\n",
    "import typing\n",
    "from urllib.error import HTTPError, URLError\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "\n",
    "def download_mnist() -> str:\n",
    "    \"\"\"Code to download mnist originates from keras/datasets:\n",
    "\n",
    "    https://github.com/keras-team/keras/blob/v2.15.0/keras/datasets/mnist.py#L25-L86\n",
    "    \"\"\"\n",
    "    origin_folder = \"https://storage.googleapis.com/tensorflow/tf-keras-datasets/\"\n",
    "    path = _get_file(\n",
    "        \"mnist.npz\",\n",
    "        origin=origin_folder + \"mnist.npz\",\n",
    "        file_hash=(\"731c5ac602752760c8e48fbffcf8c3b850d9dc2a2aedcf2cc48468fc17b673d1\"),\n",
    "    )\n",
    "\n",
    "    return path\n",
    "\n",
    "\n",
    "def _get_file(\n",
    "    fname: str,\n",
    "    origin: str,\n",
    "    file_hash: typing.Optional[str] = None,\n",
    "):\n",
    "    cache_dir = os.path.join(os.path.expanduser(\"~\"), \".keras\")\n",
    "    datadir_base = os.path.expanduser(cache_dir)\n",
    "    if not os.access(datadir_base, os.W_OK):\n",
    "        datadir_base = os.path.join(\"/tmp\", \".keras\")\n",
    "    datadir = os.path.join(datadir_base, \"datasets\")\n",
    "    os.makedirs(datadir, exist_ok=True)\n",
    "\n",
    "    fname = os.fspath(fname) if isinstance(fname, os.PathLike) else fname\n",
    "    fpath = os.path.join(datadir, fname)\n",
    "\n",
    "    download = False\n",
    "    if os.path.exists(fpath):\n",
    "        if file_hash is not None and not _validate_file(fpath, file_hash):\n",
    "            download = True\n",
    "    else:\n",
    "        download = True\n",
    "\n",
    "    if download:\n",
    "        try:\n",
    "            error_msg = \"URL fetch failure on {}: {} -- {}\"\n",
    "            try:\n",
    "                urlretrieve(origin, fpath)\n",
    "            except HTTPError as e:\n",
    "                raise Exception(error_msg.format(origin, e.code, e.msg)) from e\n",
    "            except URLError as e:\n",
    "                raise Exception(error_msg.format(origin, e.errno, e.reason)) from e\n",
    "        except (Exception, KeyboardInterrupt):\n",
    "            if os.path.exists(fpath):\n",
    "                os.remove(fpath)\n",
    "            raise\n",
    "\n",
    "        if os.path.exists(fpath) and file_hash is not None and not _validate_file(fpath, file_hash):\n",
    "            raise ValueError(\n",
    "                \"Incomplete or corrupted file detected. \"\n",
    "                f\"The sha256 file hash does not match the provided value \"\n",
    "                f\"of {file_hash}.\",\n",
    "            )\n",
    "    return fpath\n",
    "\n",
    "\n",
    "def _validate_file(fpath, file_hash, chunk_size=65535):\n",
    "    hasher = hashlib.sha256()\n",
    "    with open(fpath, \"rb\") as fpath_file:\n",
    "        for chunk in iter(lambda: fpath_file.read(chunk_size), b\"\"):\n",
    "            hasher.update(chunk)\n",
    "\n",
    "    return str(hasher.hexdigest()) == str(file_hash)\n",
    "\n",
    "\n",
    "mnist_path = download_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create\n",
    "rng = np.random.default_rng(33)\n",
    "size = 1000\n",
    "\n",
    "with np.load(mnist_path, allow_pickle=True) as fp:\n",
    "    test_images, labels = fp[\"x_train\"][:size], fp[\"y_train\"][:size]\n",
    "\n",
    "norm_test_imgs = np.repeat(test_images[:, np.newaxis, :, :], 3, axis=1) / 255\n",
    "jitter = rng.integers(10, size=norm_test_imgs.shape)\n",
    "norm_test_imgs += jitter\n",
    "\n",
    "\n",
    "rng.shuffle(test_images)\n",
    "rng.shuffle(norm_test_imgs)\n",
    "\n",
    "print(test_images.shape)\n",
    "print(norm_test_imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import daml._internal.metrics._stats as _stats\n",
    "\n",
    "reload(_stats)\n",
    "\n",
    "# test_images\n",
    "# norm_test_imgs\n",
    "d = _stats.ImageStats(norm_test_imgs)\n",
    "# print(d.stats)\n",
    "print(list(d.stats.keys()))\n",
    "\n",
    "c = _stats.ChannelStats(norm_test_imgs)\n",
    "print(list(c.stats.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.stats[\"idx_map\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.stats[\"skew\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
