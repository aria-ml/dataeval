{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial on exploring out-of-distribution examples using metadata.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Problem Statement_\n",
    "\n",
    "In computer vision tasks like **image classification** and **object detection**, when OOD examples are detected, there are two things we might like to know about them. First of all, looking at each example as an individual, what makes it stand out from the reference dataset? Second, as a population, what about them has shifted the most relative to the reference dataset? Metadata can help address both of these things.\n",
    "\n",
    "For the first, we can look at the values of each metadata feature of each example, and find which lies furthest out in the tails of the reference distribution. Since we are interested in extreme values, we should take the median of each reference feature, and then find the absolute deviation of each OOD example's from that median. (We can store the sign if we wish, but we should evaluate significance in terms of absolute deviation). In order to compare between features, we will normalize deviations by the inter-quartile range of each feature's reference distribution.\n",
    "\n",
    "For the second, we can compare the distribution of each feature to the reference using the Kolmogorov-Smirnov test. For features which show a statistically significant difference, we can use the Wasserstein to measure it (again, normalized by the IQR of the reference).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _When to use_\n",
    "\n",
    "When OOD examples have been detected, e.g. by the DataEval `OOD_AE` class or similar, the tools developed here should be used to try to learn more about specifically what image properties move each image out of the distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _What you will need_\n",
    "\n",
    "1. A training image dataset with low percentage of known OOD images.\n",
    "2. A test image dataset to evaluate for OOD images.\n",
    "3. A python environment with the following packages installed:\n",
    "   - `dataeval[all]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Setting up_\n",
    "\n",
    "Let's import the required libraries needed to set up a minimal working example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from metadata_utils import InstanceMNIST\n",
    "from metadata_utils import collate_fn_2 as collate_fn\n",
    "\n",
    "from dataeval.detectors.ood.ae_torch import OOD_AE\n",
    "from dataeval.utils.torch.models import AE_torch\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from dataeval.utils.torch.datasets import MNIST\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "We will use InstanceMNIST, a PyTorch wrapper for the TensorFlow MNIST datasets, fgor this tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corruption_list = ['identity', 'identity', 'translate', 'shot_noise', 'motion_blur', 'scale']\n",
    "\n",
    "mnist = InstanceMNIST(corruption_list, size=8000)\n",
    "mnist_val = InstanceMNIST('identity', train=False, size=8000)\n",
    "\n",
    "refdata = mnist.identity\n",
    "valdata = mnist_val.identity\n",
    "shiftdata = mnist.translate\n",
    "spikydata = mnist.shot_noise\n",
    "blurdata = mnist.motion_blur\n",
    "scaledata = mnist.scale\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_batch_size = 2000\n",
    "collate_fn = collate_fn\n",
    "\n",
    "refbb = DataLoader(refdata, collate_fn=collate_fn, batch_size=big_batch_size)\n",
    "for images, labels, metadata in refbb:\n",
    "    break\n",
    "\n",
    "# Now adjust shape of images array. \n",
    "input_shape = (*images[0].shape, 1)\n",
    "bbshape = (*images.shape,1)\n",
    "images = images.reshape(bbshape).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load in the training mnist dataset and use the first 4000\n",
    "train_ds = MNIST(root=\"./data/\", train=True, download=True, size=4000, unit_interval=True, channels=\"channels_first\")\n",
    "\n",
    "# Split out the images and labels\n",
    "images, labels = train_ds.data, train_ds.targets\n",
    "input_shape = images[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the model\n",
    "\n",
    "Now, lets look at how to use DataEval's OOD detection methods.  \n",
    "We will focus on a simple autoencoder network from our Alibi Detect provider.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (1, 28, 28) # n_channels, n_rows, n_columns\n",
    "detectors = [\n",
    "    OOD_AE(AE_torch(input_shape))] # implement as list to make it easy to try additional detectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "\n",
    "Next we will train a model on the dataset.\n",
    "For better results, the epochs can be increased.\n",
    "We set the threshold to detect the most extreme 1% of training data as out-of-distribution. (Training may take several minutes.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training OOD_AE...\n",
      "Epoch 0...\n",
      "loss: 0.111, |grad|: 0.176\n",
      "loss: 0.025, |grad|: 0.207\n",
      "loss: 0.046, |grad|: 0.210\n",
      "loss: 0.022, |grad|: 0.160\n",
      "loss: 0.013, |grad|: 0.104\n",
      "loss: 0.012, |grad|: 0.108\n",
      "loss: 0.011, |grad|: 0.141\n",
      "loss: 0.016, |grad|: 0.095\n",
      "Epoch 1...\n",
      "loss: 0.013, |grad|: 0.110\n",
      "loss: 0.013, |grad|: 0.135\n",
      "loss: 0.024, |grad|: 0.205\n",
      "loss: 0.012, |grad|: 0.118\n",
      "loss: 0.009, |grad|: 0.089\n",
      "loss: 0.008, |grad|: 0.079\n",
      "loss: 0.005, |grad|: 0.062\n",
      "loss: 0.016, |grad|: 0.083\n",
      "Epoch 2...\n",
      "loss: 0.011, |grad|: 0.101\n",
      "loss: 0.009, |grad|: 0.115\n",
      "loss: 0.018, |grad|: 0.173\n",
      "loss: 0.009, |grad|: 0.093\n",
      "loss: 0.007, |grad|: 0.079\n",
      "loss: 0.007, |grad|: 0.090\n",
      "loss: 0.004, |grad|: 0.058\n",
      "loss: 0.014, |grad|: 0.083\n",
      "Epoch 3...\n",
      "loss: 0.009, |grad|: 0.085\n",
      "loss: 0.006, |grad|: 0.077\n",
      "loss: 0.010, |grad|: 0.102\n",
      "loss: 0.007, |grad|: 0.079\n",
      "loss: 0.006, |grad|: 0.072\n",
      "loss: 0.005, |grad|: 0.059\n",
      "loss: 0.004, |grad|: 0.079\n",
      "loss: 0.013, |grad|: 0.082\n",
      "Epoch 4...\n",
      "loss: 0.009, |grad|: 0.094\n",
      "loss: 0.006, |grad|: 0.084\n",
      "loss: 0.010, |grad|: 0.104\n",
      "loss: 0.006, |grad|: 0.073\n",
      "loss: 0.005, |grad|: 0.094\n",
      "loss: 0.006, |grad|: 0.115\n",
      "loss: 0.003, |grad|: 0.046\n",
      "loss: 0.014, |grad|: 0.108\n",
      "Epoch 5...\n",
      "loss: 0.007, |grad|: 0.085\n",
      "loss: 0.005, |grad|: 0.059\n",
      "loss: 0.009, |grad|: 0.112\n",
      "loss: 0.007, |grad|: 0.089\n",
      "loss: 0.004, |grad|: 0.076\n",
      "loss: 0.004, |grad|: 0.085\n",
      "loss: 0.004, |grad|: 0.091\n",
      "loss: 0.012, |grad|: 0.110\n",
      "Epoch 6...\n",
      "loss: 0.006, |grad|: 0.065\n",
      "loss: 0.004, |grad|: 0.054\n",
      "loss: 0.007, |grad|: 0.074\n",
      "loss: 0.006, |grad|: 0.124\n",
      "loss: 0.004, |grad|: 0.062\n",
      "loss: 0.005, |grad|: 0.073\n",
      "loss: 0.003, |grad|: 0.075\n",
      "loss: 0.011, |grad|: 0.090\n",
      "Epoch 7...\n",
      "loss: 0.006, |grad|: 0.069\n",
      "loss: 0.004, |grad|: 0.062\n",
      "loss: 0.006, |grad|: 0.065\n",
      "loss: 0.004, |grad|: 0.055\n",
      "loss: 0.003, |grad|: 0.074\n",
      "loss: 0.005, |grad|: 0.083\n",
      "loss: 0.003, |grad|: 0.074\n",
      "loss: 0.009, |grad|: 0.107\n",
      "Epoch 8...\n",
      "loss: 0.006, |grad|: 0.096\n",
      "loss: 0.004, |grad|: 0.056\n",
      "loss: 0.006, |grad|: 0.080\n",
      "loss: 0.005, |grad|: 0.098\n",
      "loss: 0.003, |grad|: 0.064\n",
      "loss: 0.003, |grad|: 0.047\n",
      "loss: 0.002, |grad|: 0.058\n",
      "loss: 0.009, |grad|: 0.090\n",
      "Epoch 9...\n",
      "loss: 0.005, |grad|: 0.076\n",
      "loss: 0.004, |grad|: 0.088\n",
      "loss: 0.006, |grad|: 0.099\n",
      "loss: 0.005, |grad|: 0.088\n",
      "loss: 0.004, |grad|: 0.088\n",
      "loss: 0.004, |grad|: 0.071\n",
      "loss: 0.002, |grad|: 0.052\n",
      "loss: 0.009, |grad|: 0.099\n",
      "Epoch 10...\n",
      "loss: 0.006, |grad|: 0.093\n",
      "loss: 0.003, |grad|: 0.099\n",
      "loss: 0.005, |grad|: 0.095\n",
      "loss: 0.005, |grad|: 0.126\n",
      "loss: 0.003, |grad|: 0.070\n",
      "loss: 0.004, |grad|: 0.060\n",
      "loss: 0.003, |grad|: 0.077\n",
      "loss: 0.008, |grad|: 0.080\n",
      "Epoch 11...\n",
      "loss: 0.005, |grad|: 0.093\n",
      "loss: 0.003, |grad|: 0.073\n",
      "loss: 0.006, |grad|: 0.098\n",
      "loss: 0.003, |grad|: 0.063\n",
      "loss: 0.003, |grad|: 0.069\n",
      "loss: 0.003, |grad|: 0.061\n",
      "loss: 0.003, |grad|: 0.073\n",
      "loss: 0.008, |grad|: 0.088\n"
     ]
    }
   ],
   "source": [
    "# 12 epochs in 5 minutes\n",
    "for detector in detectors:\n",
    "    print(f\"Training {detector.__class__.__name__}...\")\n",
    "    detector.fit(images, threshold_perc=99, epochs=12, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for OOD\n",
    "\n",
    "We have trained our detector on a dataset of digits.  \n",
    "What happens when we give it corrupted images of digits (which we expect to be \"OOD\")?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrbb = DataLoader(shiftdata, collate_fn=collate_fn, batch_size=big_batch_size)\n",
    "for corrimages, corrlabels, corrmetadata in corrbb:\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we evaluate the two datasets using the trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('OOD_AE', 0.01)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(type(detector).__name__, np.mean(detector.predict(images).is_ood)) for detector in detectors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('OOD_AE', 0.816)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(type(detector).__name__, np.mean(detector.predict(corrimages).is_ood)) for detector in detectors]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "We can see that the Autoencoder based OOD detector was able to identify many of the shot_noise images as outliers.\n",
    "\n",
    "Depending on your needs, other outlier detectors may work better under specific conditions; you can add them to the detectors list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ood_detector = detectors[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand OOD using metadata\n",
    "\n",
    "We can now look at the metadata features for OOD examples, and find which metadata features are the most surprising for each one. The function [least_likely_features()](metadata_tools.py#least_likely_features) will do this for us.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature        |  occurences\n",
      "=============================\n",
      "fill_frac      :    2895\n",
      "spikiness      :    1538\n",
      "cm_y           :    1095\n",
      "y_ctr          :    858\n",
      "cm_x           :    684\n",
      "x_ctr          :    622\n",
      "height         :    137\n",
      "width          :    11\n",
      "isolated_pixels:    2\n"
     ]
    }
   ],
   "source": [
    "from metadata_tools import least_likely_features\n",
    "od = least_likely_features(refdata, spikydata, ood_detector)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compare the distribution of each metadata feature to the reference. [ks_compare](metadata_tools.py#ks_compare) uses the Kolmogorov-Smirnov two-sample test to look for significant shifts of metadata features, and reports them in order of decreasing statitical significance. It also reports the Wasserstein distance between each pair of distributions, in units of the IQR of the reference.\n",
    "\n",
    "We compare first to the validation data, where we see no significant metadata shifts as expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity\n",
      "feature        | p-value | shift/IQR\n",
      "====================================\n",
      "fill_frac      :  0.024  :   0.029\n",
      "width          :  0.060  :   0.028\n",
      "random         :  0.196  :   0.017\n",
      "spikiness      :  0.239  :   0.022\n",
      "cm_y           :  0.517  :   0.008\n",
      "y_ctr          :  0.740  :   0.026\n",
      "x_ctr          :  0.968  :   0.015\n",
      "cm_x           :  0.973  :   0.005\n",
      "height         :  1.000  :   0.003\n",
      "isolated_pixels:  1.000  :   0.001\n",
      "\n",
      "translate\n",
      "feature        | p-value | shift/IQR\n",
      "====================================\n",
      "cm_x           :  0.000  :   5.422\n",
      "cm_y           :  0.000  :   5.327\n",
      "y_ctr          :  0.000  :   1.899\n",
      "x_ctr          :  0.000  :   1.002\n",
      "height         :  0.000  :   0.039\n",
      "width          :  0.000  :   0.020\n",
      "spikiness      :  0.017  :   0.025\n",
      "fill_frac      :  0.409  :   0.013\n",
      "random         :  0.917  :   0.010\n",
      "isolated_pixels:  1.000  :   0.002\n",
      "\n",
      "shot_noise\n",
      "feature        | p-value | shift/IQR\n",
      "====================================\n",
      "fill_frac      :  0.000  :   0.729\n",
      "cm_y           :  0.000  :   0.144\n",
      "cm_x           :  0.000  :   0.066\n",
      "spikiness      :  0.000  :   0.121\n",
      "width          :  0.000  :   0.053\n",
      "x_ctr          :  0.001  :   0.080\n",
      "height         :  0.015  :   0.009\n",
      "isolated_pixels:  0.045  :   0.024\n",
      "random         :  0.247  :   0.012\n",
      "y_ctr          :  0.815  :   0.035\n",
      "\n",
      "motion_blur\n",
      "feature        | p-value | shift/IQR\n",
      "====================================\n",
      "cm_x           :  0.000  :   3.802\n",
      "cm_y           :  0.000  :   1.054\n",
      "x_ctr          :  0.000  :   3.803\n",
      "width          :  0.000  :   1.242\n",
      "height         :  0.000  :   0.487\n",
      "fill_frac      :  0.000  :   1.079\n",
      "spikiness      :  0.000  :   0.506\n",
      "y_ctr          :  0.000  :   0.600\n",
      "random         :  0.977  :   0.009\n",
      "isolated_pixels:  1.000  :   0.006\n",
      "\n",
      "scale\n",
      "feature        | p-value | shift/IQR\n",
      "====================================\n",
      "x_ctr          :  0.000  :   2.143\n",
      "width          :  0.000  :   0.780\n",
      "height         :  0.000  :   0.967\n",
      "spikiness      :  0.000  :   1.100\n",
      "fill_frac      :  0.000  :   0.763\n",
      "cm_x           :  0.000  :   0.306\n",
      "cm_y           :  0.000  :   0.275\n",
      "y_ctr          :  0.000  :   0.261\n",
      "random         :  0.674  :   0.011\n",
      "isolated_pixels:  1.000  :   0.004\n"
     ]
    }
   ],
   "source": [
    "from metadata_tools import ks_compare\n",
    "\n",
    "valbb = DataLoader(valdata, collate_fn=collate_fn, batch_size=big_batch_size)\n",
    "corr1bb = DataLoader(shiftdata, collate_fn=collate_fn, batch_size=big_batch_size)\n",
    "corr2bb = DataLoader(spikydata, collate_fn=collate_fn, batch_size=big_batch_size)\n",
    "corr3bb = DataLoader(blurdata, collate_fn=collate_fn, batch_size=big_batch_size)\n",
    "corr4bb = DataLoader(scaledata, collate_fn=collate_fn, batch_size=big_batch_size)\n",
    "\n",
    "print('identity')\n",
    "ks_compare(refbb, valbb);\n",
    "print('\\ntranslate')\n",
    "ks_compare(refbb, corr1bb);\n",
    "print('\\nshot_noise')\n",
    "ks_compare(refbb, corr2bb);\n",
    "print('\\nmotion_blur')\n",
    "ks_compare(refbb, corr3bb);\n",
    "print('\\nscale')\n",
    "ks_compare(refbb, corr4bb);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Summary_\n",
    "\n",
    "We demonstrate a method for investigating individiual OOD examples using metadata, by finding which metadata feature is most unusual for each example, relative to the reference dataset.\n",
    "\n",
    "We also demonstrate a method for finding significant distributional shifts in metadata features, and display the p-values of these shifts. We also compute a measure of the magnitudes of distributional shifts, relative to the width of their reference distributions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
