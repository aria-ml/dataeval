{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04e97bb5",
   "metadata": {},
   "source": [
    "# Detecting common augmentations as duplicates\n",
    "\n",
    "This tutorial demonstrates how DataEval's duplicate detection methods handle common torchvision augmentations.\n",
    "\n",
    "Estimated time to complete: 10 minutes\n",
    "\n",
    "Relevant ML stages: [Data Engineering](../concepts/users/ML_Lifecycle.md#data-engineering)\n",
    "\n",
    "Relevant personas: Data Engineer, ML Engineer\n",
    "\n",
    "## What you'll do\n",
    "\n",
    "- Create synthetic test images and apply 30+ torchvision transformations\n",
    "- Run both D4 hash-based and BoVW embedding-based duplicate detection\n",
    "- Compare which transformations each method catches or misses\n",
    "- Tune detection sensitivity with the `cluster_threshold` parameter\n",
    "\n",
    "## What you'll learn\n",
    "\n",
    "- Which augmentation types are detectable as near-duplicates (and which aren't)\n",
    "- When to use D4 hashes vs BoVW embeddings for duplicate detection\n",
    "- How D4 and BoVW have complementary strengths that improve coverage when combined\n",
    "\n",
    "### Quick reference: detection methods\n",
    "\n",
    "| Method                             | Best For                                  | Speed   | Rotation Invariant      |\n",
    "| ---------------------------------- | ----------------------------------------- | ------- | ----------------------- |\n",
    "| **D4 Hashes** (phash_d4, dhash_d4) | Detecting rotated/flipped copies          | Fast    | **Only 90° increments** |\n",
    "| **BoVWExtractor**                  | Semantic similarity, different viewpoints | Slower  | **Any angle**           |\n",
    "| **Basic Hashes** (phash, dhash)    | Same-orientation near-duplicates          | Fastest | No                      |\n",
    "\n",
    "**Key insight:** D4 hashes only handle the 8 symmetries of a square (0°, 90°, 180°, 270° + flips). BoVW using SIFT\n",
    "features is invariant to **any** rotation angle, making it better for detecting arbitrarily rotated duplicates.\n",
    "\n",
    "## What you'll need\n",
    "\n",
    "- A Python environment with the following packages installed:\n",
    "  - `dataeval`\n",
    "  - `opencv-python` or `opencv-python-headless`\n",
    "  - `torchvision`\n",
    "  - `matplotlib`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9a713d",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Data augmentation is a common technique in deep learning, but augmented images can inadvertently appear in both training\n",
    "and test sets, or be saved as \"new\" images when they're really transformations of existing ones. Understanding which\n",
    "augmentations are detectable as near-duplicates helps you:\n",
    "\n",
    "1. **Identify data leakage** - Find augmented copies that leaked between train/test splits\n",
    "1. **Clean datasets** - Remove redundant transformed images\n",
    "1. **Validate augmentation pipelines** - Ensure augmentations create sufficiently distinct images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fd346e",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "Let's import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310e72e4",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Google Colab Only\n",
    "try:\n",
    "    import google.colab  # noqa: F401\n",
    "\n",
    "    %pip install -q dataeval opencv-python-headless torchvision\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ee280a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numbers import Number\n",
    "from typing import cast\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms.v2 as T\n",
    "from PIL import Image\n",
    "\n",
    "from dataeval import config\n",
    "from dataeval.extractors import BoVWExtractor\n",
    "from dataeval.flags import ImageStats\n",
    "from dataeval.quality import Duplicates\n",
    "\n",
    "config.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a76539d",
   "metadata": {},
   "source": [
    "## Creating test data\n",
    "\n",
    "We'll create a synthetic image with rich texture patterns that SIFT can detect. Then we'll apply various torchvision\n",
    "transformations to test detection capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021cecaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_textured_image(seed: int, size: int) -> np.ndarray:\n",
    "    \"\"\"Create an image with texture patterns that SIFT can detect.\n",
    "\n",
    "    Returns image in CHW format (3, H, W) with uint8 values.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # Use the seed to generate random frequencies and phases\n",
    "    # so each seed produces a genuinely different pattern\n",
    "    freqs = rng.uniform(1.0, 5.0, size=6)\n",
    "    phases = rng.uniform(0, 2 * np.pi, size=6)\n",
    "    channel_offsets = rng.integers(5, 30, size=4)\n",
    "\n",
    "    x = np.linspace(0, 6 * np.pi, size)\n",
    "    y = np.linspace(0, 6 * np.pi, size)\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "\n",
    "    # Create pattern with seed-dependent frequency components\n",
    "    pattern = (\n",
    "        np.sin(xx * freqs[0] + phases[0]) * np.cos(yy * freqs[1] + phases[1])\n",
    "        + np.sin(xx * freqs[2] + phases[2]) * np.cos(yy * freqs[3] + phases[3]) * 0.5\n",
    "        + np.sin(xx * freqs[4] + yy * freqs[5] + phases[4]) * 0.3\n",
    "        + rng.random((size, size)) * 0.2\n",
    "    )\n",
    "\n",
    "    # Normalize to 0-255\n",
    "    pattern = ((pattern - pattern.min()) / (pattern.max() - pattern.min()) * 255).astype(np.uint8)\n",
    "\n",
    "    # Create RGB image with seed-dependent channel variations\n",
    "    img = np.stack(\n",
    "        [\n",
    "            pattern,\n",
    "            np.roll(pattern, int(channel_offsets[0]), axis=0),\n",
    "            np.roll(pattern, int(channel_offsets[1]), axis=1),\n",
    "        ],\n",
    "        axis=0,\n",
    "    )  # Shape: (3, H, W)\n",
    "\n",
    "    return img.astype(np.uint8)\n",
    "\n",
    "\n",
    "def numpy_to_pil(img: np.ndarray) -> Image.Image:\n",
    "    \"\"\"Convert CHW numpy array to PIL Image.\"\"\"\n",
    "    return Image.fromarray(np.transpose(img, (1, 2, 0)))\n",
    "\n",
    "\n",
    "def pil_to_numpy(img: Image.Image) -> np.ndarray:\n",
    "    \"\"\"Convert PIL Image to CHW numpy array.\"\"\"\n",
    "    return np.transpose(np.array(img), (2, 0, 1))\n",
    "\n",
    "\n",
    "def tensor_to_numpy(tensor: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"Convert torch tensor (CHW, float 0-1 or uint8) to CHW numpy uint8.\"\"\"\n",
    "    if tensor.dtype == torch.float32:\n",
    "        tensor = (tensor * 255).to(torch.uint8)\n",
    "    return tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf87c9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "\n",
    "# Create base images\n",
    "base_img1 = create_textured_image(seed=67, size=IMG_SIZE)\n",
    "base_img2 = create_textured_image(seed=123, size=IMG_SIZE)\n",
    "base_img3 = create_textured_image(seed=789, size=IMG_SIZE)\n",
    "\n",
    "# Display base images\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "for i, (img, title) in enumerate(\n",
    "    [\n",
    "        (base_img1, \"Base Image 1 (seed=67)\"),\n",
    "        (base_img2, \"Base Image 2 (seed=123)\"),\n",
    "        (base_img3, \"Base Image 3 (seed=789)\"),\n",
    "    ]\n",
    "):\n",
    "    axes[i].imshow(np.transpose(img, (1, 2, 0)))\n",
    "    axes[i].set_title(title)\n",
    "    axes[i].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0bc7e8",
   "metadata": {},
   "source": [
    "## Defining Torchvision transformations\n",
    "\n",
    "We'll test a comprehensive set of common torchvision transformations, organized by category:\n",
    "\n",
    "| Category        | Transformations                     | Expected Detection                |\n",
    "| --------------- | ----------------------------------- | --------------------------------- |\n",
    "| **Geometric**   | Rotation, Flip, Affine, Perspective | High (SIFT is geometry-invariant) |\n",
    "| **Color**       | ColorJitter, Grayscale, Invert      | Medium (depends on intensity)     |\n",
    "| **Blur/Noise**  | GaussianBlur, Noise                 | Medium to Low                     |\n",
    "| **Crop/Resize** | RandomCrop, Resize, CenterCrop      | Medium (depends on overlap)       |\n",
    "| **Severe**      | RandomErasing, Heavy distortion     | Low (features destroyed)          |\n",
    "\n",
    "**Important setup notes:**\n",
    "\n",
    "- We use `expand=True` with a resize-back step for rotation transforms so that the full rotated content is preserved (no\n",
    "  black corners or clipped content).\n",
    "- We use `fill=128` (gray) instead of the default `fill=0` (black) where fill is unavoidable. Black fill creates strong\n",
    "  artificial edges that SIFT detects, corrupting the BoVW histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a996d761",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILL = 128  # Gray fill avoids artificial SIFT edges that black (0) would create\n",
    "\n",
    "\n",
    "def _n(degrees: int) -> Number:\n",
    "    \"\"\"Helper to cast degrees to Number for Pylance.\"\"\"\n",
    "    return cast(Number, degrees)\n",
    "\n",
    "\n",
    "# Helper: rotate with expand=True to preserve full content, then resize back\n",
    "def _rotate_and_resize(degrees):\n",
    "    return T.Compose([T.RandomRotation(degrees=(degrees, degrees), expand=True, fill=FILL), T.Resize(IMG_SIZE)])\n",
    "\n",
    "\n",
    "# Define transformation categories\n",
    "transformations = {\n",
    "    # Geometric transformations - SIFT should handle these well\n",
    "    \"Rotation 15°\": _rotate_and_resize(15),\n",
    "    \"Rotation 45°\": _rotate_and_resize(45),\n",
    "    \"Rotation 90°\": _rotate_and_resize(90),\n",
    "    \"Rotation 180°\": _rotate_and_resize(180),\n",
    "    \"Horizontal Flip\": T.RandomHorizontalFlip(p=1.0),\n",
    "    \"Vertical Flip\": T.RandomVerticalFlip(p=1.0),\n",
    "    \"Affine (rotate+translate)\": T.RandomAffine(degrees=_n(30), translate=(0.1, 0.1), fill=FILL),\n",
    "    \"Affine (rotate+scale)\": T.RandomAffine(degrees=_n(15), scale=(0.8, 1.2), fill=FILL),\n",
    "    \"Perspective (mild)\": T.RandomPerspective(distortion_scale=0.2, p=1.0, fill=FILL),\n",
    "    \"Perspective (strong)\": T.RandomPerspective(distortion_scale=0.5, p=1.0, fill=FILL),\n",
    "    # Color transformations - may or may not be detected\n",
    "    \"Brightness +30%\": T.ColorJitter(brightness=(1.3, 1.3)),\n",
    "    \"Brightness -30%\": T.ColorJitter(brightness=(0.7, 0.7)),\n",
    "    \"Contrast +50%\": T.ColorJitter(contrast=(1.5, 1.5)),\n",
    "    \"Saturation +50%\": T.ColorJitter(saturation=(1.5, 1.5)),\n",
    "    \"Hue Shift\": T.ColorJitter(hue=(0.3, 0.3)),\n",
    "    \"Full ColorJitter\": T.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "    \"Grayscale\": T.Grayscale(num_output_channels=3),\n",
    "    \"Color Invert\": T.RandomInvert(p=1.0),\n",
    "    # Blur and noise\n",
    "    \"Gaussian Blur (mild)\": T.GaussianBlur(kernel_size=5, sigma=(1.0, 1.0)),\n",
    "    \"Gaussian Blur (strong)\": T.GaussianBlur(kernel_size=11, sigma=(3.0, 3.0)),\n",
    "    # Crop and resize\n",
    "    \"Center Crop (80%)\": T.Compose([T.CenterCrop(180), T.Resize(IMG_SIZE)]),\n",
    "    \"Center Crop (50%)\": T.Compose([T.CenterCrop(112), T.Resize(IMG_SIZE)]),\n",
    "    \"Random Crop (80%)\": T.Compose([T.RandomCrop(180), T.Resize(IMG_SIZE)]),\n",
    "    \"Resize Down+Up\": T.Compose([T.Resize(112), T.Resize(IMG_SIZE)]),\n",
    "    \"Resize Down+Up (severe)\": T.Compose([T.Resize(56), T.Resize(IMG_SIZE)]),\n",
    "    # Severe transformations - likely to break detection\n",
    "    \"Random Erasing (10%)\": T.RandomErasing(p=1.0, scale=(0.02, 0.1)),\n",
    "    \"Random Erasing (33%)\": T.RandomErasing(p=1.0, scale=(0.2, 0.33)),\n",
    "    # Combinations (common augmentation pipelines)\n",
    "    \"Augment: Flip+Rotate\": T.Compose(\n",
    "        [\n",
    "            T.RandomHorizontalFlip(p=1.0),\n",
    "            T.RandomRotation(degrees=_n(15), expand=True, fill=FILL),\n",
    "            T.Resize(IMG_SIZE),\n",
    "        ]\n",
    "    ),\n",
    "    \"Augment: Flip+Color\": T.Compose(\n",
    "        [\n",
    "            T.RandomHorizontalFlip(p=1.0),\n",
    "            T.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "        ]\n",
    "    ),\n",
    "    \"Augment: Full Pipeline\": T.Compose(\n",
    "        [\n",
    "            T.RandomHorizontalFlip(p=0.5),\n",
    "            T.RandomRotation(degrees=_n(10), expand=True, fill=FILL),\n",
    "            T.Resize(IMG_SIZE),\n",
    "            T.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "            T.GaussianBlur(kernel_size=3, sigma=(0.5, 0.5)),\n",
    "        ]\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d21367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply all transformations to base image 1\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Add original images first\n",
    "images.append(base_img1)\n",
    "labels.append(\"Original (Base 1)\")\n",
    "\n",
    "# Apply each transformation to base image 1\n",
    "base_pil = numpy_to_pil(base_img1)\n",
    "\n",
    "for name, transform in transformations.items():\n",
    "    torch.manual_seed(42)  # For reproducibility\n",
    "    transformed = transform(base_pil)\n",
    "    images.append(pil_to_numpy(transformed))\n",
    "    labels.append(name)\n",
    "\n",
    "# Add other base images as \"unique\" images (should NOT be detected as duplicates)\n",
    "images.append(base_img2)\n",
    "labels.append(\"Unique: Base 2\")\n",
    "images.append(base_img3)\n",
    "labels.append(\"Unique: Base 3\")\n",
    "\n",
    "print(f\"Created {len(images)} test images:\")\n",
    "print(f\"  - {1} original\")\n",
    "print(f\"  - {len(transformations)} transformations\")\n",
    "print(f\"  - {2} unique (different base images)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a8b66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a sample of transformations\n",
    "sample_indices = [0, 1, 2, 5, 6, 10, 15, 17, 20, 25, 28, 30]\n",
    "sample_indices = [i for i in sample_indices if i < len(images)]\n",
    "\n",
    "n_cols = 4\n",
    "n_rows = (len(sample_indices) + n_cols - 1) // n_cols\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 3.5 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax_idx, img_idx in enumerate(sample_indices):\n",
    "    img = images[img_idx]\n",
    "    axes[ax_idx].imshow(np.transpose(img, (1, 2, 0)))\n",
    "    axes[ax_idx].set_title(f\"[{img_idx}] {labels[img_idx]}\", fontsize=9)\n",
    "    axes[ax_idx].axis(\"off\")\n",
    "\n",
    "for i in range(len(sample_indices), len(axes)):\n",
    "    axes[i].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Sample of Torchvision Transformations Applied to Base Image\", y=1.02, fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8d81fc",
   "metadata": {},
   "source": [
    "## Running near-duplicate detection\n",
    "\n",
    "We'll use both hash-based detection (D4 hashes) and BoVWExtractor to compare their effectiveness on different\n",
    "transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cbe6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: D4 Hash-based detection (rotation/flip invariant at 90° increments)\n",
    "d4_detector = Duplicates(flags=ImageStats.HASH_DUPLICATES_D4)\n",
    "d4_results = d4_detector.evaluate(images)\n",
    "\n",
    "print(\"=== D4 Hash Results ===\")\n",
    "print(\"\\nNear duplicates detected:\")\n",
    "if d4_results.items.near:\n",
    "    for group in d4_results.items.near:\n",
    "        indices = [i[0] if isinstance(i, tuple) else i for i in group.indices]\n",
    "        print(f\"  Group: {indices}\")\n",
    "        print(f\"    Labels: {[labels[i] for i in indices]}\")\n",
    "        print(f\"    Methods: {sorted(group.methods)}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"  None found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b1dc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 2: BoVW-based detection (rotation invariant at any angle)\n",
    "# Use a smaller vocab_size for this small dataset (~32 images).\n",
    "# Large vocabularies create sparse histograms that cluster poorly.\n",
    "bovw_extractor = BoVWExtractor(vocab_size=32)\n",
    "cluster_threshold = 1.75\n",
    "\n",
    "bovw_detector = Duplicates(\n",
    "    flags=ImageStats.NONE,  # Skip hash computation, use only clustering\n",
    "    extractor=bovw_extractor,\n",
    "    cluster_threshold=cluster_threshold,\n",
    ")\n",
    "bovw_results = bovw_detector.evaluate(images)\n",
    "\n",
    "print(\"=== BoVW Results ===\")\n",
    "print(\"\\nNear duplicates detected:\")\n",
    "if bovw_results.items.near:\n",
    "    for group in bovw_results.items.near:\n",
    "        indices = [i[0] if isinstance(i, tuple) else i for i in group.indices]\n",
    "        print(f\"  Group: {indices}\")\n",
    "        print(f\"    Labels: {[labels[i] for i in indices]}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"  None found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad45d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 3: Combined detection (both hashes and BoVW)\n",
    "combined_detector = Duplicates(\n",
    "    flags=ImageStats.HASH_DUPLICATES_D4,\n",
    "    extractor=bovw_extractor,\n",
    "    cluster_threshold=cluster_threshold,\n",
    ")\n",
    "combined_results = combined_detector.evaluate(images)\n",
    "\n",
    "print(\"=== Combined (D4 Hash + BoVW) Results ===\")\n",
    "print(\"\\nNear duplicates detected:\")\n",
    "if combined_results.items.near:\n",
    "    for group in combined_results.items.near:\n",
    "        indices = [i[0] if isinstance(i, tuple) else i for i in group.indices]\n",
    "        print(f\"  Group: {indices}\")\n",
    "        print(f\"    Labels: {[labels[i] for i in indices]}\")\n",
    "        print(f\"    Methods: {sorted(group.methods)}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"  None found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fa8861",
   "metadata": {},
   "source": [
    "## Analyzing detection results by transformation type\n",
    "\n",
    "Let's analyze which transformations were detected as near-duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae820447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detected_indices(results):\n",
    "    \"\"\"Extract all indices detected as duplicates of index 0 (original).\"\"\"\n",
    "    detected = set()\n",
    "    if results.items.near:\n",
    "        for group in results.items.near:\n",
    "            if 0 in group.indices:  # Group contains the original\n",
    "                detected.update(group.indices)\n",
    "    detected.discard(0)  # Remove the original itself\n",
    "    return detected\n",
    "\n",
    "\n",
    "d4_detected = get_detected_indices(d4_results)\n",
    "bovw_detected = get_detected_indices(bovw_results)\n",
    "combined_detected = get_detected_indices(combined_results)\n",
    "\n",
    "print(\"Detection Summary:\")\n",
    "print(f\"  D4 Hashes detected: {len(d4_detected)} transformations\")\n",
    "print(f\"  BoVW detected: {len(bovw_detected)} transformations\")\n",
    "print(f\"  Combined detected: {len(combined_detected)} transformations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858b39f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a detailed comparison table\n",
    "print(\"\\nDetailed Detection Results:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Transformation':<35} {'D4 Hash':<10} {'BoVW':<10} {'Combined':<10}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Skip index 0 (original) and last 2 (unique images)\n",
    "for i in range(1, len(images) - 2):\n",
    "    d4_status = \"Yes\" if i in d4_detected else \"No\"\n",
    "    bovw_status = \"Yes\" if i in bovw_detected else \"No\"\n",
    "    combined_status = \"Yes\" if i in combined_detected else \"No\"\n",
    "    print(f\"{labels[i]:<35} {d4_status:<10} {bovw_status:<10} {combined_status:<10}\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check unique images (should NOT be detected)\n",
    "print(\"\\nUnique Image Verification (should NOT be detected):\")\n",
    "for i in range(len(images) - 2, len(images)):\n",
    "    d4_status = \"DETECTED\" if i in d4_detected else \"OK\"\n",
    "    bovw_status = \"DETECTED\" if i in bovw_detected else \"OK\"\n",
    "    combined_status = \"DETECTED\" if i in combined_detected else \"OK\"\n",
    "    print(f\"  {labels[i]}: D4={d4_status}, BoVW={bovw_status}, Combined={combined_status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e0377d",
   "metadata": {},
   "source": [
    "## Visualizing detected vs missed transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aa2818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize results\n",
    "detected_by_both = bovw_detected & d4_detected\n",
    "detected_by_bovw_only = bovw_detected - d4_detected\n",
    "detected_by_d4_only = d4_detected - bovw_detected\n",
    "missed_by_both = set(range(1, len(images) - 2)) - bovw_detected - d4_detected\n",
    "\n",
    "print(\"Categorized Results:\")\n",
    "print(f\"\\nDetected by BOTH D4 and BoVW ({len(detected_by_both)}):\")\n",
    "for i in sorted(detected_by_both):\n",
    "    print(f\"  [{i}] {labels[i]}\")\n",
    "\n",
    "print(f\"\\nDetected by BoVW ONLY ({len(detected_by_bovw_only)}):\")\n",
    "for i in sorted(detected_by_bovw_only):\n",
    "    print(f\"  [{i}] {labels[i]}\")\n",
    "\n",
    "print(f\"\\nDetected by D4 ONLY ({len(detected_by_d4_only)}):\")\n",
    "for i in sorted(detected_by_d4_only):\n",
    "    print(f\"  [{i}] {labels[i]}\")\n",
    "\n",
    "print(f\"\\nMissed by BOTH ({len(missed_by_both)}):\")\n",
    "for i in sorted(missed_by_both):\n",
    "    print(f\"  [{i}] {labels[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3662e088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some of the detected and missed transformations\n",
    "def visualize_category(indices, title, max_display=6):\n",
    "    \"\"\"Visualize images in a category.\"\"\"\n",
    "    if not indices:\n",
    "        print(f\"{title}: No images\")\n",
    "        return\n",
    "\n",
    "    indices = sorted(indices)[:max_display]\n",
    "    n_cols = min(len(indices), 3)\n",
    "    n_rows = (len(indices) + n_cols - 1) // n_cols\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 4 * n_rows))\n",
    "    axes = [axes] if n_rows * n_cols == 1 else axes.flatten()\n",
    "\n",
    "    for ax_idx, img_idx in enumerate(indices):\n",
    "        axes[ax_idx].imshow(np.transpose(images[img_idx], (1, 2, 0)))\n",
    "        axes[ax_idx].set_title(f\"[{img_idx}] {labels[img_idx]}\", fontsize=9)\n",
    "        axes[ax_idx].axis(\"off\")\n",
    "\n",
    "    for i in range(len(indices), len(axes)):\n",
    "        axes[i].axis(\"off\")\n",
    "\n",
    "    plt.suptitle(title, fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Show original for reference\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "ax.imshow(np.transpose(images[0], (1, 2, 0)))\n",
    "ax.set_title(\"Original Image (reference)\", fontsize=12)\n",
    "ax.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Show each category\n",
    "visualize_category(detected_by_both, \"Detected by BOTH D4 Hash and BoVW\")\n",
    "visualize_category(detected_by_bovw_only, \"Detected by BoVW ONLY (D4 missed these)\")\n",
    "visualize_category(missed_by_both, \"MISSED by Both Methods\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e31ea6a",
   "metadata": {},
   "source": [
    "## Adjusting detection sensitivity\n",
    "\n",
    "The `cluster_threshold` parameter controls how strict the near-duplicate detection is. Let's see how different\n",
    "thresholds affect detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba771198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different cluster thresholds\n",
    "thresholds = [0.75, 1.0, 1.5, 2.0, 2.5]\n",
    "threshold_results = {}\n",
    "\n",
    "for threshold in thresholds:\n",
    "    detector = Duplicates(\n",
    "        flags=ImageStats.NONE,\n",
    "        extractor=bovw_extractor,\n",
    "        cluster_threshold=threshold,\n",
    "    )\n",
    "    results = detector.evaluate(images)\n",
    "    detected = get_detected_indices(results)\n",
    "    threshold_results[threshold] = detected\n",
    "    print(f\"Threshold {threshold}: {len(detected)} transformations detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dffc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how detection changes with threshold\n",
    "print(\"\\nTransformations detected at each threshold:\")\n",
    "print(\"=\" * 90)\n",
    "header = f\"{'Transformation':<35}\"\n",
    "for t in thresholds:\n",
    "    header += f\" {t:<8}\"\n",
    "print(header)\n",
    "print(\"=\" * 90)\n",
    "\n",
    "for i in range(1, len(images) - 2):\n",
    "    row = f\"{labels[i]:<35}\"\n",
    "    for t in thresholds:\n",
    "        status = \"Yes\" if i in threshold_results[t] else \"-\"\n",
    "        row += f\" {status:<8}\"\n",
    "    print(row)\n",
    "\n",
    "print(\"=\" * 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3400d4f",
   "metadata": {},
   "source": [
    "## Key findings and recommendations\n",
    "\n",
    "### Transformations detected as near-duplicates\n",
    "\n",
    "| Transformation Type                    | D4 Hash | BoVW    | Notes                                                                     |\n",
    "| -------------------------------------- | ------- | ------- | ------------------------------------------------------------------------- |\n",
    "| **Rotation (90° increments)**          | Yes     | Yes     | Both methods detect 90° and 180° reliably                                 |\n",
    "| **Rotation (arbitrary angles)**        | No      | Yes     | BoVW's SIFT features are rotation-invariant at any angle                  |\n",
    "| **Horizontal/Vertical Flip**           | Yes     | No      | BoVW clusters flips separately from the original; D4 is designed for this |\n",
    "| **Perspective**                        | No      | Yes     | BoVW detects both mild and strong perspective distortion                  |\n",
    "| **Affine (rotate+translate)**          | No      | Yes     | BoVW handles combined rotation and translation                            |\n",
    "| **Brightness / Contrast / Saturation** | Partial | Partial | Both detect some color shifts; depends on which channel is affected       |\n",
    "| **Grayscale**                          | No      | Yes     | SIFT operates on luminance, so grayscale conversion preserves features    |\n",
    "| **Color Inversion**                    | Yes     | Yes     | Both methods detect inversion                                             |\n",
    "| **Gaussian Blur (mild)**               | Yes     | Yes     | Both methods tolerate mild blur                                           |\n",
    "| **Gaussian Blur (strong)**             | Yes     | No      | D4 hashes are more resilient to strong blur than SIFT                     |\n",
    "| **Resize Down+Up**                     | No      | Yes     | BoVW detects mild resolution loss; both miss severe downsampling          |\n",
    "\n",
    "### Transformations missed by both methods\n",
    "\n",
    "| Transformation Type              | Why Missed                                                                        |\n",
    "| -------------------------------- | --------------------------------------------------------------------------------- |\n",
    "| **Hue shift / Full ColorJitter** | Changes pixel values enough to alter both hashes and SIFT descriptors             |\n",
    "| **All crops (center, random)**   | Removes too much content; remaining features don't match the full-image histogram |\n",
    "| **Severe downsampling**          | Destroys fine-grained SIFT keypoints and alters hash signatures                   |\n",
    "| **Random erasing**               | Destroys local features in erased regions                                         |\n",
    "| **Affine (rotate+scale)**        | Combined scaling with rotation changes SIFT descriptor distributions              |\n",
    "\n",
    "### Complementary strengths\n",
    "\n",
    "A key finding is that D4 hashes and BoVW have **complementary** detection strengths:\n",
    "\n",
    "- **D4 detects but BoVW misses**: Flips, brightness reduction, strong blur\n",
    "- **BoVW detects but D4 misses**: Arbitrary rotations, perspective, affine, grayscale, mild resize, contrast shifts\n",
    "\n",
    "The combined method detected **22 out of 30** transformations (73%) by merging groups across both methods.\n",
    "\n",
    "### Recommendations\n",
    "\n",
    "1. **Use both methods together** for best coverage — they complement each other well\n",
    "1. **For detecting rotated copies**: D4 hashes handle 90° increments and flips; add BoVW for arbitrary angles\n",
    "1. **For data augmentation validation**: Use BoVW with a higher `cluster_threshold` (1.5–2.0) to catch subtle duplicates\n",
    "1. **For large datasets**: Start with fast D4 hashes, then run BoVW on remaining candidates\n",
    "1. **Adjust `cluster_threshold`**: Lower (1.0–1.25) for strict matching, higher (1.5–2.0) for permissive — note that no\n",
    "   transformations are detected at 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dad4fb",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "### TEST ASSERTION CELL ###\n",
    "# Verify that unique images are NOT detected as duplicates\n",
    "unique_indices = {len(images) - 2, len(images) - 1}\n",
    "assert unique_indices.isdisjoint(combined_detected), \"Unique images should not be detected as duplicates\"\n",
    "\n",
    "# Verify that at least some rotations are detected by BoVW\n",
    "# (BoVW uses SIFT which is inherently rotation-invariant)\n",
    "n_transforms = len(transformations)\n",
    "transform_names = list(transformations.keys())\n",
    "rotation_indices = {i + 1 for i, name in enumerate(transform_names) if name.startswith(\"Rotation\")}\n",
    "assert len(rotation_indices & bovw_detected) >= 1, \"BoVW should detect at least one rotation\"\n",
    "\n",
    "# Verify that flips are detected by at least D4 hashes\n",
    "flip_indices = {i + 1 for i, name in enumerate(transform_names) if \"Flip\" in name}\n",
    "assert len(flip_indices & d4_detected) >= 1, \"D4 should detect at least one flip\"\n",
    "\n",
    "print(\"All assertions passed!\")\n",
    "print(\"\\nFinal Summary:\")\n",
    "print(f\"  Total transformations tested: {n_transforms}\")\n",
    "print(f\"  Detected by combined method: {len(combined_detected)}\")\n",
    "print(f\"  Detection rate: {len(combined_detected) / n_transforms * 100:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "default_lexer": "ipython3"
  },
  "kernelspec": {
   "display_name": "dataeval",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
