{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "347a1391",
   "metadata": {},
   "source": [
    "# Identify out-of-distribution samples\n",
    "\n",
    "This guide demonstrates how to identify [out-of-distribution](../concepts/OOD.md) (OOD) samples using\n",
    "reconstruction-based methods with different model architectures.\n",
    "\n",
    "Estimated time to complete: 10-15 minutes\n",
    "\n",
    "Relevant ML stages: [Monitoring](../concepts/users/ML_Lifecycle.md#monitoring),\n",
    "[Data Engineering](../concepts/users/ML_Lifecycle.md#data-engineering)\n",
    "\n",
    "Relevant personas: Machine Learning Engineer, T&E Engineer, Data Scientist\n",
    "\n",
    "## What you'll do\n",
    "\n",
    "- Train different reconstruction models (AE, VAE) for OOD detection\n",
    "- Use Gaussian Mixture Models (GMM) to enhance OOD detection\n",
    "- Compare KNN-based OOD detection with cosine and Euclidean distance metrics\n",
    "- Compare model performance on different OOD scenarios\n",
    "- Visualize reconstruction quality and OOD scores\n",
    "\n",
    "## What you'll learn\n",
    "\n",
    "- When to use Autoencoder (AE) vs Variational Autoencoder (VAE) for OOD detection\n",
    "- How GMM in latent space improves OOD detection\n",
    "- When to choose cosine vs Euclidean distance for KNN-based detection\n",
    "- How to interpret OOD scores and set appropriate thresholds\n",
    "- Different use cases for each model configuration\n",
    "\n",
    "## What you'll need\n",
    "\n",
    "- Knowledge of Python\n",
    "- Basic understanding of PyTorch and neural networks\n",
    "- Understanding of autoencoders (helpful but not required)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfddf89",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Out-of-distribution (OOD) detection is critical for ensuring model reliability in production. When models encounter data\n",
    "that differs significantly from their training distribution, predictions become unreliable. This tutorial demonstrates\n",
    "seven different approaches to OOD detection:\n",
    "\n",
    "**Reconstruction-Based Methods:**\n",
    "\n",
    "1. **Standard Autoencoder (AE)**: Simple reconstruction-based detection using mean squared error\n",
    "1. **Variational Autoencoder (VAE)**: Probabilistic approach with regularized latent space\n",
    "1. **AE with GMM**: Enhanced detection by modeling latent space with Gaussian Mixture Models\n",
    "1. **VAE with GMM**: Combining probabilistic encoding with GMM for robust detection\n",
    "\n",
    "**Distance-Based Methods:**\n",
    "\n",
    "1. **KNN with Cosine Distance**: Measures angular similarity in learned [embeddings](../concepts/Embeddings.md)\n",
    "1. **KNN with Euclidean Distance**: Measures straight-line distance in learned embeddings\n",
    "\n",
    "For this tutorial, you'll use the MNIST dataset of handwritten digits. You'll train models to recognize digits 0-7 and\n",
    "test their ability to detect digits 8-9 as out-of-distribution samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4de1e4",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, install the required packages and import necessary libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45084fa",
   "metadata": {},
   "source": [
    "### Important note on expected results\n",
    "\n",
    "OOD detection performance depends heavily on **how different** the OOD data is from the in-distribution data:\n",
    "\n",
    "- **Easy OOD**: Completely different data (e.g., cats vs dogs) â†’ near 100% detection\n",
    "- **Hard OOD**: Similar data (e.g., digit 8 vs digit 0, both have circles) â†’ lower detection rates\n",
    "\n",
    "In this tutorial, we use digits 8-9 as OOD against training on 0-7. This is a **moderately challenging** scenario\n",
    "because:\n",
    "\n",
    "- Digit 8 shares circular shapes with 0, 6\n",
    "- Digit 9 shares curves with 3, 5\n",
    "\n",
    "Therefore, you should expect:\n",
    "\n",
    "- **In-distribution accuracy**: ~95% (matching our threshold)\n",
    "- **OOD detection rates**: Variable (20-80%), depending on model and similarity\n",
    "- **Score separation**: OOD scores higher than in-dist, but distributions may overlap\n",
    "\n",
    "This reflects real-world scenarios where OOD data often shares features with training data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdc2bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab  # noqa: F401\n",
    "\n",
    "    %pip install -q dataeval torchvision\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ac0768",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import cast\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from maite_datasets.image_classification import CIFAR10, MNIST\n",
    "\n",
    "import dataeval\n",
    "from dataeval import Embeddings\n",
    "from dataeval.extractors import TorchExtractor\n",
    "from dataeval.selection import ClassFilter, Limit, Select, Shuffle\n",
    "from dataeval.shift import OODKNeighbors, OODReconstruction\n",
    "from dataeval.utils.models import AE, VAE, GMMDensityNet\n",
    "from dataeval.utils.preprocessing import rescale, resize, to_canonical_grayscale\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "dataeval.config.set_seed(173, all_generators=True)\n",
    "\n",
    "# Set default batch size\n",
    "dataeval.config.set_batch_size(64)\n",
    "\n",
    "# Set default torch device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e5c9d3",
   "metadata": {},
   "source": [
    "## Prepare the data\n",
    "\n",
    "You'll load the MNIST dataset and split it into in-distribution (digits 0-7) and out-of-distribution (digits 8-9)\n",
    "samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7f9797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    return x.astype(np.float32) / 255.0\n",
    "\n",
    "\n",
    "in_dist_digits = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "out_of_dist_digits = [8, 9]\n",
    "\n",
    "\n",
    "mnist_train = Select(\n",
    "    MNIST(\"./data\", image_set=\"train\", download=True, transforms=normalize),\n",
    "    selections=[Shuffle(), Limit(10000), ClassFilter(in_dist_digits)],\n",
    ")\n",
    "mnist_test_in = Select(\n",
    "    MNIST(\"./data\", image_set=\"test\", download=True, transforms=normalize),\n",
    "    selections=[Shuffle(), Limit(1000), ClassFilter(in_dist_digits)],\n",
    ")\n",
    "mnist_test_ood = Select(\n",
    "    MNIST(\"./data\", image_set=\"test\", download=True, transforms=normalize),\n",
    "    selections=[Shuffle(), Limit(1000), ClassFilter(out_of_dist_digits)],\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(mnist_train)}\")\n",
    "print(f\"Test set size: {len(mnist_test_in)}\")\n",
    "print(f\"Test set size: {len(mnist_test_ood)}\")\n",
    "\n",
    "# Set the input shape (MNIST images are 28x28 grayscale)\n",
    "input_shape = (1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ddc06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data and labels from prefiltered datasets\n",
    "def extract_data_labels(dataset):\n",
    "    \"\"\"Extract images and labels from a dataset.\"\"\"\n",
    "    data, labels = [], []\n",
    "\n",
    "    for img, label_probs, _ in dataset:\n",
    "        label = np.argmax(label_probs)\n",
    "        data.append(img)\n",
    "        labels.append(label)\n",
    "\n",
    "    return np.stack(data), np.asarray(labels)\n",
    "\n",
    "\n",
    "# Extract training and test data (already filtered for correct classes)\n",
    "train_in, train_in_labels = extract_data_labels(mnist_train)\n",
    "test_in, test_in_labels = extract_data_labels(mnist_test_in)\n",
    "test_ood, test_ood_labels = extract_data_labels(mnist_test_ood)\n",
    "\n",
    "print(f\"Training in-distribution: {train_in.shape}\")\n",
    "print(f\"Test in-distribution: {test_in.shape}\")\n",
    "print(f\"Test out-of-distribution: {test_ood.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191ec2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some in-distribution and OOD samples\n",
    "fig, axes = plt.subplots(2, 8, figsize=(12, 3))\n",
    "\n",
    "# Show in-distribution samples (0-7) - one of each digit\n",
    "for digit in range(8):\n",
    "    # Find the first occurrence of this digit\n",
    "    idx = (train_in_labels == digit).nonzero()[0][0]\n",
    "    axes[0, digit].imshow(train_in[idx].squeeze(), cmap=\"gray\")\n",
    "    axes[0, digit].axis(\"off\")\n",
    "    axes[0, digit].set_title(f\"Digit {digit}\")\n",
    "\n",
    "# Show OOD samples (8-9) - 4 of each\n",
    "for i in range(8):\n",
    "    digit = 8 if i < 4 else 9\n",
    "    idx = (test_ood_labels == digit).nonzero()[0][(i % 4) * 50]\n",
    "    axes[1, i].imshow(test_ood[idx].squeeze(), cmap=\"gray\")\n",
    "    axes[1, i].axis(\"off\")\n",
    "    if i % 4 == 0:\n",
    "        axes[1, i].set_title(f\"Digit {digit} (OOD)\", color=\"red\")\n",
    "\n",
    "axes_text_kwargs = {\"ha\": \"right\", \"va\": \"center\", \"fontsize\": 12, \"fontweight\": \"bold\"}\n",
    "axes[0, 0].text(-0.5, 0.5, \"In-Dist\\n(Train)\", transform=axes[0, 0].transAxes, **axes_text_kwargs)\n",
    "axes[1, 0].text(-0.5, 0.5, \"OOD\\n(Test)\", transform=axes[1, 0].transAxes, **axes_text_kwargs)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8163e6",
   "metadata": {},
   "source": [
    "## K-nearest neighbors (KNN) for OOD detection\n",
    "\n",
    "KNN-based OOD detection is a simple yet effective approach that utilizes a pretrained model to create learned\n",
    "embeddings. It works by measuring how far test samples are from their nearest neighbors in the training data. Samples\n",
    "that are far from all training samples are likely OOD.\n",
    "\n",
    "**Use Case**: Fast baseline for OOD detection without model training, interpretable distance-based scoring.\n",
    "\n",
    "> **âš ï¸ Important Note on Embeddings**: KNN performance depends entirely on the quality of the embeddings you provide:\n",
    "\n",
    "- **Better embeddings = better OOD detection**: Use task-specific, well-trained models\n",
    "- **For images**: ResNets, Vision Transformers (ViT), CLIP, or custom CNNs trained on similar data\n",
    "- **For text**: BERT, sentence transformers, domain-specific language models\n",
    "- **For time series**: LSTMs, Transformers trained on temporal data\n",
    "- **For tabular**: MLPs or autoencoders trained on your feature space\n",
    "\n",
    "This tutorial trains a simple CNN for demonstration, but using pretrained models (e.g., ImageNet-pretrained ResNet)\n",
    "would likely improve results significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3199930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple CNN for learning embeddings\n",
    "class EmbeddingNet(torch.nn.Module):\n",
    "    \"\"\"Simple CNN that learns embeddings for digit classification.\"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim=64):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv_layers = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2),  # 28x28 -> 14x14\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2),  # 14x14 -> 7x7\n",
    "        )\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(64 * 7 * 7, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, embedding_dim),\n",
    "        )\n",
    "\n",
    "        # Classification head (for training only)\n",
    "        self.classifier = torch.nn.Linear(embedding_dim, 8)  # 8 digit classes (0-7)\n",
    "\n",
    "    def forward(self, x, return_embedding=False):\n",
    "        \"\"\"Forward pass. Returns embeddings if return_embedding=True, else logits.\"\"\"\n",
    "        emb = self.embedding(self.conv_layers(x))\n",
    "        return emb if return_embedding else self.classifier(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e87da17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the embedding model\n",
    "embedding_model = EmbeddingNet(embedding_dim=64).to(device)\n",
    "optimizer = torch.optim.Adam(embedding_model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Training embedding model for digit classification...\")\n",
    "print(f\"Embedding dimension: {embedding_model.embedding_dim}\")\n",
    "\n",
    "# Train for a few epochs\n",
    "num_epochs = 3\n",
    "batch_size = 256\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    embedding_model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "    # Create batches\n",
    "    num_batches = len(train_in) // batch_size\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "\n",
    "        batch_imgs = torch.as_tensor(train_in[start_idx:end_idx], device=device)\n",
    "        batch_labels = torch.as_tensor(train_in_labels[start_idx:end_idx], device=device)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        logits = embedding_model(batch_imgs)\n",
    "        loss = criterion(logits, batch_labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        total += batch_labels.size(0)\n",
    "        correct += (predicted == batch_labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "print(\"âœ“ Embedding model trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da753cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create extractor using the trained embedding model\n",
    "knn_extractor = TorchExtractor(embedding_model, device=device)\n",
    "\n",
    "# Get embeddings for all datasets\n",
    "print(\"Extracting embeddings...\")\n",
    "train_in_emb = Embeddings(train_in, extractor=knn_extractor)\n",
    "test_in_emb = Embeddings(test_in, extractor=knn_extractor)\n",
    "test_ood_emb = Embeddings(test_ood, extractor=knn_extractor)\n",
    "\n",
    "print(f\"Training embeddings shape: {train_in_emb.shape}\")\n",
    "print(f\"Test in-dist embeddings shape: {test_in_emb.shape}\")\n",
    "print(f\"Test OOD embeddings shape: {test_ood_emb.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747f8a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create KNN detector with learned embeddings\n",
    "ood_knn_cos = OODKNeighbors(k=10, distance_metric=\"cosine\")\n",
    "\n",
    "print(\"\\nFitting KNN detector with learned embeddings...\")\n",
    "ood_knn_cos.fit(train_in_emb, threshold_perc=95.0)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15080c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions with learned embeddings\n",
    "knn_cos_result_in = ood_knn_cos.predict(test_in_emb)\n",
    "knn_cos_result_ood = ood_knn_cos.predict(test_ood_emb)\n",
    "\n",
    "# Calculate detection accuracy\n",
    "in_acc_knn = 100 * (1 - knn_cos_result_in.is_ood.mean())\n",
    "ood_rate_knn = 100 * knn_cos_result_ood.is_ood.mean()\n",
    "\n",
    "print(\"\\n--- KNN Cosine (Embeddings) Results ---\")\n",
    "print(f\"In-distribution correctly identified: {in_acc_knn:.1f}%\")\n",
    "print(f\"OOD samples detected: {ood_rate_knn:.1f}%\")\n",
    "print(f\"Average score (in-dist): {knn_cos_result_in.instance_score.mean():.4f}\")\n",
    "print(f\"Average score (OOD): {knn_cos_result_ood.instance_score.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612871af",
   "metadata": {},
   "source": [
    "### KNN with Euclidean distance\n",
    "\n",
    "Euclidean distance measures the straight-line distance between points in the embedding space. Unlike cosine similarity,\n",
    "which only considers the angle between vectors, Euclidean distance also accounts for their magnitude. This makes it\n",
    "better suited for embeddings where the scale of the vectors carries meaningful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c89d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create KNN detector with Euclidean distance\n",
    "ood_knn_euc = OODKNeighbors(k=10, distance_metric=\"euclidean\")\n",
    "\n",
    "print(\"Fitting KNN (Euclidean) detector with learned embeddings...\")\n",
    "ood_knn_euc.fit(train_in_emb, threshold_perc=95.0)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc528be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions with Euclidean distance\n",
    "knn_euc_result_in = ood_knn_euc.predict(test_in_emb)\n",
    "knn_euc_result_ood = ood_knn_euc.predict(test_ood_emb)\n",
    "\n",
    "# Calculate detection accuracy\n",
    "in_acc_knn_euc = 100 * (1 - knn_euc_result_in.is_ood.mean())\n",
    "ood_rate_knn_euc = 100 * knn_euc_result_ood.is_ood.mean()\n",
    "\n",
    "print(\"\\n--- KNN Euclidean (Embeddings) Results ---\")\n",
    "print(f\"In-distribution correctly identified: {in_acc_knn_euc:.1f}%\")\n",
    "print(f\"OOD samples detected: {ood_rate_knn_euc:.1f}%\")\n",
    "print(f\"Average score (in-dist): {knn_euc_result_in.instance_score.mean():.4f}\")\n",
    "print(f\"Average score (OOD): {knn_euc_result_ood.instance_score.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dc1100",
   "metadata": {},
   "source": [
    "## Standard autoencoder (AE) for OOD detection\n",
    "\n",
    "The simplest approach uses a standard autoencoder that learns to reconstruct normal (in-distribution) images. When\n",
    "presented with OOD data, reconstruction error increases, signaling anomalous samples.\n",
    "\n",
    "**Use Case**: Fast, simple OOD detection when you have well-separated distributions and don't need probabilistic\n",
    "interpretations.\n",
    "\n",
    "> **âš ï¸ Important Note on Model Architecture**: This tutorial uses a simple, generic AE architecture provided by DataEval\n",
    "> for demonstration purposes. In production:\n",
    "\n",
    "- **Design architectures for your data type**: CNNs for images, LSTMs/Transformers for sequences, MLPs for tabular data\n",
    "- **Match complexity to your problem**: Deeper networks for complex data, simpler for basic patterns\n",
    "- **Tune hyperparameters**: Latent dimension size, layer widths, activation functions, etc.\n",
    "- **Your model choice significantly impacts OOD detection performance**\n",
    "\n",
    "The DataEval `OODReconstruction` class works with any PyTorch model you provideâ€”customize it for best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f22fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and configure the autoencoder\n",
    "ae_model = AE(input_shape=input_shape)\n",
    "\n",
    "# Configure training parameters\n",
    "config = OODReconstruction.Config(\n",
    "    epochs=3,\n",
    "    batch_size=64,\n",
    "    threshold_perc=95.0,  # 95% of training data considered normal\n",
    ")\n",
    "\n",
    "# Initialize OOD detector\n",
    "ood_ae = OODReconstruction(ae_model, device=device, config=config)\n",
    "\n",
    "print(\"Training Standard Autoencoder...\")\n",
    "print(f\"Model type detected: {ood_ae.model_type}\")\n",
    "print(f\"Using GMM: {ood_ae.use_gmm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6240b2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on in-distribution data\n",
    "ood_ae.fit(train_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa235e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "ae_result_in = ood_ae.predict(test_in)\n",
    "ae_result_ood = ood_ae.predict(test_ood)\n",
    "\n",
    "# Calculate detection accuracy\n",
    "in_acc_ae = 100 * (1 - ae_result_in.is_ood.mean())\n",
    "ood_rate_ae = 100 * ae_result_ood.is_ood.mean()\n",
    "\n",
    "print(\"\\n--- Standard AE Results ---\")\n",
    "print(f\"In-distribution correctly identified: {in_acc_ae:.1f}%\")\n",
    "print(f\"OOD samples detected: {ood_rate_ae:.1f}%\")\n",
    "print(f\"Average score (in-dist): {ae_result_in.instance_score.mean():.4f}\")\n",
    "print(f\"Average score (OOD): {ae_result_ood.instance_score.mean():.4f}\")\n",
    "\n",
    "# Validation: Check if OOD scores are higher than in-dist scores\n",
    "score_separation = ae_result_ood.instance_score.mean() - ae_result_in.instance_score.mean()\n",
    "print(f\"\\nScore separation (OOD - In-Dist): {score_separation:.4f}\")\n",
    "if score_separation > 0:\n",
    "    print(\"âœ“ Expected: OOD samples have higher scores than in-distribution samples\")\n",
    "else:\n",
    "    print(\"âš  Warning: OOD scores should be higher than in-distribution scores\")\n",
    "\n",
    "# Check if we're near the target threshold\n",
    "if 90 <= in_acc_ae <= 98:\n",
    "    print(f\"âœ“ Expected: ~95% of in-distribution samples correctly identified (got {in_acc_ae:.1f}%)\")\n",
    "else:\n",
    "    print(f\"âš  Note: Expected ~95% in-dist accuracy, got {in_acc_ae:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6639179d",
   "metadata": {},
   "source": [
    "## Variational autoencoder (VAE) for OOD detection\n",
    "\n",
    "VAEs learn a probabilistic latent representation, which provides better generalization and more structured latent spaces\n",
    "compared to standard AEs. This can improve OOD detection, especially when in-distribution data has high variability.\n",
    "\n",
    "**Use Case**: When you need a more robust latent representation or when your in-distribution data has significant\n",
    "variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3fdbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and configure the VAE\n",
    "vae_model = VAE(input_shape=input_shape)\n",
    "\n",
    "# Initialize OOD detector (auto-detects as VAE)\n",
    "ood_vae = OODReconstruction(vae_model, device=device, config=config)\n",
    "\n",
    "print(\"Training Variational Autoencoder...\")\n",
    "print(f\"Model type detected: {ood_vae.model_type}\")\n",
    "print(f\"Using GMM: {ood_vae.use_gmm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546f9873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the VAE\n",
    "ood_vae.fit(train_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90957a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate VAE performance\n",
    "vae_result_in = ood_vae.predict(test_in)\n",
    "vae_result_ood = ood_vae.predict(test_ood)\n",
    "\n",
    "in_acc_vae = 100 * (1 - vae_result_in.is_ood.mean())\n",
    "ood_rate_vae = 100 * vae_result_ood.is_ood.mean()\n",
    "\n",
    "print(\"\\n--- VAE Results ---\")\n",
    "print(f\"In-distribution correctly identified: {in_acc_vae:.1f}%\")\n",
    "print(f\"OOD samples detected: {ood_rate_vae:.1f}%\")\n",
    "print(f\"Average score (in-dist): {vae_result_in.instance_score.mean():.4f}\")\n",
    "print(f\"Average score (OOD): {vae_result_ood.instance_score.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510477f4",
   "metadata": {},
   "source": [
    "## Autoencoder with GMM for enhanced OOD detection\n",
    "\n",
    "Adding a Gaussian Mixture Model (GMM) to the latent space provides an additional signal for OOD detection. The GMM\n",
    "models the density of the latent representations, and samples with low density are likely to be OOD. This combines\n",
    "reconstruction error with density estimation using **sensor fusion**: both components are standardized (z-score\n",
    "normalized) and combined with configurable weighting.\n",
    "\n",
    "**Use Case**: When you need higher detection accuracy and have complex in-distribution data that naturally clusters into\n",
    "multiple groups.\n",
    "\n",
    "> **âš ï¸ Important**: GMM fusion parameters (`gmm_weight` and `gmm_score_mode`) significantly impact performance. The\n",
    "> default `gmm_weight=0.7` favors the GMM component, which typically works well. Experiment with values in [0.5, 0.9]\n",
    "> for your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d53a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AE with GMM density network\n",
    "# The latent dimension is auto-computed by AE\n",
    "ae_model_gmm = AE(input_shape=input_shape)\n",
    "latent_dim = cast(int, ae_model_gmm.encoder.flatten[1].out_features)\n",
    "\n",
    "# Create GMM density network with 3 components\n",
    "gmm_density_net = GMMDensityNet(latent_dim=latent_dim, n_gmm=3)\n",
    "ae_model_gmm.gmm_density_net = gmm_density_net\n",
    "\n",
    "# Configure training parameters\n",
    "config_gmm = OODReconstruction.Config(\n",
    "    epochs=3,\n",
    "    batch_size=64,\n",
    "    threshold_perc=95.0,  # 95% of training data considered normal\n",
    "    gmm_weight=0.7,  # For GMM models: balance reconstruction (30%) and GMM energy (70%)\n",
    "    gmm_score_mode=\"standardized\",  # Use z-score normalization for score fusion\n",
    ")\n",
    "\n",
    "# Initialize OOD detector (auto-detects GMM usage)\n",
    "ood_ae_gmm = OODReconstruction(ae_model_gmm, device=device, config=config_gmm)\n",
    "\n",
    "print(\"Training Autoencoder with GMM...\")\n",
    "print(f\"Model type detected: {ood_ae_gmm.model_type}\")\n",
    "print(f\"Using GMM: {ood_ae_gmm.use_gmm}\")\n",
    "print(f\"Latent dimension: {latent_dim}\")\n",
    "print(f\"Number of GMM components: {gmm_density_net.n_gmm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174a7691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the AE+GMM model\n",
    "ood_ae_gmm.fit(train_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fbb5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate AE+GMM performance\n",
    "ae_gmm_result_in = ood_ae_gmm.predict(test_in)\n",
    "ae_gmm_result_ood = ood_ae_gmm.predict(test_ood)\n",
    "\n",
    "in_acc_ae_gmm = 100 * (1 - ae_gmm_result_in.is_ood.mean())\n",
    "ood_rate_ae_gmm = 100 * ae_gmm_result_ood.is_ood.mean()\n",
    "\n",
    "print(\"\\n--- AE + GMM Results ---\")\n",
    "print(f\"In-distribution correctly identified: {in_acc_ae_gmm:.1f}%\")\n",
    "print(f\"OOD samples detected: {ood_rate_ae_gmm:.1f}%\")\n",
    "print(f\"Average score (in-dist): {ae_gmm_result_in.instance_score.mean():.4f}\")\n",
    "print(f\"Average score (OOD): {ae_gmm_result_ood.instance_score.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a2b43a",
   "metadata": {},
   "source": [
    "## VAE with GMM for maximum robustness\n",
    "\n",
    "Combining VAE's probabilistic latent space with GMM density estimation provides the most sophisticated OOD detection\n",
    "approach. This is particularly effective when you need high reliability and have sufficient computational resources.\n",
    "\n",
    "**Use Case**: Production systems where false negatives (missing OOD samples) are costly, and you need maximum detection\n",
    "reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3da21a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create VAE with GMM density network\n",
    "vae_model_gmm = VAE(input_shape=input_shape)\n",
    "vae_latent_dim = vae_model_gmm.latent_dim\n",
    "\n",
    "# Create GMM density network\n",
    "gmm_density_net_vae = GMMDensityNet(latent_dim=vae_latent_dim, n_gmm=3)\n",
    "vae_model_gmm.gmm_density_net = gmm_density_net_vae\n",
    "\n",
    "# Initialize OOD detector\n",
    "ood_vae_gmm = OODReconstruction(vae_model_gmm, device=device, config=config_gmm)\n",
    "\n",
    "print(\"Training VAE with GMM...\")\n",
    "print(f\"Model type detected: {ood_vae_gmm.model_type}\")\n",
    "print(f\"Using GMM: {ood_vae_gmm.use_gmm}\")\n",
    "print(f\"Latent dimension: {vae_latent_dim}\")\n",
    "print(f\"Number of GMM components: {gmm_density_net_vae.n_gmm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dee4746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the VAE+GMM model\n",
    "ood_vae_gmm.fit(train_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426f107e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate VAE+GMM performance\n",
    "vae_gmm_result_in = ood_vae_gmm.predict(test_in)\n",
    "vae_gmm_result_ood = ood_vae_gmm.predict(test_ood)\n",
    "\n",
    "in_acc_vae_gmm = 100 * (1 - vae_gmm_result_in.is_ood.mean())\n",
    "ood_rate_vae_gmm = 100 * vae_gmm_result_ood.is_ood.mean()\n",
    "\n",
    "print(\"\\n--- VAE + GMM Results ---\")\n",
    "print(f\"In-distribution correctly identified: {in_acc_vae_gmm:.1f}%\")\n",
    "print(f\"OOD samples detected: {ood_rate_vae_gmm:.1f}%\")\n",
    "print(f\"Average score (in-dist): {vae_gmm_result_in.instance_score.mean():.4f}\")\n",
    "print(f\"Average score (OOD): {vae_gmm_result_ood.instance_score.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3405f2",
   "metadata": {},
   "source": [
    "## Compare all methods\n",
    "\n",
    "Now let's visualize and compare the performance of all six approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb8e2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison\n",
    "methods = [\"KNN\\nCosine\", \"KNN\\nEuclidean\", \"AE\", \"VAE\", \"AE+GMM\", \"VAE+GMM\"]\n",
    "in_dist_acc = [in_acc_knn, in_acc_knn_euc, in_acc_ae, in_acc_vae, in_acc_ae_gmm, in_acc_vae_gmm]\n",
    "ood_detect = [ood_rate_knn, ood_rate_knn_euc, ood_rate_ae, ood_rate_vae, ood_rate_ae_gmm, ood_rate_vae_gmm]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 5))\n",
    "\n",
    "# Plot in-distribution accuracy\n",
    "colors = [\"#3498db\", \"#2980b9\", \"#9b59b6\", \"#8e44ad\", \"#2ecc71\", \"#e74c3c\"]\n",
    "bars1 = ax1.bar(methods, in_dist_acc, color=colors)\n",
    "ax1.set_ylabel(\"Accuracy (%)\", fontsize=12)\n",
    "ax1.set_title(\"In-Distribution Samples Correctly Identified\", fontsize=14, fontweight=\"bold\")\n",
    "ax1.set_ylim([0, 105])\n",
    "ax1.axhline(y=95, color=\"gray\", linestyle=\"--\", alpha=0.5, label=\"Target: 95%\")\n",
    "ax1.legend()\n",
    "ax1.tick_params(axis=\"x\", rotation=0)\n",
    "text_kwargs = {\"ha\": \"center\", \"va\": \"bottom\", \"fontsize\": 9, \"fontweight\": \"bold\"}\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width() / 2.0, height, f\"{height:.1f}%\", **text_kwargs)\n",
    "\n",
    "# Plot OOD detection rate\n",
    "bars2 = ax2.bar(methods, ood_detect, color=colors)\n",
    "ax2.set_ylabel(\"Detection Rate (%)\", fontsize=12)\n",
    "ax2.set_title(\"Out-of-Distribution Samples Detected\", fontsize=14, fontweight=\"bold\")\n",
    "ax2.set_ylim([0, 105])\n",
    "ax2.tick_params(axis=\"x\", rotation=0)\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width() / 2.0, height, f\"{height:.1f}%\", **text_kwargs)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38d8a43",
   "metadata": {},
   "source": [
    "### Key observations\n",
    "\n",
    "1. In-distribution accuracy should be close to threshold (95%)\n",
    "1. KNN (Cosine) uses angular similarity, which is effective when embedding magnitude is less informative\n",
    "1. KNN (Euclidean) uses absolute distance, which can capture magnitude differences in embeddings\n",
    "1. Comparing both KNN variants reveals how distance metric choice affects detection sensitivity\n",
    "1. GMM models add latent density information for better separation\n",
    "1. All models show some OOD detection capability\n",
    "\n",
    "Note: Digits 8 and 9 share features with 0-7 (circles, curves), making this a challenging OOD scenario. Lower detection\n",
    "rates (20-70%) are expected and realistic for this hard case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5b6313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize OOD score distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "results = [\n",
    "    (knn_cos_result_in, knn_cos_result_ood, \"KNN (Cosine)\"),\n",
    "    (ae_result_in, ae_result_ood, \"AE\"),\n",
    "    (vae_result_in, vae_result_ood, \"VAE\"),\n",
    "    (knn_euc_result_in, knn_euc_result_ood, \"KNN (Euclidean)\"),\n",
    "    (ae_gmm_result_in, ae_gmm_result_ood, \"AE + GMM\"),\n",
    "    (vae_gmm_result_in, vae_gmm_result_ood, \"VAE + GMM\"),\n",
    "]\n",
    "\n",
    "for idx, result in enumerate(results):\n",
    "    row, col = idx // 3, idx % 3\n",
    "    ax = axes[row, col]\n",
    "\n",
    "    result_in, result_ood, title = result\n",
    "\n",
    "    # Plot histograms\n",
    "    ax.hist(result_in.instance_score, bins=50, alpha=0.6, label=\"In-Distribution\", color=\"blue\")\n",
    "    ax.hist(result_ood.instance_score, bins=50, alpha=0.6, label=\"Out-of-Distribution\", color=\"red\")\n",
    "\n",
    "    ax.set_xlabel(\"OOD Score\", fontsize=11)\n",
    "    ax.set_ylabel(\"Frequency\", fontsize=11)\n",
    "    ax.set_title(title, fontsize=13, fontweight=\"bold\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69ab769",
   "metadata": {},
   "source": [
    "### Interpreting score distributions\n",
    "\n",
    "What to look for:\n",
    "\n",
    "- Good separation: Blue (in-dist) and red (OOD) histograms are well-separated\n",
    "- Poor separation: Significant overlap between distributions\n",
    "- KNN (Cosine): Scores based on angular distance - effective for normalized embeddings\n",
    "- KNN (Euclidean): Scores based on absolute distance - captures magnitude differences\n",
    "- GMM models: Add latent density information for better separation\n",
    "\n",
    "Expected behavior:\n",
    "\n",
    "- All OOD scores should be shifted right (higher) compared to in-dist scores\n",
    "- More separation = better OOD detection capability\n",
    "- Some overlap is normal, especially when OOD samples (8,9) share features with in-dist (0-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f123edbc",
   "metadata": {},
   "source": [
    "## Visualize reconstructions\n",
    "\n",
    "Let's examine how reconstruction-based models reconstruct in-distribution vs out-of-distribution samples. Good OOD\n",
    "detection should show clear degradation in reconstruction quality for OOD samples.\n",
    "\n",
    "Note: KNN doesn't use reconstruction, so we'll focus on the autoencoder-based methods here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f511fd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get reconstructions\n",
    "def get_reconstructions(model, data, device):\n",
    "    \"\"\"Get reconstructions from a model.\"\"\"\n",
    "    model.model.to(device)\n",
    "    model.model.eval()\n",
    "    with torch.no_grad():\n",
    "        data_tensor = torch.from_numpy(data).float().to(device)\n",
    "        output = model.model(data_tensor)\n",
    "        reconstruction = output[0] if isinstance(output, tuple) else output\n",
    "        return reconstruction.cpu().numpy()\n",
    "\n",
    "\n",
    "# Get samples: 1 in-dist, 1 OOD stacked as rows\n",
    "n_samples = 2\n",
    "originals = np.concatenate([test_in[:n_samples], test_ood[:n_samples]], axis=0)  # (4, 1, 28, 28)\n",
    "\n",
    "# Get reconstructions for all samples\n",
    "recon_ae = get_reconstructions(ood_ae, originals, device)  # (4, 1, 28, 28)\n",
    "recon_vae_gmm = get_reconstructions(ood_vae_gmm, originals, device)  # (4, 1, 28, 28)\n",
    "\n",
    "# Stack columns: Original, AE, VAE -> shape (4, 3, 1, 28, 28)\n",
    "recon_grid = np.stack([originals, recon_ae, recon_vae_gmm], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf0644f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize reconstructions: rows = samples, columns = Original/AE/VAE\n",
    "fig, axes = plt.subplots(4, 3, figsize=(6, 8))\n",
    "\n",
    "# Column titles\n",
    "col_titles = [\"Original\", \"AE\", \"VAE+GMM\"]\n",
    "for j, title in enumerate(col_titles):\n",
    "    axes[0, j].set_title(title, fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "# Row labels\n",
    "row_labels = [\"In-Dist\", \"In-Dist\", \"OOD\", \"OOD\"]\n",
    "\n",
    "# Plot each cell using recon_grid[row, col]\n",
    "for i, label in enumerate(row_labels):\n",
    "    # Add row label\n",
    "    color = \"darkgreen\" if \"In-Dist\" in label else \"darkred\"\n",
    "    axes[i, 0].text(\n",
    "        -0.3,\n",
    "        0.5,\n",
    "        label,\n",
    "        transform=axes[i, 0].transAxes,\n",
    "        ha=\"right\",\n",
    "        va=\"center\",\n",
    "        fontsize=11,\n",
    "        fontweight=\"bold\",\n",
    "        color=color,\n",
    "    )\n",
    "\n",
    "    for j in range(3):\n",
    "        axes[i, j].imshow(recon_grid[i, j].squeeze(), cmap=\"gray\")\n",
    "        axes[i, j].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183cf6ca",
   "metadata": {},
   "source": [
    "### Understanding reconstructions\n",
    "\n",
    "What to observe:\n",
    "\n",
    "- **Columns**: Original image, AE reconstruction, VAE+GMM reconstruction\n",
    "- **Rows 1-2**: In-distribution samples (digits 5 and 4)\n",
    "- **Rows 3-4**: Out-of-distribution samples (digit 8)\n",
    "\n",
    "Expected reconstruction behavior:\n",
    "\n",
    "- **In-dist**: Model has learned these patterns â†’ good reconstruction â†’ low error\n",
    "- **OOD**: Model hasn't seen these patterns â†’ worse reconstruction â†’ high error\n",
    "\n",
    "Note: The degree of degradation depends on similarity between in-dist and OOD:\n",
    "\n",
    "- Digits 8 and 9 share some features with 0-7 (curves, circles)\n",
    "- So reconstructions may still look reasonable but will have higher error\n",
    "- More distinct OOD data (e.g., letters instead of digits) would show clearer degradation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed508326",
   "metadata": {},
   "source": [
    "## Comparing use cases - when does each method excel?\n",
    "\n",
    "> **âš ï¸ IMPORTANT**: Results Reflect Limited Training & Generic Models\n",
    "\n",
    "This comparison uses:\n",
    "\n",
    "- **Only 3 epochs** for AE/VAE training and KNN embedding model training (production typically needs 10-50+ epochs)\n",
    "- **Small sample size**: 10K training, 3K test samples\n",
    "- **Generic model architectures**: Simple CNNs not optimized for MNIST\n",
    "- **Fast demonstration** prioritized over optimal performance\n",
    "\n",
    "**What this means:**\n",
    "\n",
    "- Results show what happens with _minimal_ training and _generic_ models (useful for quick prototypes)\n",
    "- VAE and GMM methods typically need more training to show their theoretical advantages\n",
    "- **Model architecture matters**: Custom architectures designed for your data type (images, time series, tabular) will\n",
    "  perform significantly better\n",
    "- With proper training/tuning and domain-specific architectures, the performance rankings may change significantly\n",
    "- Use these results as a starting point, not definitive guidance\n",
    "\n",
    "> **ðŸ’¡ Key Insight**: The AE, VAE, and GMM methods use **models you provide**. Performance heavily depends on:\n",
    "\n",
    "- Choosing appropriate architectures for your data type and complexity\n",
    "- Proper hyperparameter tuning (latent dimensions, layer sizes, activation functions)\n",
    "- Sufficient training epochs and data\n",
    "- Appropriate loss functions and regularization\n",
    "\n",
    "The simple models used here serve as examplesâ€”real applications should use architectures targeted to the specific\n",
    "scenario.\n",
    "\n",
    "Let's test each method on different OOD scenarios to understand their strengths and weaknesses in this limited-training\n",
    "setting.\n",
    "\n",
    "We'll create three different OOD scenarios with increasing difficulty:\n",
    "\n",
    "1. **Easy OOD**: CIFAR10 natural images (converted to grayscale 28x28) - completely different from digits\n",
    "1. **Medium OOD**: Rotated digits - same objects, different orientation\n",
    "1. **Hard OOD**: Digits 8-9 - similar features to training data (current scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75115dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create different OOD scenarios\n",
    "\n",
    "# Scenario 1: Easy OOD - CIFAR10 (completely different domain: natural images vs digits)\n",
    "# Load CIFAR10 and convert to match MNIST format\n",
    "cifar_dataset = CIFAR10(\"./data\", image_set=\"test\", download=True)\n",
    "easy_ood_list = []\n",
    "for i in range(500):\n",
    "    img = cifar_dataset[i][0]\n",
    "    img_gray = resize(to_canonical_grayscale(rescale(img, 8)), 28)[np.newaxis, :]\n",
    "    easy_ood_list.append(normalize(img_gray))\n",
    "easy_ood = np.stack(easy_ood_list)\n",
    "\n",
    "# Scenario 2: Medium OOD - Rotated digits (same domain, different transformation)\n",
    "medium_ood = np.rot90(test_in[:500], k=1, axes=(2, 3)).copy()\n",
    "\n",
    "# Scenario 3: Hard OOD - Digits 8-9 (already created as test_ood_subset)\n",
    "hard_ood = test_ood\n",
    "\n",
    "# Get embeddings for all OOD scenarios (reuse the same extractor)\n",
    "easy_ood_emb = Embeddings(easy_ood, extractor=knn_extractor)\n",
    "medium_ood_emb = Embeddings(medium_ood, extractor=knn_extractor)\n",
    "hard_ood_emb = Embeddings(hard_ood, extractor=knn_extractor)\n",
    "\n",
    "print(\"Created three OOD scenarios:\")\n",
    "print(f\"1. Easy (CIFAR10 â†’ grayscale): {easy_ood.shape}\")\n",
    "print(f\"2. Medium (Rotated digits): {medium_ood.shape}\")\n",
    "print(f\"3. Hard (Digits 8-9): {hard_ood.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf75c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the different OOD scenarios\n",
    "fig, axes = plt.subplots(3, 5, figsize=(12, 7))\n",
    "\n",
    "ood_by_scenario = [easy_ood, medium_ood, hard_ood]\n",
    "ood_title = [(\"Easy OOD (CIFAR10)\", \"red\"), (\"Medium OOD (Rotated)\", \"orange\"), (\"Hard OOD (Digits 8-9)\", \"darkred\")]\n",
    "\n",
    "# Easy OOD - CIFAR10 (grayscale)\n",
    "for i in range(5):\n",
    "    for j in range(3):\n",
    "        if i == 0:\n",
    "            axes[j, 0].set_title(ood_title[j][0], fontweight=\"bold\", color=ood_title[j][1])\n",
    "        axes[j, i].imshow(ood_by_scenario[j][i * 20].squeeze(), cmap=\"gray\")\n",
    "        axes[j, i].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62753d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models on all three OOD scenarios\n",
    "models = {\n",
    "    \"KNN Cosine\": ood_knn_cos,\n",
    "    \"KNN Euclidean\": ood_knn_euc,\n",
    "    \"AE\": ood_ae,\n",
    "    \"VAE\": ood_vae,\n",
    "    \"AE+GMM\": ood_ae_gmm,\n",
    "    \"VAE+GMM\": ood_vae_gmm,\n",
    "}\n",
    "\n",
    "scenarios = {\n",
    "    \"Easy (CIFAR10)\": (easy_ood, easy_ood_emb),\n",
    "    \"Medium (Rotated)\": (medium_ood, medium_ood_emb),\n",
    "    \"Hard (Digits 8-9)\": (hard_ood, hard_ood_emb),\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results_matrix = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    results_matrix[model_name] = {}\n",
    "    for scenario_name, (ood_data, ood_data_emb) in scenarios.items():\n",
    "        # Use appropriate data format\n",
    "        data_to_use = ood_data_emb if model_name.startswith(\"KNN\") else ood_data\n",
    "\n",
    "        result = model.predict(data_to_use)\n",
    "        detection_rate = 100 * result.is_ood.mean()\n",
    "        results_matrix[model_name][scenario_name] = detection_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b83db6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "model_names = list(results_matrix.keys())\n",
    "scenario_names = list(scenarios.keys())\n",
    "\n",
    "# Create matrix for heatmap\n",
    "data = np.array([[results_matrix[model][scenario] for scenario in scenario_names] for model in model_names])\n",
    "\n",
    "im = ax.imshow(data, cmap=\"viridis\", aspect=\"auto\", vmin=0, vmax=100)\n",
    "\n",
    "# Set ticks and labels\n",
    "ax.set_xticks(np.arange(len(scenario_names)))\n",
    "ax.set_yticks(np.arange(len(model_names)))\n",
    "ax.set_xticklabels(scenario_names)\n",
    "ax.set_yticklabels(model_names)\n",
    "\n",
    "# Rotate the tick labels for better readability\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(model_names)):\n",
    "    for j in range(len(scenario_names)):\n",
    "        text = ax.text(j, i, f\"{data[i, j]:.1f}%\", ha=\"center\", va=\"center\", color=\"black\", fontweight=\"bold\")\n",
    "\n",
    "ax.set_title(\"OOD Detection Rate by Model and Scenario\", fontsize=14, fontweight=\"bold\", pad=20)\n",
    "fig.colorbar(im, ax=ax, label=\"Detection Rate (%)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fabd49",
   "metadata": {},
   "source": [
    "ðŸ” What the Results Show:\n",
    "\n",
    "âœ… All models excel on Easy OOD (CIFAR10): 86-100% detection\n",
    "\n",
    "âš ï¸ Medium OOD (Rotations): Wide variation (5-87%)\n",
    "\n",
    "- KNN variants and GMM methods (with proper fusion) perform best\n",
    "- Cosine and Euclidean KNN may differ depending on embedding geometry\n",
    "- VAE struggles with limited training\n",
    "\n",
    "âŒ Hard OOD (Digits 8-9): Challenging for all (5-50%)\n",
    "\n",
    "- KNN variants are strongest (40-50%)\n",
    "- Cosine vs Euclidean performance gap depends on embedding structure\n",
    "- GMM methods competitive with proper score fusion (10-20%)\n",
    "- Standard AE provides baseline performance (20-25%)\n",
    "- VAE underperforms without extensive training (5-10%)\n",
    "\n",
    "ðŸ’¡ Takeaway: KNN with good embeddings and GMM methods with proper score fusion show the strongest performance. Comparing\n",
    "cosine and Euclidean distance reveals how embedding geometry affects detection. Simpler methods (AE) provide reliable\n",
    "baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f2f042",
   "metadata": {},
   "source": [
    "### Analysis: what the results show\n",
    "\n",
    "> **âš ï¸ Important Context**: These results are based on limited training (3 epochs) with small datasets (10K train, 3K\n",
    "> test) and generic model architectures. Performance patterns will differ significantly with more training, larger\n",
    "> datasets, and architectures optimized for your specific problem.\n",
    "\n",
    "#### Performance by OOD difficulty\n",
    "\n",
    "**Easy OOD (CIFAR10 - completely different domain):**\n",
    "\n",
    "- All methods achieve excellent detection (84-99%+)\n",
    "- Even simple approaches work well when OOD data is very different\n",
    "- GMM methods reach near-perfect detection (99%+)\n",
    "\n",
    "**Medium OOD (Rotated digits - same objects, different orientation):**\n",
    "\n",
    "- **KNN (both metrics)**: Strong performance (75-85%) - learned embeddings capture orientation-invariant features\n",
    "- **GMM methods**: Excellent with proper fusion (85-90%)\n",
    "- **Standard AE**: Moderate (50-55%) - reconstruction sensitive to orientation\n",
    "- **VAE**: Poor (5-10%) - insufficient training for robust latent structure\n",
    "\n",
    "**Hard OOD (Digits 8-9 - similar features to training data):**\n",
    "\n",
    "- **KNN (both metrics)**: Best performers (40-50%) - distance metrics in embedding space most discriminative\n",
    "- **Standard AE**: Reliable baseline (20-25%)\n",
    "- **GMM methods**: Competitive with tuning (10-20%) - sensitive to `gmm_weight` parameter\n",
    "- **VAE**: Struggles (5-10%) - needs extensive training to show advantages\n",
    "\n",
    "#### Summary observations\n",
    "\n",
    "1. **KNN with learned embeddings** consistently outperformed reconstruction-based methods\n",
    "1. **Cosine vs Euclidean**: Performance depends on embedding properties - cosine excels with normalized embeddings while\n",
    "   Euclidean captures magnitude differences\n",
    "1. **GMM score fusion is critical**: Proper `gmm_weight` (0.6-0.8) significantly impacts performance\n",
    "1. **VAE underperforms** with limited training - requires 10-20x more epochs to converge\n",
    "1. **Simpler methods (AE) provide reliable baselines** with minimal tuning\n",
    "1. **Performance gap narrows** as OOD difficulty decreases (all methods work well on easy OOD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d364eda8",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, you learned how to use DataEval's OOD detection capabilities with six different approaches: KNN with\n",
    "cosine distance, KNN with Euclidean distance, Standard AE, VAE, AE+GMM, and VAE+GMM.\n",
    "\n",
    "### Method selection guide\n",
    "\n",
    "Based on the comparative analysis across three OOD difficulty levels, here's how to choose the right method for your use\n",
    "case:\n",
    "\n",
    "#### When to choose cosine vs Euclidean distance\n",
    "\n",
    "When using KNN-based OOD detection, the choice of distance metric matters:\n",
    "\n",
    "**Choose cosine distance when:**\n",
    "\n",
    "- Your embeddings come from models that produce **normalized** or **direction-oriented** vectors (e.g., CLIP,\n",
    "  sentence-transformers, contrastive learning models)\n",
    "- You care about **semantic similarity** rather than absolute magnitude\n",
    "- Embedding dimensions vary in scale and you want to ignore that variation\n",
    "- Your embeddings are **high-dimensional** â€” cosine similarity is more robust to the \"curse of dimensionality\" than\n",
    "  Euclidean distance\n",
    "\n",
    "**Choose Euclidean distance when:**\n",
    "\n",
    "- Your embeddings come from models where **magnitude carries meaning** (e.g., autoencoders, raw feature extractors,\n",
    "  PCA-reduced features)\n",
    "- You want to capture **absolute differences** between samples, not just angular ones\n",
    "- Your embeddings are **low-dimensional** or have been standardized to similar scales\n",
    "- You are working with **raw pixel features** or **tabular data** where L2 distance is natural\n",
    "\n",
    "**In practice:**\n",
    "\n",
    "- Cosine is the safer default for pretrained model embeddings (ResNet, ViT, CLIP)\n",
    "- Euclidean works well when embeddings have been explicitly standardized or when magnitude is informative\n",
    "- When unsure, try both â€” as shown in this tutorial, the performance difference depends on the specific embedding space\n",
    "  and data distribution\n",
    "\n",
    "#### **Quick decision table:**\n",
    "\n",
    "| Your Situation                           | Recommended Method   | Why                                                  |\n",
    "| ---------------------------------------- | -------------------- | ---------------------------------------------------- |\n",
    "| Pretrained normalized embeddings         | **KNN (Cosine)**     | Best for direction-oriented embeddings               |\n",
    "| Embeddings where magnitude matters       | **KNN (Euclidean)**  | Captures absolute distance differences               |\n",
    "| Need fast baseline                       | **Standard AE**      | Simple, reliable, minimal tuning                     |\n",
    "| Multi-modal data clusters                | **AE + GMM**         | Enhanced detection with density modeling             |\n",
    "| Maximum accuracy (can train extensively) | **KNN or VAE + GMM** | KNN for strong embeddings, VAE+GMM for 30-50+ epochs |\n",
    "| Limited computational resources          | **Standard AE**      | Fastest training, good baseline                      |\n",
    "\n",
    "#### **By application domain:**\n",
    "\n",
    "| Domain               | Best Method        | Rationale                                                         |\n",
    "| -------------------- | ------------------ | ----------------------------------------------------------------- |\n",
    "| Medical imaging      | KNN or VAE+GMM     | Safety-critical, leverage pretrained models or extensive training |\n",
    "| Manufacturing QA     | AE+GMM or KNN      | Natural defect clusters, fast inference                           |\n",
    "| Fraud detection      | KNN or Standard AE | Clear separation, interpretable                                   |\n",
    "| Autonomous systems   | KNN                | Complex scenarios, use pretrained vision models                   |\n",
    "| Research/Prototyping | KNN or Standard AE | Quick iteration, establish baseline                               |\n",
    "\n",
    "### Implementation recommendations\n",
    "\n",
    "#### **For KNN (best overall)**\n",
    "\n",
    "```python\n",
    "# Train embedding model or use pretrained\n",
    "embedding_model = YourPretrainedModel()  # ResNet, ViT, CLIP, etc.\n",
    "\n",
    "# Create embeddings\n",
    "train_emb = Embeddings(train_data, model=embedding_model)\n",
    "test_emb = Embeddings(test_data, model=embedding_model)\n",
    "\n",
    "# Cosine distance â€” best for normalized/pretrained embeddings\n",
    "ood_knn_cos = OODKNeighbors(k=10, distance_metric=\"cosine\")\n",
    "ood_knn_cos.fit(train_emb, threshold_perc=95.0)\n",
    "result_cos = ood_knn_cos.predict(test_emb)\n",
    "\n",
    "# Euclidean distance â€” best when magnitude is informative\n",
    "ood_knn_euc = OODKNeighbors(k=10, distance_metric=\"euclidean\")\n",
    "ood_knn_euc.fit(train_emb, threshold_perc=95.0)\n",
    "result_euc = ood_knn_euc.predict(test_emb)\n",
    "```\n",
    "\n",
    "**Key Success Factors**:\n",
    "\n",
    "- Embedding quality â€” invest in domain-specific pretrained models\n",
    "- Distance metric â€” use cosine for normalized embeddings, Euclidean when magnitude matters\n",
    "\n",
    "#### **For standard AE (reliable baseline)**\n",
    "\n",
    "```python\n",
    "config = OODReconstruction.Config(\n",
    "    epochs=10,  # 10-20 for production\n",
    "    batch_size=256,\n",
    "    threshold_perc=95.0,\n",
    ")\n",
    "ood_ae = OODReconstruction(your_ae_model, device=device, config=config)\n",
    "```\n",
    "\n",
    "**Key Success Factor**: Architecture design - match to your data type\n",
    "\n",
    "#### **For GMM methods (advanced)**\n",
    "\n",
    "```python\n",
    "# Add GMM to your model\n",
    "gmm_net = GMMDensityNet(latent_dim=256, n_gmm=8)\n",
    "your_model.gmm_density_net = gmm_net\n",
    "\n",
    "# Configure fusion parameters\n",
    "config = OODReconstruction.Config(\n",
    "    epochs=15,  # 15-30 for AE+GMM, 30-50 for VAE+GMM\n",
    "    batch_size=256,\n",
    "    threshold_perc=95.0,\n",
    "    gmm_weight=0.7,  # Tune in [0.5, 0.9]\n",
    "    gmm_score_mode=\"standardized\",\n",
    ")\n",
    "```\n",
    "\n",
    "**Key Success Factors**:\n",
    "\n",
    "- Tune `gmm_weight` for your data (try 0.6-0.8)\n",
    "- Match `n_gmm` to natural data clusters\n",
    "- More training epochs than standard AE/VAE\n",
    "\n",
    "### Critical takeaways\n",
    "\n",
    "**âš ï¸ Results Context:**\n",
    "\n",
    "- This tutorial used minimal training (3 epochs) and generic architectures\n",
    "- Your results will improve significantly with:\n",
    "  - More training epochs (10-50+)\n",
    "  - Architectures designed for your data type\n",
    "  - Larger datasets and proper hyperparameter tuning\n",
    "  - Domain-specific pretrained models (for KNN)\n",
    "\n",
    "**What Matters Most:**\n",
    "\n",
    "1. **Embedding quality (KNN)**: Use pretrained models (ResNet, ViT, CLIP) or train task-specific embeddings\n",
    "1. **Architecture design (AE/VAE)**: Generic models shown here are examples - customize for your data\n",
    "1. **GMM configuration**: `gmm_weight` parameter critically impacts performance (0.6-0.8 range)\n",
    "1. **Training investment**: VAE needs 10-20x more epochs than shown here to reach potential\n",
    "1. **Threshold selection**: Balance false positives vs detection rate for your use case\n",
    "\n",
    "### Performance expectations\n",
    "\n",
    "Based on OOD similarity to in-distribution data:\n",
    "\n",
    "- **Easy OOD** (completely different): 85-100% detection with any method\n",
    "- **Medium OOD** (same domain, different features): 50-90% - KNN and GMM methods excel\n",
    "- **Hard OOD** (very similar): 10-50% - KNN best, requires careful tuning\n",
    "\n",
    "Remember: Digits 8-9 vs 0-7 is a **hard** OOD case (shared features). Real-world performance depends on your specific\n",
    "data distributions.\n",
    "\n",
    "### What's next\n",
    "\n",
    "To learn more about OOD detection and related concepts:\n",
    "\n",
    "- Read the [OOD Detection concept page](../concepts/OOD.md)\n",
    "- Learn about [monitoring operational data](./tt_monitor_shift.md)\n",
    "- Try the [data cleaning tutorial](./tt_clean_dataset.md)\n",
    "\n",
    "### Try it yourself\n",
    "\n",
    "Experiment with:\n",
    "\n",
    "- **Better embeddings for KNN**: ResNet, ViT, CLIP, or domain-specific pretrained models\n",
    "- **Distance metrics**: Compare cosine vs Euclidean on your embeddings to find the best fit\n",
    "- **More training**: 10-20 epochs for AE/AE+GMM, 30-50+ for VAE/VAE+GMM\n",
    "- **GMM tuning**: Try `gmm_weight` values in [0.5, 0.9] and different `n_gmm` (match to data clusters)\n",
    "- **Custom architectures**: Design models for your specific data type (not generic examples)\n",
    "- **Different OOD scenarios**: Test on your own data with varying difficulty levels\n",
    "- **Threshold adjustment**: Tune `threshold_perc` for your false positive tolerance\n",
    "- **Transfer learning**: Use pretrained models instead of training from scratch"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "default_lexer": "ipython3"
  },
  "kernelspec": {
   "display_name": "dataeval",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
