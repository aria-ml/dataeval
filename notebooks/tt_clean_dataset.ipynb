{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f56871a2",
   "metadata": {},
   "source": [
    "# Introduction to data cleaning\n",
    "\n",
    "Part 1 of our introduction to exploratory data analysis guide\n",
    "\n",
    "Estimated time to complete: 15 minutes\n",
    "\n",
    "Relevant ML stages: [Data Engineering](../concepts/users/ML_Lifecycle.md#data-engineering)\n",
    "\n",
    "Relevant personas: Data Engineer, ML Engineer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fff86f1",
   "metadata": {},
   "source": [
    "## What you'll do\n",
    "\n",
    "- You will use DataEval's cleaners to assess the 2012 VOC dataset.\n",
    "- You will analyze the results through various plots and tables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd93780c",
   "metadata": {},
   "source": [
    "## What you'll learn\n",
    "\n",
    "- You'll learn how to assess a dataset for extreme and/or redundant data points.\n",
    "- You'll learn helpful questions to determine when to remove or collect additional data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b5a708",
   "metadata": {},
   "source": [
    "## What you'll need\n",
    "\n",
    "- Environment Requirements\n",
    "  - `dataeval` or `dataeval[all]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a455a50d",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Exploratory Data Analysis (EDA) is an approach to analyzing data sets to summarize the main characteristics and identify\n",
    "incongruencies in the data. Before diving into machine learning or statistical modeling, it is crucial to understand the\n",
    "data you are working with. EDA helps in understanding the patterns, detecting anomalies, checking assumptions, and\n",
    "determining relationships in the data.\n",
    "\n",
    "One of the most important aspects of EDA is [data cleaning](../concepts/DataCleaning.md). A portion of DataEval is\n",
    "dedicated to being able to identify duplicates and [outliers](../concepts/Outliers.md) as well as data points that have\n",
    "missing or too many extreme values. These techniques help\n",
    "ensure that you only include high quality data for your projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df51af1d",
   "metadata": {},
   "source": [
    "### Step-by-step guide\n",
    "\n",
    "This guide will walk through how to use DataEval to perform basic data cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a252f71b",
   "metadata": {},
   "source": [
    "#### Setup\n",
    "\n",
    "You'll begin by importing the necessary libraries to walk through this guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a15bcae",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab  # noqa: F401\n",
    "\n",
    "    # specify the version of DataEval (==X.XX.X) for versions other than the latest\n",
    "    %pip install -q dataeval maite-datasets\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e06b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "from dataeval_plots import plot\n",
    "from maite_datasets.object_detection import VOCDetection\n",
    "\n",
    "from dataeval import Metadata\n",
    "\n",
    "# Load the classes from DataEval that are helpful for EDA\n",
    "from dataeval.config import set_max_processes\n",
    "from dataeval.core import calculate, label_stats\n",
    "from dataeval.flags import ImageStats\n",
    "from dataeval.quality import Duplicates, Outliers\n",
    "\n",
    "# Print all rows of dataframes\n",
    "_ = pl.Config.set_tbl_rows(-1)\n",
    "\n",
    "# Set the random value\n",
    "rng = np.random.default_rng(213)\n",
    "\n",
    "# Set multiprocessing for DataEval stats\n",
    "set_max_processes(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7452578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper method to plot sample images by class\n",
    "def plot_sample_images_by_class(dataset, image_indices_per_class) -> None:\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Plot random images from each category\n",
    "    _, axs = plt.subplots(5, 4, figsize=(8, 10))\n",
    "\n",
    "    for ax, (category, indices) in zip(axs.flat, image_indices_per_class.items(), strict=False):\n",
    "        # Randomly select an index from the list of indices\n",
    "        ax.imshow(dataset[rng.choice(indices)][0].transpose(1, 2, 0))\n",
    "        ax.set_title(dataset.metadata[\"index2label\"][category])\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Helper method to plot images of interest\n",
    "def plot_sample_outlier_images_by_metric(dataset, outlier_class, outlier_result, metric, layout) -> None:\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.cm import ScalarMappable\n",
    "    from matplotlib.colors import Normalize\n",
    "\n",
    "    # Filter issues DataFrame for the specific metric\n",
    "    metric_issues = outlier_result.issues.filter(pl.col(\"metric_name\") == metric)\n",
    "    image_ids = metric_issues[\"item_id\"].unique().to_list()\n",
    "\n",
    "    if not image_ids:\n",
    "        print(f\"No images flagged for metric: {metric}\")\n",
    "        return\n",
    "\n",
    "    # Get all metric values for the entire dataset to understand the distribution\n",
    "    all_metric_values = outlier_class.stats[\"stats\"][metric]\n",
    "    quantiles = np.quantile(all_metric_values, [0, 0.25, 0.5, 0.75, 1])\n",
    "    median = quantiles[2]\n",
    "\n",
    "    # Calculate distance from median for each flagged image and sort by distance (descending)\n",
    "    metric_values_with_ids = []\n",
    "    for img_id in image_ids:\n",
    "        metric_value = metric_issues.filter(pl.col(\"item_id\") == img_id)[\"metric_value\"][0]\n",
    "        distance_from_median = abs(metric_value - median)\n",
    "        metric_values_with_ids.append((img_id, metric_value, distance_from_median))\n",
    "\n",
    "    # Sort by distance from median (most outlier first)\n",
    "    metric_values_with_ids.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    # Create figure with space for colorbar\n",
    "    fig = plt.figure(figsize=(12, layout[0] * 4))\n",
    "\n",
    "    # Create grid spec with extra space for colorbar\n",
    "    import matplotlib.gridspec as gridspec\n",
    "\n",
    "    gs = gridspec.GridSpec(\n",
    "        layout[0],\n",
    "        layout[1] + 1,\n",
    "        width_ratios=[1] * layout[1] + [0.05],\n",
    "        hspace=0.4,\n",
    "        wspace=0.1,\n",
    "        left=0.05,\n",
    "        right=0.92,\n",
    "        top=0.92,\n",
    "        bottom=0.02,\n",
    "    )\n",
    "\n",
    "    # Determine number of samples to plot\n",
    "    n_samples = min(int(np.prod(layout)), len(image_ids))\n",
    "\n",
    "    # Create colormap normalization based on full metric distribution\n",
    "    vmin, vmax = quantiles[0], quantiles[4]\n",
    "    norm = Normalize(vmin=vmin, vmax=vmax)\n",
    "    cmap = plt.get_cmap(\"viridis\")\n",
    "\n",
    "    # Plot images\n",
    "    for i in range(n_samples):\n",
    "        row = i // layout[1]\n",
    "        col = i % layout[1]\n",
    "        ax = fig.add_subplot(gs[row, col])\n",
    "\n",
    "        img_id, metric_value, _ = metric_values_with_ids[i]\n",
    "\n",
    "        # Get color for this metric value\n",
    "        color = cmap(norm(metric_value))\n",
    "\n",
    "        # Plot image\n",
    "        ax.imshow(dataset[img_id][0].transpose(1, 2, 0))\n",
    "\n",
    "        # Place metric details as footer below the image using xlabel (black text for legibility)\n",
    "        ax.set_xlabel(f\"index: {img_id}\\n{metric}: {np.round(metric_value, 3)}\", fontsize=9, color=\"black\")\n",
    "\n",
    "        # Turn off ticks but keep spines for colored border\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "        # Add 5-pixel colored border to indicate extremeness\n",
    "        for spine in ax.spines.values():\n",
    "            spine.set_edgecolor(color)\n",
    "            spine.set_linewidth(5)\n",
    "            spine.set_visible(True)\n",
    "\n",
    "    # Add colorbar with quantile markers\n",
    "    cbar_ax = fig.add_subplot(gs[:, -1])\n",
    "    sm = ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, cax=cbar_ax)\n",
    "    cbar.set_ticks([])\n",
    "\n",
    "    # Add quantile markers on colorbar\n",
    "    quantile_labels = [\"Min (Q0)\", \"Q1 (25%)\", \"Median (Q2)\", \"Q3 (75%)\", \"Max (Q4)\"]\n",
    "    for q_val, q_label in zip(quantiles, quantile_labels, strict=False):\n",
    "        cbar.ax.axhline(q_val, color=\"black\", linestyle=\"--\", linewidth=0.8, alpha=0.7)\n",
    "        cbar.ax.text(\n",
    "            1.5,\n",
    "            q_val,\n",
    "            f\"{q_label}\\n{np.round(q_val, 2)}\",\n",
    "            va=\"center\",\n",
    "            fontsize=8,\n",
    "            transform=cbar.ax.get_yaxis_transform(),\n",
    "        )\n",
    "\n",
    "    # Add overall title with more top space\n",
    "    fig.suptitle(f'Outlier Images for \"{metric}\" (sorted by distance from median)', fontsize=12, y=0.99)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93734b8",
   "metadata": {},
   "source": [
    "## Step 1: Understand the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcea062",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "You are going to work with the PASCAL VOC 2012 dataset. This dataset is a small curated dataset that was used for a\n",
    "computer vision competition. The images were used for classification, object detection, and segmentation. This dataset\n",
    "was chosen because it has multiple classes and images with a variety of sizes and objects.\n",
    "\n",
    "If this data is already on your computer you can change the file location from `\"./data\"` to wherever the data is\n",
    "stored. Just remember to also change the download value from `True` to `False`.\n",
    "\n",
    "For the sake of ensuring that this tutorial runs quickly on most computers, you are going to analyze only the training\n",
    "set of the data, which is a little under 6000 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8eeb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data and then load it as a torch Tensor\n",
    "ds = VOCDetection(\"./data\", image_set=\"train\", year=\"2012\", download=True)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a48b4ef",
   "metadata": {},
   "source": [
    "### Inspect the data\n",
    "\n",
    "As this data was used for a computer vision competition, it will most likely have very few issues, but it is always\n",
    "worth it to check. Many of the large webscraped datasets available for use do contain image issues. Verifying in the\n",
    "beginning that you have a high quality dataset is always easier than finding out later that you trained a model on a\n",
    "dataset with erroneous images or a set of splits with leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aa2559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Metadata object\n",
    "md = Metadata(ds)\n",
    "\n",
    "lstats = label_stats(md.class_labels, md.item_indices, ds.index2label, image_count=len(ds))\n",
    "\n",
    "# View per_class data as a DataFrame\n",
    "pl.DataFrame(\n",
    "    {\n",
    "        \"class_name\": list(ds.index2label.values()),\n",
    "        \"label_count\": [lstats[\"label_counts_per_class\"][k] for k in ds.index2label],\n",
    "        \"image_count\": [lstats[\"image_counts_per_class\"][k] for k in ds.index2label],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425c3c5d",
   "metadata": {},
   "source": [
    "The above table shows that this dataset has a total of 20 classes.\n",
    "\n",
    "Of the classes, `person` is the class with the highest total object count followed by `chair` and `car`, while `person`,\n",
    "`chair` and `dog` are the classes with the highest number of images.\n",
    "\n",
    "`cow`, `sheep`, and `bus` are the classes with least number of objects, while `bus`, `train` and `cow` are the classes\n",
    "with the least number of images.\n",
    "\n",
    "This table helps point out the wide variation in\n",
    "\n",
    "- the number of classes per image,\n",
    "- the number of objects per image,\n",
    "- and the number of objects of each class per image.\n",
    "\n",
    "This highlights an important concept - [class balance](../concepts/ClassBalancing.md). A dataset that is imbalanced can\n",
    "result in a model that chooses\n",
    "the more prominent class more often just because there are more samples in that class. To explore this concept further,\n",
    "see the bias tutorial in the [What's Next](#whats-next) section at the end of this tutorial.\n",
    "\n",
    "Now that the metadata has been examined, it's important to inspect random images to get an idea of the variety of\n",
    "backgrounds, the range of colors, the locations of objects in images, and how often an image is seen with a single\n",
    "object versus multiple objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a44b52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_images_by_class(ds, lstats[\"image_indices_per_class\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6b832d",
   "metadata": {},
   "source": [
    "Plotting the images displays the variety in the images, including image sizes, image brightness, object sizes,\n",
    "backgrounds, number of objects in the image, and even the lack of color in a few images which are black and white.\n",
    "\n",
    "This is where DataEval comes in. It's designed to help you make sense of the many different aspects that affect building\n",
    "representative datasets and robust models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fcf403",
   "metadata": {},
   "source": [
    "In addition to making sure that you understand the structure of the labels and have visualized some of the images from\n",
    "the dataset, you can also visualize the data distribution across different statistics such as the image size or the\n",
    "pixel mean. In order to view these distributions, you have to use DataEval's stat functions and plot the results.\n",
    "\n",
    "Now, you can move on to identifying which images have a statistical difference from the rest of the images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81a05b2",
   "metadata": {},
   "source": [
    "## Step 2: Identify any outlying data points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c5f692",
   "metadata": {},
   "source": [
    "### Extreme/missing values\n",
    "\n",
    "Here you will detect and identify the images associated with the extreme values from DataEval's stat functions. To\n",
    "detect these extreme values, you will use the :class:`.Outliers` class. The `Outliers` class has multiple methods to\n",
    "determine the extreme values, which are discussed in the [Data Cleaning explanation](../concepts/DataCleaning.md). For\n",
    "this guide, you will use the \"zscore\" as the Z score defines outliers in a normal distribution.\n",
    "\n",
    "The output of the `Outliers` class contains a DataFrame with columns for image_id, metric_name, and metric_value for\n",
    "each flagged outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf1f3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell takes about 1-5 minutes to run depending on your hardware\n",
    "\n",
    "# Initialize the Outliers class\n",
    "outliers = Outliers(outlier_method=\"zscore\")\n",
    "\n",
    "# Find the extreme images\n",
    "outlier_imgs = outliers.evaluate(ds, per_target=False)\n",
    "\n",
    "# View the number of extreme images\n",
    "print(f\"Number of images with extreme values: {len(outlier_imgs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3526472",
   "metadata": {},
   "source": [
    "This class can flag a lot of images, depending on how varied the dataset is and which method you use to define extreme\n",
    "values. Using the zscore, it flagged 480 images across 15 metrics out of the 5717 images in the dataset. However,\n",
    "switching the method can give different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dff8bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the metrics with an extreme value\n",
    "outlier_imgs.aggregate_by_metric()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ab8692",
   "metadata": {},
   "source": [
    "Digging into the flagged images and organizing them by category shows that the metric with the most extreme values is\n",
    "\"size\" while \"sharpness\" has the least number of extreme values.\n",
    "\n",
    "`Outliers` is designed to flag any images on the edge of each metric's data distribution. Some images will get flagged\n",
    "as an outlier by multiple metrics, while others will get flagged by only a single metric. It is then up to you, the\n",
    "user, to shift through the information provided by the result from `Outliers`.\n",
    "\n",
    "Part of exploring the results includes displaying how the flagged images are spread across the 20 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf61b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the outliers by class label\n",
    "outlier_imgs.aggregate_by_class(md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f35e171",
   "metadata": {},
   "source": [
    "Some of the trends to note from the table above which splits the issues by class and metric:\n",
    "\n",
    "- An image with an unusual aspect ratio is most likely to contain a bottle or person.\n",
    "- An image with an issue in brightness is most likely to contain an aeroplane.\n",
    "- An image with an issue in darkness is most likely to be a person.\n",
    "- Images with high contrast are likely to fall within 1 of 4 classes: bottle, cat, chair, person.\n",
    "- Images with low entropy (think image with constant pixels) are likely to fall within 1 of 4 classes: aeroplane, bird,\n",
    "  bottle, person.\n",
    "- Unusual skew and kurtosis images follow a similar trend as entropy.\n",
    "- Every class has images with size issues.\n",
    "\n",
    "Something to remember is that there are different number of images for each class and that effective use of this tool\n",
    "requires understanding the dataset in question. For example, 36 low entropy images out of the 2000 for person might be\n",
    "outliers while 28 low entropy images out of 300 for aeroplane might not be; low entropy might be an inherent\n",
    "characteristic of the aeroplane class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be5693a",
   "metadata": {},
   "source": [
    "In order to understand the above table, you will plot sample images from a few of the metrics, specifically:\n",
    "\n",
    "- entropy\n",
    "- size\n",
    "- zeros\n",
    "- sharpness\n",
    "\n",
    "Entropy, variance, standard deviation, kurtosis, and skew all measure (in different ways) how much change there is\n",
    "across the pixels in the image, and entropy will be the easiest to understand.\n",
    "\n",
    "Size, width, height and aspect ratio are all interrelated and size has the most extreme images from those.\n",
    "\n",
    "Zeros is a category unto itself but it is closely related to brightness, contrast, darkness, and mean. Zeros measures\n",
    "the percentage of pixels with a zero value compared to the average image.\n",
    "\n",
    "Sharpness is also in it's own category and it measures the perceived edges in an image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca16d674",
   "metadata": {},
   "source": [
    "(questions)=\n",
    "\n",
    "#### Questions\n",
    "\n",
    "When looking at these images, you want to think about the following questions:\n",
    "\n",
    "- Does this image represent something that would be expected in operation?\n",
    "- Is there commonality to the objects in the images?\n",
    "- Is there commonality to the backgrounds of the images?\n",
    "- Is there commonality to the class of objects in the images?\n",
    "\n",
    "Asking these questions will help you notice things like all objects being located on the leftside of the image or all\n",
    "the images of a specific class have a specific background. Training a model with data that has commonalities can cause\n",
    "your model to develop biases or limit your model's ability to generalize to non-training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2813d0",
   "metadata": {},
   "source": [
    "### Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b192afc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot images flagged for \"entropy\"\n",
    "plot_sample_outlier_images_by_metric(ds, outliers, outlier_imgs, \"entropy\", (1, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4f0c03",
   "metadata": {},
   "source": [
    "When you examine the flagged images for entropy, look for patterns in the content of the images. Many of these images\n",
    "may feature backgrounds with very little variation, such as water or sky. Others might have darker backgrounds than\n",
    "usual.\n",
    "\n",
    "For example, in an operational setting, water or sky backgrounds may or may not appear frequently, depending on the\n",
    "expected use case. Similarly, darker images may indicate low-light conditions, which could suggest either operational\n",
    "relevance (e.g., night operations) or anomalies that need to be addressed.\n",
    "\n",
    "To refine your dataset, decide whether these flagged images represent scenarios that align with your goals. If they do,\n",
    "consider collecting more data with similar characteristics to balance your dataset. If not, these images may be excluded\n",
    "as outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b53fe74",
   "metadata": {},
   "source": [
    "### Aspect ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfea5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot images flagged for \"aspect_ratio\"\n",
    "plot_sample_outlier_images_by_metric(ds, outliers, outlier_imgs, \"aspect_ratio\", (1, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea24c47",
   "metadata": {},
   "source": [
    "Flagged images for aspect ratio often include examples where the objects in the image are unusually wide or tall\n",
    "relative to the rest of the dataset. For instance, bottle images might by cropped tall.\n",
    "\n",
    "If your workflow involves preprocessing images to a uniform size, verify that resizing does not distort important\n",
    "details. For example, cropping could remove key parts of the image, while resizing could stretch or compress objects.\n",
    "Alternatively, if you plan to filter images based on size, ensure this doesn’t introduce bias—for example, by\n",
    "disproportionately excluding images of certain classes or contexts.\n",
    "\n",
    "After evaluating the flagged images, you may notice that dimensional discrepancies are common across multiple classes,\n",
    "as shown in the earlier table. This observation suggests that these issues are a general feature of the dataset, and\n",
    "dropping all size outliers might be an appropriate step. However, be cautious and verify whether this action creates any\n",
    "imbalances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b1bb78",
   "metadata": {},
   "source": [
    "### Zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e725ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot images flagged for \"zeros\"\n",
    "plot_sample_outlier_images_by_metric(ds, outliers, outlier_imgs, \"zeros\", (1, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348ec8b3",
   "metadata": {},
   "source": [
    "Images flagged for zeros typically feature large regions of completely black or gray pixels. Some of these may also\n",
    "appear in grayscale. These characteristics could indicate issues like underexposed photos, scanning errors, or specific\n",
    "use cases.\n",
    "\n",
    "Grayscale images, in particular, might stand out if the rest of your dataset is primarily in color. Check whether\n",
    "grayscale images are relevant to your operational scenario or whether they are artifacts of the data collection process.\n",
    "\n",
    "For instance, if grayscale images are operationally irrelevant, consider removing them. However, if grayscale scenarios\n",
    "are possible, ensure that you have sufficient representation of these types of images to train a robust model.\n",
    "Similarly, dark images with many zero-value pixels may indicate rare but valid scenarios (e.g., nighttime operations) or\n",
    "irrelevant anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f0e01e",
   "metadata": {},
   "source": [
    "### Sharpness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639dc5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot images flagged for \"sharpness\"\n",
    "plot_sample_outlier_images_by_metric(ds, outliers, outlier_imgs, \"sharpness\", (1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f275ab6",
   "metadata": {},
   "source": [
    "Sharpness measures the clarity of edges in an image. Flagged images often include those with unusually crisp or blurry\n",
    "details. For instance, you might notice a close-up shot of leaves or grass, where the texture stands out significantly\n",
    "compared to other images in the dataset.\n",
    "\n",
    "Evaluate whether these highly detailed images are typical of your use case. If they are uncommon in your operational\n",
    "scenario, they might skew your model's ability to generalize. In such cases, consider excluding these images.\n",
    "Conversely, if they are operationally relevant, ensure that similar images are sufficiently represented in your dataset\n",
    "to prevent biases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b55488",
   "metadata": {},
   "source": [
    "## Cleaning summary\n",
    "\n",
    "The Outliers class identifies images that deviate significantly from the dataset's overall distribution. While it cannot\n",
    "determine operational relevance, it highlights patterns that may require further investigation.\n",
    "\n",
    "For example, flagged images might reflect real-world scenarios underrepresented in your dataset, such as night\n",
    "operations or objects photographed from unusual angles. Alternatively, they may reveal anomalies, such as artifacts from\n",
    "the data collection process.\n",
    "\n",
    "By reviewing flagged images for multiple metrics and examples, you can better understand how the Outliers class\n",
    "identifies extremes. This hands-on exploration helps you decide whether to include or exclude specific images based on\n",
    "your dataset's intended use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89f0b1b",
   "metadata": {},
   "source": [
    "## Step 3: Identify duplicate data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ba917c",
   "metadata": {},
   "source": [
    "### Duplicates\n",
    "\n",
    "Now that you know how to identify poor quality images in your dataset, another important aspect of data cleaning is\n",
    "detecting and removing any duplicates.\n",
    "\n",
    "The `Duplicates` class identifies both exact duplicates and potential (near) duplicates. Potential duplicates can occur\n",
    "in a variety of ways:\n",
    "\n",
    "- Intentional perturbations\n",
    "  - Images with varying brightness\n",
    "  - Translating the image\n",
    "  - Padding the image\n",
    "  - Cropping the image\n",
    "- Unintentional changes\n",
    "  - Copying the image from one format to another (png->jpeg)\n",
    "  - Using the same image with two different filenames\n",
    "  - Duplicate frames from video extraction\n",
    "  - Oversight in the data collection process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24facfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Duplicates class\n",
    "dups = Duplicates(ImageStats.HASH)\n",
    "\n",
    "# Find the duplicates\n",
    "results = dups.evaluate(ds, per_target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91920f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Exact image duplicates: {results.items.exact}\")\n",
    "print(f\"Near image duplicates: {results.items.near}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b56dcd",
   "metadata": {},
   "source": [
    "As expected there are no duplicate images in this dataset, since it was curated for a specific competition. But there\n",
    "are 2 near duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73e691c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot(ds, figsize=(12, 6), indices=(1548, 1561))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecf6021",
   "metadata": {},
   "source": [
    "As you can see, the image is indeed a near duplicate; image 1561 is a grayscale version of image 1548."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd356f30",
   "metadata": {},
   "source": [
    "To highlight the abilities of the `Duplicates` class, you will add some duplicates to the dataset and then rerun the\n",
    "`Duplicates` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b207d224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create exact and duplicate images\n",
    "\n",
    "# Copy images 23 and 46 to create exact duplicates\n",
    "# Copy and crop images 5 and 4376 to create near duplicates\n",
    "# Rotate image 100 by 90 degrees to create a rotated duplicate\n",
    "# Mirror and rotate image 200 to create a mirrored+rotated duplicate\n",
    "dupes = [\n",
    "    ds[23][0],  # exact duplicate\n",
    "    ds[46][0],  # exact duplicate\n",
    "    ds[5][0][:, 2:-2, 2:-2],  # cropped near duplicate\n",
    "    ds[4376][0][:, :-5, 5:],  # cropped near duplicate\n",
    "    np.rot90(ds[100][0], k=1, axes=(1, 2)),  # 90° rotation\n",
    "    np.flip(np.rot90(ds[200][0], k=2, axes=(1, 2)), axis=2),  # 180° rotation + horizontal flip\n",
    "]\n",
    "\n",
    "dupes_stats = calculate(dupes, None, ImageStats.HASH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beadc1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the duplicates appended to the dataset\n",
    "duplicates = dups.from_stats([dups.stats, dupes_stats])\n",
    "\n",
    "print(f\"Exact duplicates: {duplicates.items.exact}\")\n",
    "\n",
    "# Distinguish same-orientation vs rotated/flipped duplicates\n",
    "print(\"\\nNear duplicates:\")\n",
    "for group in duplicates.items.near or []:\n",
    "    print(f\"Group of duplicates: {group}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4bf26a",
   "metadata": {},
   "source": [
    "As shown above, the `Duplicates` class identified all images from the second dataset as exact or near duplicates.\n",
    "\n",
    "- **Exact duplicates**: Images 23 and 46 from dataset 0 are identified as exact duplicates of images 0 and 1 from\n",
    "  dataset 1 respectively.\n",
    "- **Same-orientation near duplicates**: Images 5 and 4376 from dataset 0 are identified as near duplicates of images 2\n",
    "  and 3 from dataset 1 (cropped versions). These are detected by both basic hashes (phash, dhash) and D4 hashes.\n",
    "- **Rotated/flipped duplicates**: Images 100 and 200 from dataset 0 are identified as duplicates of images 4 and 5 from\n",
    "  dataset 1 (rotated and mirrored+rotated versions). These are detected **only** by D4 hashes (phash_d4, dhash_d4)\n",
    "  because the basic perceptual hashes are orientation-sensitive.\n",
    "\n",
    "By using `ImageStats.HASH` (which computes both basic and D4 hashes), you can distinguish between same-orientation\n",
    "duplicates and rotated/flipped duplicates by checking which methods detected the group. This is useful when you want to:\n",
    "\n",
    "- Keep one version of each rotated duplicate set\n",
    "- Identify images that may have been augmented with rotations\n",
    "- Detect unintentional orientation variations in your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649f16cb",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c40919",
   "metadata": {},
   "source": [
    "Through this process, you've learned how to use DataEval's `Outliers` class to identify and analyze images that deviate\n",
    "from the overall distribution of your dataset and DataEval's `Duplicates` class to identify exact and near duplicates.\n",
    "By examining the images flagged by the different metrics, you gained a deeper understanding of potential issues within\n",
    "your dataset. In this tutorial, the following were covered:\n",
    "\n",
    "- **Underrepresented classes** that may require additional data collection.\n",
    "- **Inconsistencies in image characteristics**, such as brightness, sharpness, or size, which could affect model\n",
    "  performance.\n",
    "- **Duplicate data** that can affect model performance.\n",
    "\n",
    "This work has provided a clearer picture of your dataset's strengths and limitations. You are now equipped to make\n",
    "informed decisions about which data points to keep, remove, or augment. For example, you may decide to exclude\n",
    "irrelevant outliers, collect more data for underrepresented scenarios, or address biases that could impact your model's\n",
    "generalizability.\n",
    "\n",
    "By using DataEval, you are not just refining your dataset—you are laying the groundwork for creating a more\n",
    "representative, balanced, and reliable dataset. These insights ultimately enable the development of models that perform\n",
    "robustly in real-world operational settings.\n",
    "\n",
    "DataEval’s tools empower you to move from raw data to actionable insights, ensuring your dataset is not only\n",
    "comprehensive but also aligned with your specific goals and requirements.\n",
    "\n",
    "Good luck with your data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae2b637",
   "metadata": {},
   "source": [
    "## What's next\n",
    "\n",
    "Learn how to do the following:\n",
    "\n",
    "- [Assess the data space](./tt_assess_data_space.md)\n",
    "- [Identify bias and correlations](./tt_identify_bias.md)\n",
    "- [Monitor shifting operational data](./tt_monitor_shift.md)\n",
    "\n",
    "To learn more about specific functions or classes, see the [API Reference](../reference/autoapi/dataeval/index.rst)\n",
    "section. To learn more about data cleaning, see the [Data Cleaning](../concepts/DataCleaning.md) explanation page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f968d3",
   "metadata": {},
   "source": [
    "## On your own\n",
    "\n",
    "Now that you've gone through a tutorial on exploring a dataset, try going through the tutorial again with the test set,\n",
    "full dataset, or even your own dataset. One thing to look for when checking other sets of data is to observe how the\n",
    "stats of each grouping of data changes or doesn't change.\n",
    "\n",
    "You can also play around with the different statistical methods that the `Outlier` class employs to see how the method\n",
    "affects the number and type of issues detected."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "default_lexer": "ipython3"
  },
  "kernelspec": {
   "display_name": "dataeval",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
