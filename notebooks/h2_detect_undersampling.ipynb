{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f595f29",
   "metadata": {},
   "source": [
    "# How to detect undersampled data subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40959550",
   "metadata": {},
   "source": [
    "## Problem statement\n",
    "\n",
    "For most computer vision tasks like **image classification** and **object detection**, we often have a lot of images,\n",
    "but certain subsets of the images can be undersampled, such as label, style within a label, etc. A way to detect this\n",
    "regional sparsity is through [coverage](../concepts/Coverage.md) analysis.\n",
    "\n",
    "To help with this, DataEval has introduced a {func}`.coverage` function, that provides a user with example images which\n",
    "have few similar instances within the provided dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57af11b",
   "metadata": {},
   "source": [
    "### When to use\n",
    "\n",
    "The `coverage` function should be used when you have lots of images, but only a small fraction from certain\n",
    "regimes/labels.\n",
    "\n",
    "### What you will need\n",
    "\n",
    "1. Image classification dataset.\n",
    "1. Autoencoder trained on image classification dataset for dimension reduction.\n",
    "1. A Python environment with the following packages installed:\n",
    "   - `dataeval`\n",
    "   - `tabulate`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5048e1af",
   "metadata": {},
   "source": [
    "### Getting started\n",
    "\n",
    "Let's import the required libraries needed to set up a minimal working example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9322d9",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Google Colab Only\n",
    "try:\n",
    "    import base64\n",
    "    import io\n",
    "    import json\n",
    "\n",
    "    import google.colab  # noqa: F401\n",
    "    import torch\n",
    "\n",
    "    # specify the version of DataEval (==X.XX.X) for versions other than the latest\n",
    "    %pip install -q dataeval maite-datasets\n",
    "    !export LC_ALL=\"en_US.UTF-8\"\n",
    "    !export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
    "    !export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
    "    !ldconfig /usr/lib64-nvidia\n",
    "\n",
    "    # Code below is to download the pretrained model weights stored on github\n",
    "    !mkdir models\n",
    "    !curl -o gitlfsbinary https://api.github.com/repos/aria-ml/dataeval/git/blobs/ad520d5589fdc49830f98d28aa5eaed0bbdfe5cb\n",
    "\n",
    "    with open(\"gitlfsbinary\") as f:\n",
    "        rawfile = json.load(f)\n",
    "\n",
    "    binaryfile = base64.b64decode(rawfile[\"content\"])\n",
    "    buffer = io.BytesIO(binaryfile)\n",
    "\n",
    "    temp = torch.load(buffer, weights_only=False)\n",
    "    torch.save(temp, \"models/ae\")\n",
    "\n",
    "    del rawfile\n",
    "    del binaryfile\n",
    "    del buffer\n",
    "    del temp\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "%pip install -q tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588fc203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from maite_datasets.image_classification import MNIST\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from dataeval import Embeddings, Metadata\n",
    "from dataeval.core import coverage_adaptive\n",
    "from dataeval.extractors import TorchExtractor\n",
    "from dataeval.selection import Limit, Select\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fe4e30",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "Load the MNIST data and create the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63318de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds\n",
    "torch.manual_seed(14)\n",
    "\n",
    "transforms = [\n",
    "    lambda x: x / 255.0,  # scale to [0, 1]\n",
    "    lambda x: (x - 0.1307) / 0.3081,  # normalize\n",
    "    lambda x: x.astype(np.float32),  # convert to float32\n",
    "]\n",
    "\n",
    "# MNIST with mean 0 unit variance\n",
    "train_ds = MNIST(root=\"./data\", image_set=\"train\", transforms=transforms, download=True)\n",
    "\n",
    "# Select a subset of the dataset\n",
    "subset = Select(train_ds, Limit(2000))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829e1f64",
   "metadata": {},
   "source": [
    "In this tutorial, we will use an autoencoder to reduce the dimension of the MNIST images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0d4159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            # 28 x 28\n",
    "            nn.Conv2d(1, 4, kernel_size=5),\n",
    "            # 4 x 24 x 24\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(4, 8, kernel_size=5),\n",
    "            nn.ReLU(True),\n",
    "            # 8 x 20 x 20 = 3200\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3200, 10),\n",
    "            # 10\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            # 10\n",
    "            nn.Linear(10, 400),\n",
    "            # 400\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(400, 4000),\n",
    "            # 4000\n",
    "            nn.ReLU(True),\n",
    "            nn.Unflatten(1, (10, 20, 20)),\n",
    "            # 10 x 20 x 20\n",
    "            nn.ConvTranspose2d(10, 10, kernel_size=5),\n",
    "            # 24 x 24\n",
    "            nn.ConvTranspose2d(10, 1, kernel_size=5),\n",
    "            # 28 x 28\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5093f52f",
   "metadata": {},
   "source": [
    "For computational reasons, we will simply load the trained autoencoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e381cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The trained autoencoder was trained for 1000 epochs\n",
    "sd = torch.load(\"models/ae\", weights_only=True)\n",
    "model = Autoencoder()\n",
    "model.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48a1bed",
   "metadata": {},
   "source": [
    "For the purposes of this example, we will take only the first 2000 entries of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2379e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create extractor using the autoencoder's encoder portion\n",
    "extractor = TorchExtractor(model.encoder)\n",
    "\n",
    "# Calculate the embeddings and extract the labels from the dataset\n",
    "embeddings = Embeddings(subset, extractor=extractor, batch_size=64)\n",
    "labels = Metadata(subset).class_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88a6732",
   "metadata": {},
   "source": [
    "To visualize the encodings, we will use TSNE on them to view separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65af1f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 10d as 2d with TSNE\n",
    "tsne = TSNE(n_components=2)\n",
    "red_dim = tsne.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f39712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results with color being label\n",
    "fig, ax = plt.subplots()\n",
    "scatter = ax.scatter(\n",
    "    x=red_dim[:, 0],\n",
    "    y=red_dim[:, 1],\n",
    "    c=labels,\n",
    "    label=labels,\n",
    ")\n",
    "ax.legend(*scatter.legend_elements(), loc=\"upper right\", ncols=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f175b25d",
   "metadata": {},
   "source": [
    "Some good separation, but you can see a few images in the \"gaps\". This could be an artifact of dimension reduction, or\n",
    "suggest that we have poor coverage for some covariates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeca26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use data adaptive cutoff\n",
    "cvrg = coverage_adaptive(embeddings, 20, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a72ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the least covered 1%\n",
    "f, axs = plt.subplots(4, 5, figsize=(5, 5))\n",
    "axs = axs.flatten()\n",
    "for count, i in enumerate(axs):\n",
    "    idx = cvrg[\"uncovered_indices\"][count]\n",
    "    i.imshow(np.squeeze(train_ds[idx][0]), cmap=\"gray\")\n",
    "    i.set_axis_off()\n",
    "    i.title.set_text(int(labels[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe965ac7",
   "metadata": {},
   "source": [
    "The Coverage tool identified that in this set of 2000 images, there is potential under-coverage when it comes to wonky\n",
    "2s and 7s. Other digits have some undercovered instances, but could be they are just outliers.\n",
    "\n",
    "<!--\n",
    "More investigation into outlier status is needed, see\n",
    "[How to identify outliers and/or anomalies in a dataset](ClustererTutorial.ipynb)\n",
    "for more info.\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3896a8d4",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "### TEST ASSERTION CELL ###\n",
    "wonky = sum(labels[i] == 2 or labels[i] == 7 for idx, i in enumerate(cvrg[\"uncovered_indices\"]) if idx < 20)\n",
    "print(wonky)\n",
    "assert (wonky / 20) > 0.4"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "default_lexer": "ipython3"
  },
  "kernelspec": {
   "display_name": "dataeval",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
