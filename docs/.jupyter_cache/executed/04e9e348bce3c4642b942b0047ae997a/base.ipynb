{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Google Colab Only\n",
    "try:\n",
    "    import google.colab  # noqa: F401\n",
    "\n",
    "    %pip install -q daml[torch]\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "import os\n",
    "\n",
    "from pytest import approx\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms.v2 as v2\n",
    "from torch.utils.data import Subset\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_tensor = v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])\n",
    "training_dataset = MNIST(root=\"./data/\", train=True, transform=to_tensor, download=True)\n",
    "testing_dataset = MNIST(root=\"./data/\", train=False, transform=to_tensor, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: torch.Size([60000, 28, 28])\n",
      "Training labels size: torch.Size([60000])\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data size:\", training_dataset.data.shape)\n",
    "print(\"Training labels size:\", training_dataset.targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from daml.models.torch import AETrainer, AriaAutoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AriaAutoencoder(channels=1)\n",
    "trainer = AETrainer(model, device=device, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11283737234771252\n"
     ]
    }
   ],
   "source": [
    "training_subset = Subset(training_dataset, range(6000))\n",
    "training_loss = trainer.train(training_subset, epochs=10)\n",
    "print(training_loss[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11400804466332871\n"
     ]
    }
   ],
   "source": [
    "eval_loss = trainer.eval(testing_dataset)\n",
    "print(eval_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11283737234771252\n",
      "0.11400804466332871\n"
     ]
    }
   ],
   "source": [
    "### TEST ASSERTION ###\n",
    "print(training_loss[-1])\n",
    "print(eval_loss)\n",
    "assert training_loss[-1] == approx(0.112837, abs=1e-4)\n",
    "assert eval_loss == approx(0.114008, abs=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = trainer.encode(training_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6000, 64, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "### TEST ASSERTION ###\n",
    "print(embeddings.shape)\n",
    "assert embeddings.shape == torch.Size([6000, 64, 6, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded image shape: torch.Size([6000, 64, 6, 6])\n"
     ]
    }
   ],
   "source": [
    "print(\"Embedded image shape:\", embeddings.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}