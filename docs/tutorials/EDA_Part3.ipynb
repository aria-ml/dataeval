{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying Bias and Correlations Guide\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What you'll do\n",
    "\n",
    "You will learn to identify bias and correlations, understand their impact on your data, and mitigate them in the future.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What you'll learn\n",
    "\n",
    "- You'll learn how to use DataEval's `balance`, `diversity` and `parity` functions to identify bias and correlations present in a dataset.\n",
    "- You'll be able to create a workflow using DataEval for identifying bias.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What you'll need\n",
    "\n",
    "- Environment Requirements\n",
    "  - `torchvision`\n",
    "  - `matplotlib`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Identifying any biases or correlations present in a dataset is essential to accurately interpreting your model's performance and its ability to generalize to new data.\n",
    "A common cause of poor generalization is shortcut learning &mdash; where a model uses secondary or background information to make predictions &mdash; which is enabled or exacerbated by dataset sampling biases.\n",
    "Understanding biases or correlations present in your dataset is a key component to creating meaningful data splits.\n",
    "Bias in data can lead to misleading conclusions and poor model performance on operational data.\n",
    "There are many different [types of bias](https://arxiv.org/abs/1908.09635).\n",
    "A few of these biases occur during data collection, others occur during dataset development, others occur during model development, while others are a result of the user.\n",
    "This guide does not focus on elminiating all bias. It focuses on identifying the bias that can be found when developing a dataset.\n",
    "\n",
    "DataEval has three dedicated methods for identifying and understanding the bias or correlations that may be present in a dataset, the `balance`, `diversity` and `parity` functions.\n",
    "The balance function measures correlational relationships between metadata factors and classes by calculating the mutual information between the metadata factors and the labels.\n",
    "The diversity function measures the evenness or uniformity of the sampling of metadata factors over a dataset using the inverse Simpson index or Shannon index.\n",
    "The parity function measures the relationship between metadata factors and classes using a chi-squared test.\n",
    "\n",
    "These techniques help ensure that when you split the data for your projects, you minimize things like shortcut learning and leakage between training and testing sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the necessary libraries\n",
    "\n",
    "You'll begin by importing the necessary libraries to walk through this guide.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab  # noqa: F401\n",
    "\n",
    "    # specify the version of DataEval (==X.XX.X) for versions other than the latest\n",
    "    %pip install -q dataeval[torch]\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will need matplotlib for visualing your results and numpy to be able to handle the data.\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# You will only use torchvision to load in the dataset.\n",
    "# If you already have the data stored on your computer in a numpy friendly manner,\n",
    "# then feel free to load it directly into numpy arrays.\n",
    "import torchvision.transforms.v2 as v2\n",
    "from torchvision import datasets\n",
    "\n",
    "# Load the classes from DataEval that are helpful for bias\n",
    "from dataeval.metrics.bias import balance, diversity, parity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is code to help create the visualizations for the bias metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a helper plotting function\n",
    "def heatmap(\n",
    "    data,\n",
    "    row_labels,\n",
    "    col_labels,\n",
    "    xlabel=\"\",\n",
    "    ylabel=\"\",\n",
    "    top=True,\n",
    "    cbarlabel=\"\",\n",
    "    valfmt=None,\n",
    "    textkw=None,\n",
    "    **kwargs,\n",
    "):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "    # Plot the heatmap\n",
    "    im = ax.imshow(data, vmin=0, vmax=1.0, **kwargs)\n",
    "\n",
    "    # Create colorbar\n",
    "    cbar = fig.colorbar(im, shrink=0.5)\n",
    "    cbar.set_ticks([0.0, 0.25, 0.5, 0.75, 1.0])\n",
    "    cbar.set_ticklabels([\"0.0\", \"0.25\", \"0.5\", \"0.75\", \"1.0\"])\n",
    "    cbar.set_label(cbarlabel, loc=\"center\")\n",
    "\n",
    "    # Show all ticks and label them with the respective list entries.\n",
    "    ax.set_xticks(np.arange(data.shape[1]), labels=col_labels)\n",
    "    ax.set_yticks(np.arange(data.shape[0]), labels=row_labels)\n",
    "\n",
    "    # Let the horizontal axes labeling appear on top.\n",
    "    if top:\n",
    "        ax.tick_params(top=True, bottom=False, labeltop=True, labelbottom=False)\n",
    "        # Rotate the tick labels and set their alignment.\n",
    "        plt.setp(ax.get_xticklabels(), rotation=-30, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    else:\n",
    "        ax.tick_params(top=False, bottom=True, labeltop=False, labelbottom=True)\n",
    "        # Rotate the tick labels and set their alignment.\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "    # Turn spines off and create white grid.\n",
    "    ax.spines[:].set_visible(False)\n",
    "\n",
    "    ax.set_xticks(np.arange(data.shape[1] + 1) - 0.5, minor=True)\n",
    "    ax.set_yticks(np.arange(data.shape[0] + 1) - 0.5, minor=True)\n",
    "    ax.grid(which=\"minor\", color=\"w\", linestyle=\"-\", linewidth=3)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "\n",
    "    if xlabel:\n",
    "        ax.set_xlabel(xlabel)\n",
    "    if ylabel:\n",
    "        ax.set_ylabel(ylabel)\n",
    "\n",
    "    # Get the formatter in case a string is supplied\n",
    "    if valfmt and isinstance(valfmt, str):\n",
    "        valfmt = matplotlib.ticker.StrMethodFormatter(valfmt)  # type: ignore\n",
    "    else:\n",
    "        valfmt = matplotlib.ticker.FuncFormatter(func)  # type: ignore\n",
    "\n",
    "    # Normalize the threshold to the images color range.\n",
    "    threshold = im.norm(1.0) / 2.0\n",
    "\n",
    "    # Set default alignment to center, but allow it to be\n",
    "    # overwritten by textkw.\n",
    "    kw = {\"horizontalalignment\": \"center\", \"verticalalignment\": \"center\"}\n",
    "    if textkw:\n",
    "        kw.update(textkw)\n",
    "\n",
    "    # Loop over the data and create a `Text` for each \"pixel\".\n",
    "    # Change the text's color depending on the data.\n",
    "    textcolors = (\"white\", \"black\")\n",
    "    texts = []\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            kw.update(color=textcolors[int(im.norm(data[i, j]) > threshold)])\n",
    "            text = im.axes.text(j, i, valfmt(data[i, j], None), **kw)  # type: ignore\n",
    "            texts.append(text)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Function to define how the text is displayed in the heatmap\n",
    "def func(x, pos):\n",
    "    return f\"{x:.2f}\".replace(\"0.00\", \"0\").replace(\"0.\", \".\").replace(\"nan\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the Data\n",
    "\n",
    "You are going to work with the PASCAL VOC 2011 dataset.\n",
    "This dataset is a small curated dataset that was used for a computer vision competition.\n",
    "The images were used for classification, object detection, and segmentation.\n",
    "This dataset was chosen because it has multiple classes and a variety of images and metadata.\n",
    "\n",
    "If this data is already on your computer you can change the file location from `\"./data\"` to wherever the data is stored.\n",
    "Remember to also change the download value from `True` to `False`.\n",
    "\n",
    "For the sake of ensuring that this tutorial runs quickly on most computers, you are going to analyze only the training dataset, which is a little under 6000 images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data and then load it as a torch Tensor.\n",
    "to_tensor = v2.ToImage()\n",
    "ds = datasets.VOCDetection(root=\"./data\", year=\"2011\", image_set=\"train\", download=True, transform=to_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the size of the loaded dataset\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, verify that the above code cell printed out 5717 for the size of the [dataset](http://host.robots.ox.ac.uk/pascal/VOC/voc2011/dbstats.html).\n",
    "\n",
    "This ensures that everything is working as needed for the tutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Structure the Metadata\n",
    "\n",
    "As this data was used for a computer vision competition, it will most likely have very few issues, but it is always worth it to check.\n",
    "Many of the large webscraped datasets available for use do contain image issues.\n",
    "Verifying in the beginning that you have a high quality dataset is always easier than finding out later that you trained a model on a dataset with erroneous images or a set of splits with leakage.\n",
    "\n",
    "This guide focuses on the labels and metadata for the images, rather than the images themselves, so you will load in the labels and metadata.\n",
    "However, there is no standard for metadata associated with images.\n",
    "Thus, you will load the metadata associated with the first image to explore it's metadata structure and determine exactly what is contained where in the metadata.\n",
    "This way you can extract all of the metadata into their associated groupings.\n",
    "\n",
    "For this dataset, the second element of a dataset item contains the metadata in a dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the label structure\n",
    "ds[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above shows that the metadata comes through as a nested dictionary.  \n",
    "You will restructure the above metadata to be a single-layer dictionary of lists.\n",
    "A few of the DataEval functions expect the labels representing the classes to be a separate list.\n",
    "\n",
    "To help you understand what categories are good ones to keep and what categories are not, below will walk you through the current metadata categories.\n",
    "This raw metadata dictionary contains 23 different dictionary keys, but not all of them contain useful information.\n",
    "\n",
    "Metadata keys\n",
    "\n",
    "- annotation\n",
    "- folder\n",
    "- filename\n",
    "- source\n",
    "- database\n",
    "- annotation\n",
    "- image\n",
    "- size\n",
    "- width\n",
    "- height\n",
    "- depth\n",
    "- segmented\n",
    "- object\n",
    "- name\n",
    "- pose\n",
    "- truncated\n",
    "- occluded\n",
    "- bndbox\n",
    "- xmin\n",
    "- ymin\n",
    "- xmax\n",
    "- ymax\n",
    "- difficult\n",
    "\n",
    "Now you need to determine which keys contain useful information and which keys do not. Below provides context with why a key is useful or not:\n",
    "\n",
    "- Outer _annotation_ category: **not useful** because its value is another dictionary.\n",
    "- _folder_: **not useful** because it is arbitrary and lacks information about the dataset. (These values could change from computer to computer.)\n",
    "- _filename_: **not useful** because it is arbitrary and lacks information about the dataset.\n",
    "- _source_: **not useful** because its value is another dictionary.\n",
    "- _database_: **useful** because its value contains information about the images.\n",
    "- _annotation_: **useful** because its value contains information abou the images.\n",
    "- _image_: **useful** because its value contains information abou the images.\n",
    "- _size_: **not useful** because its value is another dictionary.\n",
    "- _width_: **useful** because its value contains information abou the images.\n",
    "- _height_: **useful** because its value contains information abou the images.\n",
    "- _depth_: **useful** because its value contains information abou the images.\n",
    "- _segmented_: **useful** because its value contains information abou the images.\n",
    "- _object_: **not useful** because its value is another dictionary.\n",
    "- _name_: **useful** because its value contains the class information.\n",
    "- _pose_: **useful** because its value contains information abou the images.\n",
    "- _truncated_: **useful** because its value contains information abou the images.\n",
    "- _occluded_: **useful** because its value contains information abou the images.\n",
    "- _bndbox_: **not useful** because its value is another dictionary.\n",
    "- _xmin_: **useful** because its value contains information abou the images.\n",
    "- _ymin_: **useful** because its value contains information abou the images.\n",
    "- _xmax_: **useful** because its value contains information abou the images.\n",
    "- _ymax_: **useful** because its value contains information abou the images.\n",
    "- _difficult_: **useful** because its value contains information abou the images.\n",
    "\n",
    "There is one additional distinction to make from the keys, _database_ and _annotation_ are almost identical keys as they represent the same information and therefore you only need one of them.\n",
    "Now, run through all of the metadata to create the dictionary and labels list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_dict = {\n",
    "    \"database\": [],\n",
    "    \"image\": [],\n",
    "    \"width\": [],\n",
    "    \"height\": [],\n",
    "    \"channels\": [],\n",
    "    \"segmented\": [],\n",
    "    \"pose\": [],\n",
    "    \"truncated\": [],\n",
    "    \"occluded\": [],\n",
    "    \"xmin\": [],\n",
    "    \"ymin\": [],\n",
    "    \"xmax\": [],\n",
    "    \"ymax\": [],\n",
    "    \"difficult\": [],\n",
    "}\n",
    "\n",
    "class_labels = []\n",
    "\n",
    "for data in ds:\n",
    "    base = data[1][\"annotation\"]\n",
    "    objects = base[\"object\"]\n",
    "    for obj in objects:\n",
    "        # Append each value to the corresponding list in the metadata_dict\n",
    "        metadata_dict[\"database\"].append(base[\"source\"][\"database\"])\n",
    "        metadata_dict[\"image\"].append(base[\"source\"][\"image\"])\n",
    "        metadata_dict[\"width\"].append(int(base[\"size\"][\"width\"]))\n",
    "        metadata_dict[\"height\"].append(int(base[\"size\"][\"height\"]))\n",
    "        metadata_dict[\"channels\"].append(int(base[\"size\"][\"depth\"]))\n",
    "        metadata_dict[\"segmented\"].append(int(base[\"segmented\"]))\n",
    "        metadata_dict[\"pose\"].append(obj[\"pose\"])\n",
    "        metadata_dict[\"truncated\"].append(int(obj[\"truncated\"]))\n",
    "        metadata_dict[\"occluded\"].append(int(obj[\"occluded\"]))\n",
    "        metadata_dict[\"xmin\"].append(int(obj[\"bndbox\"][\"xmin\"]))\n",
    "        metadata_dict[\"ymin\"].append(int(obj[\"bndbox\"][\"ymin\"]))\n",
    "        metadata_dict[\"xmax\"].append(int(obj[\"bndbox\"][\"xmax\"]))\n",
    "        metadata_dict[\"ymax\"].append(int(obj[\"bndbox\"][\"ymax\"]))\n",
    "        metadata_dict[\"difficult\"].append(int(obj[\"difficult\"]))\n",
    "        class_labels.append(obj[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the lengths match\n",
    "print(len(class_labels), len(metadata_dict[\"database\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, the `parity` function does not work with continuous data like _xmin_, so you have to discretize it into bins.\n",
    "An easy way to do this is to use numpy's histogram function with \"auto\" bins.\n",
    "However, that method can still result in bins with only a few values in them.\n",
    "\n",
    "The parity function relies on a [contingency table](https://en.wikipedia.org/wiki/Contingency_table) and recommends at least 5 samples per table slot.\n",
    "In order to achieve this recommended 5 samples per table slot in the contingency, the continuous data needs to be binned with a relatively large number of samples per bin.\n",
    "To achieve this, you will use bins with at least 1500 samples in it. The code below implements this binning of the continuous data.\n",
    "\n",
    "It's not always possible to meet the 5 sample minimum, and while this will not prevent the parity function from running,\n",
    "it will skew the results for the specific factor towards having bias and it will issue a warning informing you of the value-label combination that did not meet the recommended 5 samples.\n",
    "This means that the p-values are smaller than they might be if you met the recommended 5 samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = 1500\n",
    "\n",
    "for continuous_variable in [\"width\", \"height\", \"channels\", \"xmin\", \"ymin\", \"xmax\", \"ymax\"]:\n",
    "    counts, bin_edges = np.histogram(metadata_dict[continuous_variable], bins=\"auto\")\n",
    "\n",
    "    if counts.min() < min_count:\n",
    "        new_counts = []\n",
    "        new_bin_edges = [bin_edges[0]]\n",
    "\n",
    "        current_count = 0\n",
    "        for i in range(len(counts)):\n",
    "            current_count += counts[i]\n",
    "\n",
    "            if current_count >= min_count:\n",
    "                new_counts.append(current_count)\n",
    "                new_bin_edges.append(bin_edges[i + 1])\n",
    "                current_count = 0\n",
    "\n",
    "        if current_count > 0:\n",
    "            new_counts[-1] += current_count\n",
    "\n",
    "        if counts[-1] < min_count:\n",
    "            new_bin_edges[-1] = bin_edges[-1] + 1\n",
    "\n",
    "        # Output the adjusted histogram\n",
    "        new_counts = np.array(new_counts)\n",
    "        new_bin_edges = np.array(new_bin_edges)\n",
    "\n",
    "        discretized_variable = np.digitize(metadata_dict[continuous_variable], bins=new_bin_edges, right=False)\n",
    "\n",
    "    else:\n",
    "        discretized_variable = np.digitize(metadata_dict[continuous_variable], bins=bin_edges, right=False)\n",
    "\n",
    "    metadata_dict[continuous_variable] = list(discretized_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have translated the metadata into a single-layer dictionary and discretized the continuous variables, a few of the DataEval bias metrics require the metadata to be in the form of a list of dictionaries.\n",
    "Also, the class labels list needs to be translated from label strings to numerical format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = metadata_dict.keys()\n",
    "metadata = [dict(zip(keys, values)) for values in zip(*metadata_dict.values())]\n",
    "print(len(metadata))\n",
    "\n",
    "class_list, numerical_labels = np.unique(class_labels, return_inverse=True)\n",
    "print(len(numerical_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, there are many different forms of bias.\n",
    "However, not all forms of bias directly affect the dataset and in order to address the biases that affect datasets, you have to make a few assumptions:\n",
    "\n",
    "1. All desired classes are present.\n",
    "2. All available metadata is provided.\n",
    "3. The metadata has been recorded correctly.\n",
    "\n",
    "If any of the above assumptions are violated, then the analysis below will be inaccurate. \\\n",
    "When using your own data, you should verify the above assumptions.\n",
    "\n",
    "Now, you can move on with identifying any bias that may be present in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataEval contains 3 main functions for detecting bias and correlations in a dataset:\n",
    "\n",
    "- balance\n",
    "- diversity\n",
    "- parity\n",
    "\n",
    "You will use each function to test the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Checking Dataset Balance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `balance` function measures correlational relationships between metadata factors and classes in a dataset.\n",
    "It analyzes the metadata factors against both the classes and other factors to identify relationships.\n",
    "\n",
    "The results can be retrieved using the balance and factors keys from the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bal = balance(list(numerical_labels), metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information provided by the balance function may be visually understood with a heat map.\n",
    "To do this you will combine the balance and factors keys from the result into a single heat map.\n",
    "The balance key is the analysis between metadata factors and class, while the factors key is the analysis between all pairs of metadata factors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine balance and factors results\n",
    "data = np.concatenate([bal.balance[np.newaxis, 1:], bal.factors], axis=0)\n",
    "# Create a mask for the upper triangle of the symmetrical array, ignoring the diagonal\n",
    "mask = np.triu(data + 1, k=0) < 1\n",
    "# Finalize the data for the plot, last row is last factor x last factor so it gets dropped\n",
    "heat_data = np.where(mask, np.nan, data)[:-1]\n",
    "# Creating label array for heat map axes\n",
    "heat_labels = [\"class\"] + list(metadata_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap(heat_data, heat_labels[:-1], heat_labels[1:], cbarlabel=\"Normalized Mutual Information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The heatmap shows that the greatest correlations are in the bounding box locations (_xmin_ with _xmax_ and _ymin_ with _ymax_) and the image dimensions (_height_ and _width_).\n",
    "Also the _ymax_ of the bounding box location is correlated with the _height_ of the image.\n",
    "It is not surprising that _height_ and _width_ have correlation since many of the images are similarly sized.\n",
    "The correlations between _xmin_ and _xmax_ and between _ymin_ and _ymax_ suggests that there is repetition in bounding box width and height across the objects.\n",
    "However, the fact that _pose_ has a value near 0.10 with _class_ means that a few of the classes have specific poses across a fair percentage of the images for that class.\n",
    "An example of this would be most _pottedplant_ images having the same _pose_ value.\n",
    "\n",
    "In addition to analyzing class and other factors, the balance function also analyzes metadata factors with individual classes to identify relationships between only one class and secondary factors.\n",
    "Again, a heatmap is used for visualizing the classwise results.\n",
    "These results can be retrieved using the classwise key from the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap(\n",
    "    bal.classwise,\n",
    "    class_list,\n",
    "    heat_labels,\n",
    "    xlabel=\"Factors\",\n",
    "    ylabel=\"Class\",\n",
    "    top=False,\n",
    "    cbarlabel=\"Normalized Mutual Information\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classwise heatmap shows that factors other than _class_ do not have any significant correlation with a specific class.\n",
    "Classwise balance shows correlation of individual classes with all class labels, indicating relative class imbalance.\n",
    "In this case the _person_ class is over-represented relative to most other classes.\n",
    "This means that a model might learn a bias towards the _person_ class label due to its frequency in the training set, which becomes a problem if the test/operational dataset doesn't have the same imbalance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Checking Dataset Diversity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `diversity` function measures the evenness or uniformity of the sampling of metadata factors over a dataset.\n",
    "Values near 1 indicate uniform sampling, while values near 0 indicate imbalanced sampling, e.g. all values taking a single value.\n",
    "\n",
    "The results can be retrieved using the diversity_index key from the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "div = diversity(list(numerical_labels), metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, it's often easiest to see the differences between the different factors when visualizing them.\n",
    "This time you will use a box plot as there is only a factor-class analysis, no factor-factor analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.bar(heat_labels, div.diversity_index)\n",
    "ax.set_xlabel(\"Factors\")\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When analyzing a diversity plot, there are two key values to keep in mind, 1 and 0. A 1 represents uniform sampling and an unbiased factor, while a 0 represents a constant value for all samples.\n",
    "Factors with a diversity value of 0 do not provide insight as to whether you have adequately sampled the data space, for more information see the [Diversity](../concepts/Diversity.md) concept page. Diversity values near 1 indicate unbiased factors while values below 0.5 indicate basis in the factor.\n",
    "\n",
    "In the results above, the metadata factors _image_ and _channels_ contain only a single value and do not contribute to our bias analysis.\n",
    "The factors _truncated_ and _occluded_ have values near 1, meaning that there is relatively little or no bias in these factors.\n",
    "The categories of most interest are those that are between 0.5 and 0.2 because this region represents skewed value distributions for the factor.\n",
    "The following factors fall into this category:\n",
    "\n",
    "- _class_\n",
    "- _width_\n",
    "- _height_\n",
    "- _segmented_\n",
    "- _difficult_\n",
    "\n",
    "These factors contain bias that should be addressed either by adding or removing data to even out the sampling.\n",
    "For instance, the _class_ factor highlights that there is unevenness in the number of data points per class.\n",
    "\n",
    "In addition to analyzing class, the diversity function also analyzes metadata factors with individual classes to assess uniformity of metadata factors within a class.\n",
    "As above, a heatmap is used for visualizing the classwise results.\n",
    "These results can be retrieved using the classwise key from the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap(\n",
    "    div.classwise,\n",
    "    class_list,\n",
    "    list(metadata_dict.keys()),\n",
    "    xlabel=\"Factors\",\n",
    "    ylabel=\"Class\",\n",
    "    top=False,\n",
    "    cbarlabel=\"Normalized Simpson Index\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results expand the above results on a per class basis.\n",
    "Things to look for here are large variances for a given factor across the different classes.\n",
    "For example, _pose_ has values ranging from 0.01 to 0.84, which means that a few classes have almost uniform selection of the different _pose_ values while other classes essentially only have one _pose_ value.\n",
    "Should classes have different selections of _pose_ value? Yes, one would not expect the _diningtable_ class to have a _pose_ direction, while the _person_ class should have multiple _pose_ directions.\n",
    "What needs to be further investigated are things like whether the _sofa_ class should have a _pose_ direction, because a diversity value of 0.4 means that a few of the images do while others do not.\n",
    "Also, the _cat_ class has a low score signifying that most of the images fall into one or two categories rather than being spread even across the categories.\n",
    "This highlights an error in the data collection process &mdash; the value was not specified for most _cat_ images and therefore defaulted to \"Unspecified\".\n",
    "An alternative error would be a dataset in which the _cat_ images have most cats facing a specific direction, which would require additional data to overcome the bias,\n",
    "but that is not the case for this dataset. It has plenty of cats facing each direction, but only a few of them contain a _pose_ value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Checking Dataset Parity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `parity` function measures the relationship between metadata factors and classes using a chi-squared test.\n",
    "A high score with a low p-value suggests that a metadata factor is strongly correlated with a class label.\n",
    "\n",
    "Parity requires the metadata format to be a dictionary of lists with the class labels included and the class labels are required to be in numerical format (no strings).\n",
    "\n",
    "The results can be retrieved using the score and p-value keys from the output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_dict[\"class\"] = list(numerical_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par = parity(metadata_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The warning above states that the metric works best when there are more than 5 samples in each value-label combination.\n",
    "However, because of the large number of total samples, the difference between 1 and 5 samples does not significantly affect the results.\n",
    "\n",
    "When evaluating the results of parity for a large number of factors, it may be easier to understand the results by presenting the score and p-value for a given factor together.\n",
    "The code below restructures the result to this structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, score, value in zip(metadata_dict.keys(), par.score, par.p_value):\n",
    "    print(f\"{key} - {score} : {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the results, all metadata are correlated with _class_ labels, except for _image_ and _channels_.\n",
    "However, parity is based on the idea of an expected frequency and how the observed differs from what is expected.\n",
    "The expected frequencies are determined by sums of the values for each metadata category.\n",
    "\n",
    "This function works best when the expected frequencies for a given factor for each individual class are known a priori.\n",
    "So for the case above, the expected frequency for the _pose_ metadata category shouldn't be the same for all classes.\n",
    "_Diningtable_, _pottedplant_, and _bottle_ should only have a single value for _pose_ which automatically throws off the metric because not all of the classes have an identical expected frequency for _pose_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having analyzed the dataset for bias with multiple metrics, the concluding answer is this dataset has bias.\n",
    "Training a model on this dataset has the potential to learn shortcuts and underperform on operational data if the biases are not representative of biases in the operational dataset.\n",
    "\n",
    "The metadata categories identified by the `balance`, `diversity` and `parity` functions contain issues such as imbalanced classes and imbalanced parameters per class.\n",
    "DataEval isn't able to tell you exactly why they are imbalanced, but it highlights the categories that you need to check.\n",
    "\n",
    "As you can see, the DataEval methods are here to help you gain a deep understanding of your dataset and all of its strengths and limitations.\n",
    "It is designed to help you create representative and reliable datasets.\n",
    "\n",
    "Good luck with your data!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next\n",
    "\n",
    "In addition to identifying bias and correlations in a dataset, DataEval offers additional tutorials to help you learn about dataset analysis:\n",
    "\n",
    "- To clean a dataset use the [Data Cleaning Guide](EDA_Part1.ipynb).\n",
    "- To identify coverage gaps and outliers use the [Assessing the Data Space Guide](EDA_Part2.ipynb).\n",
    "- To monitor data for shifts during operation use the [Data Monitoring Guide](Data_Monitoring.ipynb).\n",
    "\n",
    "To learn more about the balance, diversity and parity functions, see the [Balance](../concepts/Balance.md), [Diversity](../concepts/Diversity.md) and [Parity](../concepts/Parity.md) concept pages.\n",
    "\n",
    "## On your own\n",
    "\n",
    "Once you are familiar with DataEval and dataset analysis, you will want to run this analysis on your own dataset.\n",
    "When you do, make sure that you analyze all of your data and not just the training set.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
