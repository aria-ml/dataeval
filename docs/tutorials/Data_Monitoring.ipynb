{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Monitoring Guide\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Monitoring is a critical step in the AI/ML lifecycle. When a model is deployed, data can, and generally will, drift from the distribution on which the model was originally trained, or may be fundamentally different from the outset for a variety of reasons. One critical step in AI T&E is the detection of changes in the operational distribution so that one may proactively address them. While some changes will not affect performance, significant deviation is often associated with model degradation.\n",
    "\n",
    "You will walk through the steps of detecting drift and parity.\n",
    "\n",
    "For this tutorial, you will use the VOC dataset, an image dataset used for computer vision competitions. You will be comparing the image distribution of the `train` split to that of the `val` split, pretending as though the `val` split represents an operational dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What you'll need\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll begin by importing the necessary libraries for this tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab  # noqa: F401\n",
    "\n",
    "    # specify the version (==X.XX.X) at the end of the statement below when testing version of DataEval other\n",
    "    # than the latest\n",
    "    %pip install -q dataeval[torch]\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "    # install numpy\n",
    "    %pip install numpy \n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, models\n",
    "\n",
    "# Drift\n",
    "from dataeval.detectors.drift import DriftCVM, DriftKS, DriftMMD\n",
    "from dataeval.metrics.bias import label_parity\n",
    "\n",
    "# Set the random value\n",
    "rng = np.random.default_rng(213)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What you'll learn\n",
    "\n",
    "- You'll learn how to detect drift on an object detection dataset\n",
    "- You'll learn how to measure Parity on metadata between your training and test set\n",
    "- You'll learn how to use embeddings to efficiently run large datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Constructing Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Images\n",
    "\n",
    "The first step in many aspects of data monitoring is reducing images down to a dimension that our tools can operate in. To do this, you will use existing model weights from ResNet18. You will apply these to the VOC dataset. A more in depth look at this dataset and the construction of embeddings can be seen in the [EDA Tutorial](./EDA_Part1.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first steps are defining the encoder network and embedding the training images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the embedding network\n",
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "embedding_net = EmbeddingNet()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embedding_net.to(device)\n",
    "\n",
    "\n",
    "# Extract embeddings\n",
    "def extract_embeddings(dataset, model):\n",
    "    model.eval()\n",
    "\n",
    "    embeddings = torch.empty(size=(0, 128)).to(device)\n",
    "    with torch.no_grad():\n",
    "        images = []\n",
    "        for i, (img, _) in enumerate(dataset):\n",
    "            images.append(img)\n",
    "            if (i + 1) % 64 == 0:\n",
    "                inputs = torch.stack(images, dim=0).to(device)\n",
    "                outputs = model(inputs)\n",
    "                embeddings = torch.vstack((embeddings, outputs))\n",
    "                images = []\n",
    "        inputs = torch.stack(images, dim=0).to(device)\n",
    "        outputs = model(inputs)\n",
    "        embeddings = torch.vstack((embeddings, outputs))\n",
    "    return embeddings.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will reload our training dataset with the desired preprocessing for our given model and then you will run the model to get the image embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pretrained model transformations\n",
    "preprocess = models.ResNet18_Weights.DEFAULT.transforms()\n",
    "\n",
    "# Load the dataset\n",
    "dataset = datasets.VOCDetection(\"./data\", year=\"2011\", image_set=\"train\", download=False, transform=preprocess)\n",
    "\n",
    "# Create image embeddings\n",
    "embeddings = extract_embeddings(dataset, embedding_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images are reduced to dimension 128. Next you do the same for the operational dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 'operational' dataset\n",
    "op_dataset = datasets.VOCDetection(\"./data\", year=\"2011\", image_set=\"val\", download=False, transform=preprocess)\n",
    "\n",
    "# Create image embeddings\n",
    "op_embeddings = extract_embeddings(op_dataset, embedding_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(op_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Drift\n",
    "\n",
    "Now that you have embedded both sets of images into 128-dimensional space, you would like to determine if the `val` dataset has drifted from the `train` dataset.\n",
    "you will use 3 dataeval tools to make this determination. Each operated by comparing the distributions of embeddings between the two images sets. They produce a probability value, where a small value means that it is very unlikely that these two sets of embeddings come from the same distribution, and therefore drift has likely occurred. Based on this p-value(s), each drift metric will output a binary `is_drift`, which you will examine here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = DriftMMD(embeddings)\n",
    "d2 = DriftCVM(embeddings)\n",
    "d3 = DriftKS(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.predict(op_embeddings).is_drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2.predict(op_embeddings).is_drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3.predict(op_embeddings).is_drift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since these two image sets are random subsets of the same dataset, you unsurprisingly do not detect and drift. However, let's add some Gaussian noise to the operational embeddings to see what happens to the drift detectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perturbed_op_embeddings = np.float32(op_embeddings + np.random.normal(size=np.shape(op_embeddings)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.predict(perturbed_op_embeddings).is_drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2.predict(perturbed_op_embeddings).is_drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3.predict(perturbed_op_embeddings).is_drift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you perturb the operational embeddings, you find that drift is detected. To give a more realistic example, you can also look at an individual class from the operational set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for data in op_dataset:\n",
    "    objects = data[1][\"annotation\"][\"object\"]\n",
    "    names = []\n",
    "    for each in objects:\n",
    "        names.append(each[\"name\"])\n",
    "    labels.append(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset embeddings of images which contain a chair\n",
    "chair_embeddings = op_embeddings[[(\"chair\" in i) for i in labels], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1.predict(chair_embeddings).is_drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2.predict(chair_embeddings).is_drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3.predict(chair_embeddings).is_drift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases, you can see the drift detectors pick up on very simple perturbations, but return `0` when the dataset is indistinguishable from that on which the model was trained.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Parity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another task you might want to perform in monitoring is looking at parity of classes between training and operational datasets. There is parity between two datasets in terms of label if the label frequencies are (approximately) equal. Lets check if the distribution of the objects in each image is the same between datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op_labels = []\n",
    "for data in op_dataset:\n",
    "    objects = data[1][\"annotation\"][\"object\"]\n",
    "    names = []\n",
    "    for each in objects:\n",
    "        names.append(each[\"name\"])\n",
    "    op_labels.append(names)\n",
    "op_labels = [x for i in op_labels for x in i]\n",
    "labels = []\n",
    "for data in dataset:\n",
    "    objects = data[1][\"annotation\"][\"object\"]\n",
    "    names = []\n",
    "    for each in objects:\n",
    "        names.append(each[\"name\"])\n",
    "    labels.append(names)\n",
    "labels = [x for i in labels for x in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Turn string labels into integer labels so the DataEval parity function can read them.\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(labels)\n",
    "label_int = le.transform(labels)\n",
    "op_label_int = le.transform(op_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_parity(label_int, op_label_int, 20).p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see, unsurprisingly, that there is no discernible difference in the distribution of classes between the datasets (the p_value is extremely high).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have checked for potential issues in the operational dataset that may affect the model after deployment. Both drift and class parity (lack thereof) can affect a model's ability to achieve the performance recorded at model training. If one detects that a dataset has drifted significantly and/or that parity has been violated, it might be a good idea to consider retraining the model, incorporating operational data into this retraining.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
