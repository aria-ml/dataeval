{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes Error Rate Estimation Tutorial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Problem Statement_\n",
    "\n",
    "For classification machine learning tasks, there is an _inherent difficulty_ associated with signal to noise ratio in the images. One way of quantifying this difficulty is the Bayes Error Rate, or irreducable error.\n",
    "\n",
    "DAML has introduced a method of calculating this error rate that uses image embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _When to use_\n",
    "\n",
    "The `BER` metric should be used when you would like to measure the feasibility of a machine learning task. For example, if you have an operational accuracy requirement of 80%, and would like to know if this is feasibly achievable given the imagery.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _What you will need_\n",
    "\n",
    "1. A set of image embeddings and their corresponding class labels. This requires training an autoencoder to compress the images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Setting up_\n",
    "\n",
    "Let's import the required libraries needed to set up a minimal working example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab  # noqa: F401\n",
    "\n",
    "    %pip install -q daml\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "import os\n",
    "\n",
    "from pytest import approx\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.datasets as tfds\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, InputLayer\n",
    "from tensorflow.nn import relu\n",
    "\n",
    "from daml.metrics import BER\n",
    "\n",
    "tf.keras.utils.set_random_seed(408)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in data\n",
    "\n",
    "Let's start by loading in tensorflow's mnist dataset,\n",
    "then we will examine it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the mnist dataset from tensorflow datasets\n",
    "(images, labels), (test_images, test_labels) = tfds.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of training samples: \", len(images))\n",
    "print(\"Image shape:\", images[0].shape)\n",
    "print(\"Label counts: \", np.unique(labels, return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To highlight the effects of modifying the dataset on its Bayes Error Rate,\n",
    "we will only include a subset of 6,000 images and their labels for digits 1, 4, and 9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_split = {}\n",
    "labels_split = {}\n",
    "\n",
    "# Keep only 1, 4, and 9\n",
    "for label in (1, 4, 9):\n",
    "    subset_indices = np.where(labels == label)\n",
    "    images_split[label] = images[subset_indices][:2000]\n",
    "    labels_split[label] = labels[subset_indices][:2000]\n",
    "\n",
    "images_subset = np.concatenate(list(images_split.values()))\n",
    "labels_subset = np.concatenate(list(labels_split.values()))\n",
    "print(images_subset.shape)\n",
    "print(np.unique(labels_subset, return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have taken a subset of the data that is only the digits 1, 4, and 9.\n",
    "The BER estimate requires 1 dimension, so we flatten the images during this step. This is ok since MNIST images are small, in practice we would need to do some dimension reduction (autoencoder) here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the images\n",
    "images_flattened = images_subset.reshape((images_subset.shape[0], -1))\n",
    "print(\"Dataset shape:\", images_flattened.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 9,000 flattened images of size 784. Next we can move on to evaluation of the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Suppose we would like to build a classifier that differentiates between the handwritten digits 1, 4, and 9 with predetermined accuracy requirement of 99%.\n",
    "\n",
    "We will use BER to check the feasibility of the task.\n",
    "As the images are small, we can simple use the flattened raw pixel intensities to calculate BER (no embedding necessary).\n",
    "_Note_: This will not be the case in general.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the BER metric\n",
    "metric = BER(images_flattened, labels_subset, method=\"MST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the BER metric for the MNIST data with digits 1, 4, 9.\n",
    "# One minus the value of this metric gives our estimate of the upper bound on accuracy.\n",
    "base_ber = metric.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The bayes error rate estimation:\", base_ber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "### TEST ASSERTION ###\n",
    "print(base_ber)\n",
    "assert base_ber[\"ber\"] == approx(0.025833, abs=1e-6)\n",
    "assert base_ber[\"ber_lower\"] == approx(0.0130443, abs=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimate of the maximum achievable accuracy is one minus the BER estimate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The maximum achievable accuracy:\", (1 - base_ber[\"ber\"]) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "The maximum achievable accuracy on a dataset of 1, 4, and 9 is about 97.4%.\n",
    "This _does not_ meet our requirement of 99% accuracy!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modify dataset classification\n",
    "\n",
    "To address insufficient accuracy, lets modify the dataset to classify an image as \"1\" or \"Not a 1\".\n",
    "By combining classes, we can hopefully achieve the desired level of attainable accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a binary mask where current label == 1 that can be used as the new labels\n",
    "labels_merged = labels_subset == 1\n",
    "print(\"New label counts:\", np.unique(labels_merged, return_counts=True))\n",
    "\n",
    "# Update the metric with merged labels with digits 1, and not 1 (classes 4 & 9).\n",
    "metric.labels = labels_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the BER metric for the MNIST data with updated labels\n",
    "new_ber = metric.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The bayes error rate estimation:\", new_ber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "### TEST ASSERTION ###\n",
    "print(new_ber)\n",
    "assert new_ber[\"ber\"] == approx(0.005, abs=1e-6)\n",
    "assert new_ber[\"ber_lower\"] == approx(0.002506, abs=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimate of the maximum achievable accuracy is one minus the BER estimate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The maximum achievable accuracy:\", 1 - new_ber[\"ber\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "The maximum achievable accuracy on a dataset of 1 and not 1 (4, 9) is about 99.5%.\n",
    "This _does_ meet our accuracy requirement.\n",
    "\n",
    "By using BER to check for feasibility early on, we were able to reformulate the problem such that it is feasible under our specifications\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a classifier\n",
    "\n",
    "We can now attempt to build a classifier that achieves this level of accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple CNN for classifying MNIST images.\n",
    "model = Sequential(\n",
    "    [\n",
    "        InputLayer(input_shape=(28, 28, 1)),\n",
    "        Conv2D(\n",
    "            64,\n",
    "            4,\n",
    "            strides=2,\n",
    "            padding=\"same\",\n",
    "            activation=relu,\n",
    "        ),\n",
    "        Conv2D(\n",
    "            128,\n",
    "            4,\n",
    "            strides=2,\n",
    "            padding=\"same\",\n",
    "            activation=relu,\n",
    "        ),\n",
    "        Conv2D(\n",
    "            512,\n",
    "            4,\n",
    "            strides=2,\n",
    "            padding=\"same\",\n",
    "            activation=relu,\n",
    "        ),\n",
    "        Flatten(),\n",
    "        Dense(2),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using a subset for training, we must also subset the testing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_indices = np.where((test_labels == 1) | (test_labels == 4) | (test_labels == 9))\n",
    "test_images_subset = test_images[test_indices]\n",
    "test_labels_subset = test_labels[test_indices]\n",
    "test_labels_merged = test_labels_subset == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test the model\n",
    "\n",
    "Now we train and test the model on the modified data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up model hyperparameters\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Fitting a model may take a few minutes\n",
    "history = model.fit(\n",
    "    images_subset,\n",
    "    labels_merged,\n",
    "    epochs=90,\n",
    "    batch_size=32,\n",
    "    steps_per_epoch=1,\n",
    "    validation_data=(test_images_subset, test_labels_merged),\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(test_images_subset, test_labels_merged, verbose=1)\n",
    "print(f\"The model accuracy: {accuracy*100:0.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "### TEST ASSERTION ###\n",
    "print(accuracy)\n",
    "assert accuracy == approx(0.9914, abs=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Model Accuracy\")\n",
    "plt.plot(range(60, 90), np.array(history.history[\"val_accuracy\"])[60:], label=\"Classifier\")\n",
    "plt.hlines(\n",
    "    y=1 - new_ber[\"ber\"],\n",
    "    colors=[\"red\"],\n",
    "    xmin=60,\n",
    "    xmax=90,\n",
    "    label=\"1-BER\",\n",
    "    linestyles=\"dashed\",\n",
    ")\n",
    "plt.hlines(\n",
    "    y=0.99,\n",
    "    colors=[\"green\"],\n",
    "    xmin=60,\n",
    "    xmax=90,\n",
    "    label=\"Accuracy Requirement\",\n",
    "    linestyles=\"dashed\",\n",
    ")\n",
    "\n",
    "plt.xticks(range(60, 91, 10))\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "The model achieves an accuracy of 99.14% accuracy, exceeding the requirement of 99%.\n",
    "\n",
    "The model accuracy does not quite approach the maximum achievable accuracy, meaning there are still improvements that can be made.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
