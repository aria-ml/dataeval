{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder Trainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Problem Statement_\n",
    "\n",
    "For most computer vision tasks like **image classification** and **object detection**, the size of the image datasets can put an enormous strain on the speed of dataset analysis methods. A way to lessen this burden is to reduce the size of the images without losing the _important_ information. This is known as **dimensionality reduction**. Given the high dimensionality of image data, this is best done using an autoencoder trained on a reconstruction task.\n",
    "\n",
    "To help with this, DAML has introduced a lightweight, easy-to-use Autoencoder Training class ( `AETrainer` ), that allows a user to have out-of-the-box functionality for this type of dimensionality reduction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _When to use_\n",
    "\n",
    "The `AETrainer` class should be used when you have lots of images, have very large images, or your given speed requirements are strict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _What you will need_\n",
    "\n",
    "1. A PyTorch Dataset with your images returned first in `__getitem__`\n",
    "1. (Optional) A PyTorch autoencoder model\n",
    "1. (Optional) A PyTorch autoencoder model with a defined `encode` function\n",
    "\n",
    "If the optional models are not given, a default architecture is used. This default has an `encode` function.  \n",
    "It is encouraged to create a custom architecture that best fits with your data as this will lead to better results during training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Setting up_\n",
    "\n",
    "Let's import the required libraries needed to set up a minimal working example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Google Colab Only\n",
    "try:\n",
    "    %pip install -q daml[torch]\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "import os\n",
    "\n",
    "from pytest import approx\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms.v2 as v2\n",
    "from torch.utils.data import Subset\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will grab a common classification dataset and look at it's statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_tensor = v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])\n",
    "training_dataset = MNIST(root=\"./data/\", train=True, transform=to_tensor, download=True)\n",
    "testing_dataset = MNIST(root=\"./data/\", train=False, transform=to_tensor, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training data size:\", training_dataset.data.shape)\n",
    "print(\"Training labels size:\", training_dataset.targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 60,000 images in the training set, each 28x28 pixels.  \n",
    "While these are not large images, you can still apply dimensionality reduction to get some speed upgrades for downstream tasks.\n",
    "\n",
    "\\*_Note_\\*  \n",
    "The MNIST dataset is very small compared to most operational datasets, and for this example does not actually reduce the image size.  \n",
    "To use your own dataset, replace `training_dataset` and `testing_dataset`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Using a default trainer_\n",
    "\n",
    "#### **Training Phase**\n",
    "\n",
    "DAML provides a simple default trainer for autoencoder tasks. Let's import the necessary classes.  \n",
    "In this simple example, we will assume you do not have an autoencoder architecture to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from daml.models.ae import AETrainer, AriaAutoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you set up the model and trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = AriaAutoencoder(channels=1)\n",
    "trainer = AETrainer(model, device=device, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model on a subset (6000 images) of the MNIST data.  \n",
    "Since this is a simpler problem, you will reduce the default 25 epochs to 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_subset = Subset(training_dataset, range(6000))\n",
    "training_loss = trainer.train(training_subset, epochs=10)\n",
    "print(training_loss[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Evaluation Phase**\n",
    "\n",
    "Now that you have a trained model, let's check its performance on a validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loss = trainer.eval(testing_dataset)\n",
    "print(eval_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "### TEST ASSERTION ###\n",
    "print(training_loss[-1])\n",
    "print(eval_loss)\n",
    "assert training_loss[-1] == approx(0.112837, abs=1e-4)\n",
    "assert eval_loss == approx(0.114008, abs=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! You can see that the model was able to perform reconstruction on unseen data. This is only done to confirm that your model did not overfit to the training data.\n",
    "\n",
    "Now you can encode the dataset and use those embeddings to speed up downstream tasks.\n",
    "\n",
    "#### **Encoding Phase**\n",
    "\n",
    "Encoding is different than training or evaluation when using an autoencoder as the latter compresses the image, and then reconstructs it back to the original size.  \n",
    "By calling only the first part of the autoencoder, the **encoder**, you can take advantage of this compression.\n",
    "\n",
    "Let's show an example using the training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = trainer.encode(training_subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "### TEST ASSERTION ###\n",
    "print(embeddings.shape)\n",
    "assert embeddings.shape == torch.Size([6000, 64, 6, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Embedded image shape:\", embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can see how the encoder can change the overall shape of your images, which can lead to significant benefits for downstream tasks when using large data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Additional Information_\n",
    "\n",
    "**Related Notebooks**\n",
    "\n",
    "1. [Bayes Error Rate](BayesErrorRateEstimationTutorial.ipynb)\n",
    "1. [Divergence](DPDivergenceTutorial.ipynb)\n",
    "1. [Sufficiency](ClassLearningCurvesTutorial.ipynb)\n",
    "1. [Outlier Detection](OutlierDetectionTutorial.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
