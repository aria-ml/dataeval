{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning Guide\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Exploratory Data Analysis (EDA) is an approach to analyzing data sets to summarize the main characteristics and identify incongruencies in the data.\n",
    "Before diving into machine learning or statistical modeling, it is crucial to understand the data you are working with.\n",
    "EDA helps in understanding the patterns, detecting anomalies, checking assumptions, and determining relationships in the data.\n",
    "\n",
    "One of the most important aspects of EDA is data cleaning.\n",
    "A portion of DataEval is dedicated to being able to identify duplicates and outliers as well as data points that have missing or too many extreme values.\n",
    "These techniques help ensure that you only include high quality data for your projects and avoid things like leakage between training and testing sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-by-Step Guide\n",
    "\n",
    "This guide will walk through how to use DataEval to perform basic data cleaning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Requirements\n",
    "\n",
    "You will need a python environment with the following packages installed:\n",
    "\n",
    "- `dataeval[torch]` or `dataeval[all]`\n",
    "- `torchvision`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll begin by installing the necessary libraries to walk through this guide.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab  # noqa: F401\n",
    "\n",
    "    %pip install -q dataeval[torch]\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need the Counter for processing the labels.\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# We will need matplotlib for visualing our dataset and numpy to be able to handle the data.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# We are importing torch in order to create image embeddings.\n",
    "# We are only using torchvision to load in the dataset.\n",
    "# If you already have the data stored on your computer in a numpy friendly manner,\n",
    "# then feel free to load it directly into numpy arrays.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.v2 as v2\n",
    "from torchvision import datasets, models\n",
    "\n",
    "# Load the classes from DataEval that are helpful for EDA\n",
    "from dataeval.detectors import Clusterer, Duplicates, Linter\n",
    "from dataeval.metrics import channelstats, imagestats\n",
    "\n",
    "# Set the random value\n",
    "rng = np.random.default_rng(213)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Understand the Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data\n",
    "\n",
    "We are going to work with the PASCAL VOC 2011 dataset.\n",
    "This dataset is a small curated dataset that was used for a computer vision competition.\n",
    "The images were used for classification, object detection, and segmentation.\n",
    "We are using this dataset because it has multiple classes and images with a variety of sizes and objects.\n",
    "\n",
    "If this data is already on your computer you can change the file location from `\"./data\"` to wherever the data is stored.\n",
    "Just remember to also change the download value from `True` to `False`.\n",
    "\n",
    "For the sake of ensuring that this tutorial runs quickly on most computers, we are going to analyze only the training set of the data, which is a little under 6000 images.\n",
    "\n",
    "However, once you are familiar with DataEval and data analysis, you will want to run this analysis on the validation set and on all of the data together.\n",
    "One thing to look for when checking the other sets of data is to see how the stats of each grouping of data changes (or doesn't change).\n",
    "DataEval also includes tools to analyze these changes and determine whether there is any bias or correlations in the different sets.\n",
    "However, those tools will not be highlighted in this guide but can be found in the **Identifying Bias and Correlations Guide**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data and then load it as a torch Tensor.\n",
    "to_tensor = v2.ToImage()\n",
    "ds = datasets.VOCDetection(\"./data\", year=\"2011\", image_set=\"train\", download=True, transform=to_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the size of the loaded dataset\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, verify that the above code cell printed out 5717 for the size of the [dataset](http://host.robots.ox.ac.uk/pascal/VOC/voc2011/dbstats.html).\n",
    "\n",
    "This ensures that everything is working as needed for the tutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the Data\n",
    "\n",
    "As this data was used for a computer vision competition, it will most likely have very few issues, but it is always worth it to check.\n",
    "Many of the large webscraped datasets available for use do contain image issues.\n",
    "Verifying in the beginning that you have a high quality dataset is always easier than finding out later that you trained a model on a dataset with erroneous images or a set of splits with leakage.\n",
    "\n",
    "All of the DataEval classes currently expect the data to be handed in as a numpy array.\n",
    "Numpy can't handle different sized images in a stacked array, it requires that all images in the stack be the same size.\n",
    "So instead of loading the dataset into a dataloader, we will load the images into a list that can be processed image by image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list = []\n",
    "for data in ds:\n",
    "    img_list.append(data[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the images, we'll also need to load the labels.\n",
    "However, there is no standard for metadata associated with images.\n",
    "Thus, we will load the metadata associated with the first image to explore it's metadata structure and determine exactly what is contained where in the metadata.\n",
    "This way we can extract just the labels for each image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the label structure\n",
    "ds[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the metadata comes through as a nested dictionary.\n",
    "What we need is the _\"object\"_ key of the dictionary which contains a list of objects in the image.\n",
    "Inside the list are additional dictionaries, one for each object found in the image.\n",
    "Inside these dictionaries, the label can be found via the _\"name\"_ key.\n",
    "\n",
    "Let's run through all of the labels and create a list of lists which just contains the name of each object in each image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for data in ds:\n",
    "    objects = data[1][\"annotation\"][\"object\"]\n",
    "    names = []\n",
    "    for each in objects:\n",
    "        names.append(each[\"name\"])\n",
    "    labels.append(names)\n",
    "\n",
    "labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double check that the values output from the code above matches the object names from the original metadata we viewed above.\n",
    "\n",
    "Now that we have a friendly version of the labels for each image, let's run some label statistics to explore the different objects found in the images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This grabs the total number of each object labelled and the number and index of images each object is present in\n",
    "object_counts = Counter()\n",
    "image_counts = Counter()\n",
    "index_location = defaultdict(list)\n",
    "\n",
    "for i, group in enumerate(labels):\n",
    "    # Count occurrences of each object in all sublists\n",
    "    object_counts.update(group)\n",
    "\n",
    "    # Create a set of unique items in the current sublist\n",
    "    unique_items = set(group)\n",
    "\n",
    "    # Update image counts and index locations\n",
    "    image_counts.update(unique_items)\n",
    "    for item in unique_items:\n",
    "        index_location[item].append(i)\n",
    "\n",
    "# Display the results\n",
    "print(\"     Object: Total Count - Image Count\")\n",
    "for obj in list(object_counts.keys()):\n",
    "    print(f\"{obj:>11}:    {object_counts[obj]:>4}     -   {image_counts[obj]:>4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above table, we can see that this dataset has a total of 20 classes.  \n",
    "Of the classes, person is the class with the highest total object count followed by chair and car , while person, chair and dog are the classes with the highest number of images.  \n",
    "Cow, sheep, and bus are the classes with least number of objects, while the classes with the least number of images are bus, train and cow.\n",
    "\n",
    "This table helps us see that there is wide variation in\n",
    "\n",
    "- the number of classes per image,\n",
    "- the number of objects per image,\n",
    "- and the number of objects of each class per image.\n",
    "\n",
    "This highlights an important concept - class balance.  \n",
    "A dataset that is imbalanced can result in a model that chooses the more prominent class more often just because it's more prominent.  \n",
    "We are not going to address this issue at this time, because we need to first determine if there are images that need to be removed from the dataset,\n",
    "but it's important to note that the dataset does not have class balance.\n",
    "This concept is further explored in the tutorial - **Identifying Bias and Correlations Guide**.\n",
    "\n",
    "Now that we've looked at our label set, let's visually inspect random images across the different classes to get an idea of the quality of the data.\n",
    "When inspecting the random images, we want to get an idea of the variety of backgrounds, the range of colors, the locations of objects in images,\n",
    "and how often an image is seen with a single object versus multiple objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot random images from each category\n",
    "fig, axs = plt.subplots(5, 4, figsize=(15, 20))\n",
    "\n",
    "for ax, (category, indices) in zip(axs.flat, index_location.items()):\n",
    "    # Randomly select an index from the list of indices\n",
    "    selected_index = rng.choice(indices)\n",
    "\n",
    "    # Plot the corresponding image - need to permute to get channels last for matplotlib\n",
    "    ax.imshow(np.moveaxis(img_list[selected_index], 0, -1))\n",
    "    ax.set_title(category)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From plotting the images, you can tell that there are a variety of image sizes, image brightness, object sizes, backgrounds, number of objects in the image, and even a few images that are in black and white.\n",
    "\n",
    "This is where DataEval comes in, it's designed to help you make sense of the many different aspects that affect building repsentative datasets and robust models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize the Data\n",
    "\n",
    "To begin, we are going to utilize two analysis functions. One that grabs the stats for the images as a whole and one that looks at the images on a per channel basis.\n",
    "\n",
    "The `imagestats` and `channelstats` functions have the option to use all built in metrics or to just analyze a few of them.\n",
    "For more information on customizing the metrics to analyze, checkout the how-to: [How to customize the metrics for data cleaning](../how_to/linting_flags.md).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell takes about 5-10 minutes to run depending on your hardware\n",
    "\n",
    "# Calculate the raw stats for the dataset\n",
    "# The output from compute is a dictionary that contains the raw values for each metric\n",
    "# Note: the stat functions expect the images as an iterable and in the (C,H,W) format\n",
    "dataset_stats = imagestats(img_list)\n",
    "ds_channel_stats = channelstats(img_list)\n",
    "\n",
    "# View the list of metrics in the image stats class\n",
    "list(dataset_stats.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the list of metrics in the channel stats class\n",
    "list(ds_channel_stats.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our stats computed, let's visualize them.\n",
    "We'll plot them once normally and once on a log scale to make sure that we can adequately see all of the trends.  \n",
    "Sometimes there are only a few extreme values in a category and they can be easily overlooked if a log scale is not used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 3, figsize=(15, 20))\n",
    "\n",
    "for ax, metric in zip(\n",
    "    axs.flat,\n",
    "    [\n",
    "        \"size\",\n",
    "        \"aspect_ratio\",\n",
    "        \"channels\",\n",
    "        \"mean\",\n",
    "        \"std\",\n",
    "        \"var\",\n",
    "        \"skew\",\n",
    "        \"kurtosis\",\n",
    "        \"zero\",\n",
    "        \"brightness\",\n",
    "        \"blurriness\",\n",
    "        \"entropy\",\n",
    "    ],\n",
    "):\n",
    "    # Plot the histogram for the chosen metric\n",
    "    ax.hist(dataset_stats[metric], bins=20)\n",
    "    ax.set_title(metric)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 3, figsize=(15, 20))\n",
    "\n",
    "for ax, metric in zip(\n",
    "    axs.flat,\n",
    "    [\n",
    "        \"size\",\n",
    "        \"aspect_ratio\",\n",
    "        \"channels\",\n",
    "        \"mean\",\n",
    "        \"std\",\n",
    "        \"var\",\n",
    "        \"skew\",\n",
    "        \"kurtosis\",\n",
    "        \"zero\",\n",
    "        \"brightness\",\n",
    "        \"blurriness\",\n",
    "        \"entropy\",\n",
    "    ],\n",
    "):\n",
    "    # Plot the histogram on a log scale for the chosen metric\n",
    "    ax.hist(dataset_stats[metric], bins=20, log=True)\n",
    "    ax.set_title(metric)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the distribution of values for each metric allows us to quickly inspect the metrics for unusual distributions.\n",
    "Without knowing anything about the images, we will assume that each metric should follow one of two types of distributions: normal or uniform.\n",
    "\n",
    "With a [uniform distribution](https://en.wikipedia.org/wiki/Discrete_uniform_distribution), we want to notice if any of the plots have areas that are a lot shorter or a lot taller than the rest of the values.\n",
    "\n",
    "With a [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution), we are looking at the edges of the bell curve to see if the values near the edges of the plot raise up or if there are gaps between the edge values and the next value in.\n",
    "\n",
    "We plotted the metrics on both a normal axis and on a log axis because sometimes values at the very edge of the plot can be hidden by the scaling of the normal axis.\n",
    "Looking at the plots, there are a few key things to point out:\n",
    "\n",
    "1. The channel metric has only one value, 3. This is interesting since some of the images from our random plot above are greyscale, which usually only has 1 channel.\n",
    "2. The entropy, zero and kurtosis metrics are single-tailed and all of them have a long tail which indicates that the images whose values are in the edges of the tail are potentially problematic.\n",
    "3. Size, aspect ratio, variance and skew have skewed or off-center distributions which is another sign of problematic images.\n",
    "4. Mean, standard deviation, brightness and blurriness appear to have a normal distribution and none have an extended tail, which is a good sign.\n",
    "\n",
    "While this does not tell us which images are the problematic ones, it gives us some intuition for the metrics we expect the `Linter` to flag.  \n",
    "From these plots, we expect the Linter to flag images with issues in the following metrics:\n",
    "\n",
    "- entropy,\n",
    "- zero,\n",
    "- kurtosis,\n",
    "- size,\n",
    "- aspect ratio,\n",
    "- variance,\n",
    "- and skew.\n",
    "\n",
    "Now, let's analyze the channel stats to see if there are any additional metrics that might be problematic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 2, figsize=(10, 15))\n",
    "\n",
    "for ax, metric in zip(axs.flat, [\"mean\", \"std\", \"var\", \"skew\", \"kurtosis\", \"entropy\"]):\n",
    "    # Plot the histogram for the chosen metric\n",
    "    # Since each image has 3 channels, a transpose is needed for matplotlib\n",
    "    # because matplotlib treats the # of columns as different datasets\n",
    "    if metric == \"mean\":\n",
    "        ax.hist(\n",
    "            np.array(ds_channel_stats[metric][3]).T,\n",
    "            bins=20,\n",
    "            density=True,\n",
    "            color=[\"red\", \"green\", \"blue\"],\n",
    "            label=[\"Channel 0\", \"Channel 1\", \"Channel 2\"],\n",
    "        )\n",
    "        ax.legend()\n",
    "    else:\n",
    "        ax.hist(np.array(ds_channel_stats[metric][3]).T, bins=20, density=True, color=[\"red\", \"green\", \"blue\"])\n",
    "    ax.set_title(f\"Channel {metric}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our understanding from above about uniform and normal distributions, we want to analyze the channel-based metrics with the same principle.\n",
    "\n",
    "Here we can see that overall shape for each of these channel metrics matches the shape of their counterparts that we already analyzed.  \n",
    "With the channel metrics, we are not as interested in the overall shape in these plots but in the comparison across each of the individual channels.  \n",
    "We want to see if the same shape holds across each channel or if there are large differences between the channels.  \n",
    "This is important because discrepancies across channels can help us detect image processing errors and channel bias.\n",
    "\n",
    "However, their is very little difference across the channels for each metric.  \n",
    "There is a slight shift in the blue channel for both the mean and skew metrics, but it is not enough of a difference to warrant suspicion.  \n",
    "Thus, no additional metric is added to our list of metrics we expect to get flagged by the Linter:\n",
    "\n",
    "- entropy,\n",
    "- zero,\n",
    "- kurtosis,\n",
    "- size,\n",
    "- aspect ratio,\n",
    "- variance,\n",
    "- and skew.\n",
    "\n",
    "Let's move on to identifying which images have a statistical difference from the rest of the images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Identify any Outlying Data Points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extreme/Missing Values\n",
    "\n",
    "We want to detect and identify the images associated with the extreme values from our plotted metrics above.\n",
    "To detect these extreme values, we will use the `Linter` class.\n",
    "The `Linter` class has multiple methods to determine the extreme values, which are discussed in the [Data Cleaning explanation](../concepts/DataCleaning.md).\n",
    "For this guide, we will use the \"zscore\" as the Z score defines outliers in a normal distribution.\n",
    "\n",
    "The output of the `Linter` class is a dictionary where the image number is the key and the value is a dictionary containing the flagged metrics and their value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Linter class (with a random image)\n",
    "lints = Linter(outlier_method=\"zscore\")\n",
    "\n",
    "# Assign the image stats compute result to the linter class result\n",
    "lints.stats = dataset_stats\n",
    "\n",
    "# Find the extreme images\n",
    "lint_imgs = lints._get_outliers()\n",
    "\n",
    "# View the number of extreme images\n",
    "print(f\"Number of images with extreme values: {len(lint_imgs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class can flag a lot of images, depending on how varied the dataset is and which method you use to define extreme values.  \n",
    "Using the zscore, it flagged 447 images across 13 metrics out of the 5717 images in the dataset.\n",
    "However, switching the method can give different results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the metrics with an extreme value\n",
    "metrics = {}\n",
    "for img, group in lint_imgs.items():\n",
    "    for extreme in group:\n",
    "        if extreme in metrics:\n",
    "            metrics[extreme].append(img)\n",
    "        else:\n",
    "            metrics[extreme] = [img]\n",
    "print(f\"Number of metrics with extremes: {len(metrics)}\")\n",
    "\n",
    "# Show the total number of extreme values for each metric\n",
    "for group, imgs in metrics.items():\n",
    "    print(f\"  {group} - {len(imgs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digging into the flagged images and organizing them by category, we can see that the metric with the most extreme values is \"size\" while \"blurriness\" has the least number of extreme values.\n",
    "It is also worth noting that the `Linter` found issues with more metrics than we noticed.\n",
    "Going back to our list, we had\n",
    "\n",
    "- entropy,\n",
    "- zero,\n",
    "- kurtosis,\n",
    "- size,\n",
    "- aspect ratio,\n",
    "- variance,\n",
    "- and skew.\n",
    "\n",
    "However, the `Linter` added mean, standard deviation, brightness, and blurriness.\n",
    "The `Linter` is not perfect but it is designed to flag any image that might be problematic.\n",
    "It is then up to the user to shift through the information provided by the `Linter`.\n",
    "\n",
    "Now let's look into each metric and display how the flagged images are spread across our 20 classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show each metric by class\n",
    "# Determine which classes are present in each image\n",
    "class_wise = {obj: {} for obj in sorted(object_counts.keys())}\n",
    "for group, imgs in metrics.items():\n",
    "    for img in imgs:\n",
    "        unique_items = set(labels[img])\n",
    "        for cat in unique_items:\n",
    "            if group not in class_wise[cat]:\n",
    "                class_wise[cat][group] = 0\n",
    "            class_wise[cat][group] += 1\n",
    "\n",
    "# Create the table for displaying\n",
    "table_header = [\"      Class\"]\n",
    "for group in sorted(metrics.keys()):\n",
    "    table_header.append(f\"{group:^10}\")\n",
    "table_header.append(\"  Total\")\n",
    "table = [table_header]\n",
    "for class_cat, results in class_wise.items():\n",
    "    table_rows = [f\"{class_cat:>11}\"]\n",
    "    total = 0\n",
    "    for group in sorted(metrics.keys()):\n",
    "        if group == \"aspect_ratio\":\n",
    "            if group in results:\n",
    "                table_rows.append(f\"{results[group]:^12}\")\n",
    "                total += results[group]\n",
    "            else:\n",
    "                table_rows.append(f\"{0:^12}\")\n",
    "        else:\n",
    "            if group in results:\n",
    "                table_rows.append(f\"{results[group]:^10}\")\n",
    "                total += results[group]\n",
    "            else:\n",
    "                table_rows.append(f\"{0:^10}\")\n",
    "    table_rows.append(f\"  {total:^5}\")\n",
    "    table.append(table_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(linting-issues-by-metric-table)=\n",
    "\n",
    "#### Linting Issues by Metric Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the table\n",
    "for row in table:\n",
    "    print(\" | \".join(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the things to note from splitting up the issues by class and metric:\n",
    "\n",
    "- An image with an unusual aspect ratio is most likely to contain a boat or aeroplane\n",
    "- An image with an issue in brightness (really bright or really dark) is most likely to contain a person or an aeroplane\n",
    "- Images with low entropy (think image with constant pixels) are likely to fall within 1 of 4 classes: aeroplane, bird, bottle, person\n",
    "- Unusual skew and kurtosis images follow a similar trend as entropy\n",
    "\n",
    "There appear to be other trends as well.  \n",
    "Something to remember is that there are different number of images for each class.\n",
    "For example, 36 low entropy images out of the 2000 for person might be outliers while 28 low entropy images out of 300 for aeroplane might not be;\n",
    "low entropy might be an inherent characteristic of class aeroplane.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand the above table, we are going to plot sample images from a few of the metrics.  \n",
    "We will look at entropy, size, zero and blurriness.  \n",
    "Entropy because Entropy, Variance, Standard deviation, Kurtosis, and Skew all measure (in slightly different ways) how much change there is across the pixels in the image, and entropy will be the easiest to understand.  \n",
    "Size because Size, Width, Height and Aspect Ratio are all interrelated and size has the most extreme images from those.  \n",
    "Zero is a category unto itself but it is closely related to Mean and Brightness. Zero measures images that have a significant number of pixels with a zero value compared to the average image.  \n",
    "Blurriness because it is also in it's own category. Blurriness measures the sharpness of lines in an image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(questions)=\n",
    "\n",
    "#### Questions\n",
    "\n",
    "When looking at these images, we want to think about the following questions:\n",
    "\n",
    "- Does this image represent something that would be expected in operation?\n",
    "- Is there commonality to the objects in the images? Such as all the objects are found on the leftside of the images.\n",
    "- Is there commonality to the backgrounds of the images? Such as similar colors, darkness/brightness, places, things (like water or snow).\n",
    "- Is there commonality to the class of objects in the images? Such as a specific pose for person or specific pot color for pottedplant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot random images from each metric\n",
    "fig, axs = plt.subplots(3, 4, figsize=(15, 10))\n",
    "selected_index = rng.choice(metrics[\"entropy\"], 12, replace=False)\n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    # Plot the corresponding image - need to permute to get channels last for matplotlib\n",
    "    ax.imshow(np.moveaxis(img_list[selected_index[i]], 0, -1))\n",
    "    ax.set_title(\"Entropy\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the flagged images for entropy, what do we see?\n",
    "\n",
    "That many of the flagged images here have an almost constant background.\n",
    "Thinking back to our questions - how many of these backgrounds will we see in operation? Are we likely to find water in our images or an object in the sky?  \n",
    "It is also worth pointing out the number of images that have a relatively dark background. How likely are we to encounter night time or dark images in our operation?  \n",
    "If water or objects in the sky or dark backgrounds are expected, then we may just need to collect more images with these kinds of backgrounds. If not, then they are outliers that can be discarded.  \n",
    "To learn more about data collection, go [here](https://viso.ai/computer-vision/data-collection/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot random images from each metric\n",
    "fig, axs = plt.subplots(3, 4, figsize=(15, 10))\n",
    "selected_index = rng.choice(metrics[\"size\"], 12, replace=False)\n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    # Plot the corresponding image - need to permute to get channels last for matplotlib\n",
    "    ax.imshow(np.moveaxis(img_list[selected_index[i]], 0, -1))\n",
    "    ax.set_title(\"Size\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get into these images, you need to decide whether your model workflow will preprocess images to be the exact same size or if you only want to only include images of a specific size.  \n",
    "If preprocessing the images, you will want to make sure that your method does not cause distortions to the image (such as resizing) and that you still have the desired information in the image (such as when cropping).\n",
    "If you are expecting an image of a specific size, then you can easily just discard the incorrectly sized images.\n",
    "However, you will want to use the **Identifying Bias and Correlations Guide** to make sure that this does not introduce any bias into your dataset.\n",
    "\n",
    "Now that you've thought about your workflow, let's look at the flagged images for size.\n",
    "\n",
    "The first thing of note is that there are a lot of images here with animals. With that, we want to think about is this an artifact of how pictures are taken of animals or just a by product of the data collection methods?\n",
    "Recalling from the [table](#linting-issues-by-metric-table) above, issues with size are pretty spread out across all classes, so dropping all of them might be okay, but you will definitely want to check for bias after dropping them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot random images from each metric\n",
    "fig, axs = plt.subplots(3, 4, figsize=(15, 10))\n",
    "selected_index = rng.choice(metrics[\"zero\"], 12, replace=False)\n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    # Plot the corresponding image - need to permute to get channels last for matplotlib\n",
    "    ax.imshow(np.moveaxis(img_list[selected_index[i]], 0, -1))\n",
    "    ax.set_title(\"Zeros\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the flagged images for zero, what do we see?\n",
    "\n",
    "Similarly to entropy, some of these images have a dark background, which we addressed above.  \n",
    "Also, of note is the grayscale images. Here, we want to think about how often will we come across greyscale images in operation, and can a malfunction in the pipeline (either hardware or software) produce greyscale images and if so how likely will that kind of malfunction occur?\n",
    "\n",
    "For both of those cases, dark backgrounds and greyscale images, do they occur proportionately throughout all of the classes or do they exist in only 1 or 2 classes?\n",
    "If they occur in only 1 or 2 classes, then you might just want to throw them out so that your model doesn't just learn to associate dark backgrounds or greyscale with those classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Blurriness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot random images from each metric\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "selected_index = metrics[\"blurriness\"]\n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    # Plot the corresponding image - need to permute to get channels last for matplotlib\n",
    "    ax.imshow(np.moveaxis(img_list[selected_index[i]], 0, -1))\n",
    "    ax.set_title(\"Blurriness\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the flagged images for blurriness, what do we see?\n",
    "\n",
    "That neither of these images appear to be blurry, but they may actually have a higher resolution than the rest of the images, thus they are significantly less blurry than average.  \n",
    "Also of note, is the background to the images, the grass and the leaves. Are those common backgrounds or are these the only images with a close up with leaves and grasses background?\n",
    "Is this operationally relevant? If not, then these two images should just be removed. If yes, then additional images are needed with these two backgrounds.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linting Summary\n",
    "\n",
    "The `Linter` can not tell you what is operationally relevant, but it does inform about which images stand out from the rest in one way or another.\n",
    "\n",
    "After viewing these images that stand out, there are two key takeaways to keep in mind:\n",
    "\n",
    "1. Many of the flagged images will be flagged by more than one metric.\n",
    "2. Plotting the flagged metrics allows us to get an idea of what the `Linter` calls an outlier.\n",
    "   Not all of these images are outliers, some of them could represent areas in our dataset that are underrepresented.\n",
    "\n",
    "DataEval is used to identify images which _may be_ problematic in your dataset, but it cannot specify whether an image is actually an outlier or not.  \n",
    "Applying the four [questions](#questions) above to each image that stands out, will help you in determining whether the image should be removed or not from the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicates\n",
    "\n",
    "We will move onto detecting and identifying any duplicates.\n",
    "\n",
    "The `Duplicates` class identifies both exact duplicates and potential (near) duplicates.\n",
    "Potential duplicates can occur in a variety of ways:\n",
    "\n",
    "- Intentional permutations\n",
    "  - Images with varying brightness\n",
    "  - Translating the image\n",
    "  - Padding the image\n",
    "  - Cropping the image\n",
    "- Unintentional changes\n",
    "  - Copying the image from one format to another (png->jpeg)\n",
    "  - Including a permuted image and the original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Duplicates class (with a random image)\n",
    "dups = Duplicates()\n",
    "\n",
    "# Assign the image stats compute result to the duplicates class result\n",
    "dups.stats = dataset_stats\n",
    "\n",
    "# Find the duplicates\n",
    "dup_imgs = dups._get_duplicates()\n",
    "\n",
    "# View the duplicates\n",
    "dup_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected there are no duplicates in this dataset, since it was curated for a specific competition.\n",
    "\n",
    "However, to highlight the abilities of the `Duplicates` class we are going to add some duplicates to our image stats and rerun the `Duplicates` class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy images to create exact duplicates\n",
    "img_list2 = [img_list[23], img_list[46]]\n",
    "\n",
    "# Copy and crop images to create near duplicates\n",
    "img5 = img_list[5][:, 5:-5, 5:-5]\n",
    "img4376 = img_list[4376][:, :-5, 5:]\n",
    "\n",
    "img_list2.extend([img5, img4376])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the duplicates using the modified dataset\n",
    "dup_imgs = dups.evaluate(img_list + img_list2)\n",
    "\n",
    "# View the duplicates\n",
    "dup_imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, it identified images 5717 and 5718 as the exact duplicates that we copied from images 23 and 46, respectively.\n",
    "It also correctly identified as near duplicate images, images 5719 and 5720 that we copied and cropped from images 5 and 4376, respectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers\n",
    "\n",
    "Now that we've explored the extreme images and identified duplicates, we want to detect and identify those images which are outside of their class distribution.\n",
    "\n",
    "For this detector, we need to translate the images into image embeddings as the images themselves are too big for the `Clusterer` class to handle efficiently.\n",
    "The `Clusterer` works best when the feature dimension is around 250 or less.\n",
    "\n",
    "For this guide, we will use a pretrained ResNet18 model and adjust the last layer to be our desired dimension of 128.\n",
    "Also, pretrained torchvision models come with all the necessary information for preprocessing your images correctly for that model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the embedding network\n",
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "embedding_net = EmbeddingNet()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embedding_net.to(device)\n",
    "\n",
    "\n",
    "# Extract embeddings\n",
    "def extract_embeddings(dataset, model):\n",
    "    model.eval()\n",
    "\n",
    "    embeddings = torch.empty(size=(0, 128)).to(device)\n",
    "    with torch.no_grad():\n",
    "        images = []\n",
    "        for i, (img, _) in enumerate(dataset):\n",
    "            images.append(img)\n",
    "            if (i + 1) % 64 == 0:\n",
    "                inputs = torch.stack(images, dim=0).to(device)\n",
    "                outputs = model(inputs)\n",
    "                embeddings = torch.vstack((embeddings, outputs))\n",
    "                images = []\n",
    "        inputs = torch.stack(images, dim=0).to(device)\n",
    "        outputs = model(inputs)\n",
    "        embeddings = torch.vstack((embeddings, outputs))\n",
    "    return embeddings.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will reload our dataset with the desired preprocessing for our given model and then we will run the model to get the image embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pretrained model transformations\n",
    "preprocess = models.ResNet18_Weights.DEFAULT.transforms()\n",
    "\n",
    "# Load the dataset\n",
    "dataset = datasets.VOCDetection(\"./data\", year=\"2011\", image_set=\"train\", download=False, transform=preprocess)\n",
    "\n",
    "# Create image embeddings\n",
    "embeddings = extract_embeddings(dataset, embedding_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our image embeddings, we will hand over the embeddings to the `Clusterer` class to generate clusters of data and identify the outliers.\n",
    "\n",
    "The `Clusterer` output is a dictionary with 4 keys:\n",
    "\n",
    "- outliers,\n",
    "- potential_outliers,\n",
    "- duplicates,\n",
    "- and near_duplicates.\n",
    "\n",
    "We already know that there are no duplicates or near duplicates in the dataset so those should be empty.\n",
    "However, from the results of the `Linter` we expect there to be images in both the outlier and potential outlier categories.\n",
    "\n",
    "Potential outliers are images which are on the edge of the cluster, but were not far enough away from the cluster to be considered an outlier.\n",
    "These are good images to compare with the outliers in order to get a sense of what was grouped versus what was not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell takes about 5-10 minutes to run depending on your hardware\n",
    "\n",
    "# Initialize the Clusterer class (with the embedded images)\n",
    "cluster = Clusterer(embeddings)\n",
    "\n",
    "# Find the outlier images\n",
    "results = cluster.evaluate()\n",
    "\n",
    "# View the number of outliers\n",
    "print(f\"Number of outliers: {len(results['outliers'])}\")\n",
    "print(f\"Number of potential outliers: {len(results['potential_outliers'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now plot the first 16 images that are considered outliers along with their labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot random images from each category\n",
    "fig, axs = plt.subplots(4, 4, figsize=(12, 12))\n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    # Selected image\n",
    "    selected_index = results[\"outliers\"][i]\n",
    "\n",
    "    # Plot the corresponding image - need to permute to get channels last for matplotlib\n",
    "    ax.imshow(np.moveaxis(img_list[selected_index], 0, -1))\n",
    "    ax.set_title(\"-\".join(set(labels[selected_index])))\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to address these outliers from the `Clusterer` in a similar manner to the way we handled the `Linter` extreme images, do they represent actual outliers or just underrepresented samples?  \n",
    "In specific context to the `Clusterer`, we want to focus on these in a class by class manner, so thinking about the person class images only in context of the person class, not the dataset as a whole.\n",
    "\n",
    "We aren't going to go through all of these images, but we will go through a few of them.  \n",
    "The first two horse images have a horse with water in the background. There are 238 total horse images and only 5 of them have water in the background. So while these images would be operationally relevant if we were trying to detect horses, they are underrepresented in the dataset. There are only 5 horse images in the whole dataset with water in the background.  \n",
    "The same goes for the third horse image. It is one of 4 images that are a close up picture of a horse standing against a fence or railing. It is most likely flagged as an outlier because it is underrepresented in the dataset.  \n",
    "Likewise with the potted plant, there are only about 4 images with a potted plant up against a solid background out of the 289 potted plant images. Likely this is also just an underrepresented image.  \n",
    "With the dog image, there are 13 dog images wearing an outfit out of 636 dog images and this is the only one in which the dog is sitting while wearing something. Likely an underrepresented image.\n",
    "\n",
    "With the last two people images that you see, the person is mostly occluded in the second to last one and they are really small and off to the side in the last one.  \n",
    "With the second to last one, it is likely that the image could be dropped unless you will often have occulsion when detecting people.  \n",
    "With the last image, you have to determine how operationally relevant it is. Are you trying to detect people far away or are you focusing on closer images? Also, what is the scale at which an object is too small for detection?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes the fun part, determining what data points are supposed to be in the data set, what points need to be removed, and whether or not you need to collect more data points for a given class or style of image.\n",
    "\n",
    "The images identified by the `Linter` and the `Clusterer` mark images that have something unique about them.\n",
    "DataEval isn't able to tell you exactly what's unique, that's up to you.\n",
    "You will want to compare each image with other images in that same class to determine whether it is an under-represented image (scenario?), an image that contains some error and needs to be removed, an image that represents a different class or it could be something else, like a whole class that are always brighter or darker or less varied than the other classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the DataEval methods are here to help you gain a deep understanding of your dataset and all of it's limitations and/or under-representated images.\n",
    "It is designed to help you ask the right questions, but it can't answer those questions for you.\n",
    "\n",
    "Once you have explored this dataset in comparison to what's operationally relevant (ie your going to see the same kind of data when your model is deployed),\n",
    "then DataEval offers additional tools to make sure there is not bias or other factors influencing your model's performance.\n",
    "Learn more about these tools in **Identifying Bias and Correlations Guide**.\n",
    "\n",
    "Good luck with your data!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
