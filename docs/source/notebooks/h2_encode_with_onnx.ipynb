{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to encode images with ONNX models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Problem Statement_\n",
    "\n",
    "When working with image datasets, generating embeddings is a common first step for many analysis tasks like clustering,\n",
    "duplicate detection, and coverage analysis. While PyTorch models are widely used, ONNX (Open Neural Network Exchange)\n",
    "provides a framework-agnostic format that offers portability and often better inference performance.\n",
    "\n",
    "DataEval's `OnnxExtractor` allows you to use any ONNX model to generate embeddings from your image datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _When to use_\n",
    "\n",
    "Use the `OnnxExtractor` when you want to:\n",
    "\n",
    "- Generate embeddings using a pre-trained ONNX model\n",
    "- Work with models exported from various frameworks (PyTorch, TensorFlow, etc.)\n",
    "- Leverage optimized inference without framework dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _What you will need_\n",
    "\n",
    "1. An image dataset (we'll use VOC2012)\n",
    "2. An ONNX model that outputs embeddings\n",
    "3. A Python environment with the following packages installed:\n",
    "   - `dataeval`\n",
    "   - `onnxruntime` (or `onnxruntime-gpu` for GPU support)\n",
    "   - `onnx` (for model preparation utilities)\n",
    "   - `maite-datasets`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Getting Started_\n",
    "\n",
    "Let's import the required libraries needed to set up a minimal working example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab  # noqa: F401\n",
    "\n",
    "    %pip install -q dataeval[onnx] maite-datasets opencv-python-headless\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import requests\n",
    "from maite_datasets.object_detection import VOCDetection\n",
    "\n",
    "from dataeval import Embeddings\n",
    "from dataeval.extractors import OnnxExtractor\n",
    "from dataeval.selection import Limit, Select\n",
    "from dataeval.utils.onnx import to_encoding_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing an ONNX model for embeddings\n",
    "\n",
    "Most pre-trained ONNX models output classification logits rather than embeddings. To extract embeddings, we need to\n",
    "modify the model to output the features from an intermediate layer (typically before the final classification layer).\n",
    "\n",
    "DataEval provides utility functions to help with this:\n",
    "\n",
    "- `find_embedding_layer`: Identifies the embedding layer in a classification model\n",
    "- `to_encoding_model`: Returns a modified model with the embedding layer name\n",
    "\n",
    "We'll download a ResNet50 model and use these utilities to prepare it for embedding extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_onnx_model(url, save_path):\n",
    "    \"\"\"Downloads the ONNX model if it doesn't exist locally.\"\"\"\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"Model already exists at {save_path}\")\n",
    "        return\n",
    "\n",
    "    print(f\"Downloading model from {url}...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                f.write(chunk)\n",
    "        print(\"Download complete.\")\n",
    "    else:\n",
    "        raise Exception(f\"Failed to download model. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and prepare the model\n",
    "model_url = \"https://github.com/onnx/models/raw/main/validated/vision/classification/resnet/model/resnet50-v2-7.onnx\"\n",
    "model_path = \"data/resnet50-v2-7.onnx\"\n",
    "\n",
    "download_onnx_model(model_url, model_path)\n",
    "\n",
    "# Find the embedding layer and create an in-memory model that outputs it\n",
    "\n",
    "encoding_model, embedding_layer = to_encoding_model(model_path)\n",
    "print(f\"Embedding layer: {embedding_layer}\")\n",
    "print(f\"In-memory encoding model: ({len(encoding_model):,} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "\n",
    "We'll use the VOC2012 dataset for this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms for ResNet50 input requirements\n",
    "def preprocess(image: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Preprocess image for ResNet50: CHW->HWC, resize, normalize, HWC->CHW.\"\"\"\n",
    "    hwc = image.transpose(1, 2, 0)  # Transpose to HWC\n",
    "    resized = cv2.resize(hwc, (224, 224))  # Resize using standard bi-linear interpolation\n",
    "    mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "    std = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "    normalized = (resized.astype(np.float32) / 255.0 - mean) / std  # Normalize\n",
    "    chw = normalized.transpose(2, 0, 1)  # Transpose back to CHW\n",
    "    return chw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VOC dataset\n",
    "dataset = VOCDetection(root=\"./data\", image_set=\"val\", year=\"2012\", download=True)\n",
    "print(f\"Dataset size: {len(dataset)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using OnnxExtractor to generate embeddings\n",
    "\n",
    "Now we can use the `OnnxExtractor` to generate embeddings from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the extractor with our in-memory embedding model\n",
    "extractor = OnnxExtractor(\n",
    "    model=encoding_model,\n",
    "    transforms=preprocess,\n",
    "    output_name=embedding_layer,  # Specify which output to use\n",
    ")\n",
    "\n",
    "print(extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings using the Embeddings class\n",
    "# We'll use a subset for demonstration\n",
    "subset = Select(dataset, Limit(100))\n",
    "embeddings = Embeddings(subset, extractor=extractor, batch_size=16)\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embeddings have shape `(N, D)` where:\n",
    "\n",
    "- `N` is the number of images (100 in our subset)\n",
    "- `D` is the embedding dimension (2048 for ResNet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "### TEST ASSERTION CELL ###\n",
    "assert embeddings.shape[0] == 100\n",
    "assert embeddings.shape[1] == 2048"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "default_lexer": "ipython3"
  },
  "kernelspec": {
   "display_name": "dataeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
