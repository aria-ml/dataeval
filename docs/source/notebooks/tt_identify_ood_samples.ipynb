{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Identify Out-of-Distribution Samples\n",
    "\n",
    "This guide demonstrates how to identify out-of-distribution (OOD) samples using reconstruction-based methods with different model architectures.\n",
    "\n",
    "Estimated time to complete: 10-15 minutes\n",
    "\n",
    "Relevant ML stages: [Monitoring](../concepts/users/ML_Lifecycle.md#monitoring), [Data Engineering](../concepts/users/ML_Lifecycle.md#data-engineering)\n",
    "\n",
    "Relevant personas: Machine Learning Engineer, T&E Engineer, Data Scientist\n",
    "\n",
    "## What you'll do\n",
    "\n",
    "- Train different reconstruction models (AE, VAE) for OOD detection\n",
    "- Use Gaussian Mixture Models (GMM) to enhance OOD detection\n",
    "- Compare model performance on different OOD scenarios\n",
    "- Visualize reconstruction quality and OOD scores\n",
    "\n",
    "## What you'll learn\n",
    "\n",
    "- When to use Autoencoder (AE) vs Variational Autoencoder (VAE) for OOD detection\n",
    "- How GMM in latent space improves OOD detection\n",
    "- How to interpret OOD scores and set appropriate thresholds\n",
    "- Different use cases for each model configuration\n",
    "\n",
    "## What you'll need\n",
    "\n",
    "- Knowledge of Python\n",
    "- Basic understanding of PyTorch and neural networks\n",
    "- Understanding of autoencoders (helpful but not required)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Out-of-distribution (OOD) detection is critical for ensuring model reliability in production. When models encounter data that differs significantly from their training distribution, predictions become unreliable. This tutorial demonstrates six different approaches to OOD detection:\n",
    "\n",
    "**Reconstruction-Based Methods:**\n",
    "\n",
    "1. **Standard Autoencoder (AE)**: Simple reconstruction-based detection using mean squared error\n",
    "2. **Variational Autoencoder (VAE)**: Probabilistic approach with regularized latent space\n",
    "3. **AE with GMM**: Enhanced detection by modeling latent space with Gaussian Mixture Models\n",
    "4. **VAE with GMM**: Combining probabilistic encoding with GMM for robust detection\n",
    "\n",
    "**Distance-Based Methods:** 5. **K-Nearest Neighbors (KNN) - Raw Pixels**: Detects OOD by measuring distance in pixel space 6. **K-Nearest Neighbors (KNN) - Embeddings**: Uses learned embeddings for better similarity\n",
    "\n",
    "For this tutorial, you'll use the MNIST dataset of handwritten digits. You'll train models to recognize digits 0-7 and test their ability to detect digits 8-9 as out-of-distribution samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, install the required packages and import necessary libraries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "csey0z3sm3",
   "metadata": {},
   "source": [
    "### Important Note on Expected Results\n",
    "\n",
    "OOD detection performance depends heavily on **how different** the OOD data is from the in-distribution data:\n",
    "\n",
    "- **Easy OOD**: Completely different data (e.g., cats vs dogs) â†’ near 100% detection\n",
    "- **Hard OOD**: Similar data (e.g., digit 8 vs digit 0, both have circles) â†’ lower detection rates\n",
    "\n",
    "In this tutorial, we use digits 8-9 as OOD against training on 0-7. This is a **moderately challenging** scenario because:\n",
    "\n",
    "- Digit 8 shares circular shapes with 0, 6\n",
    "- Digit 9 shares curves with 3, 5\n",
    "\n",
    "Therefore, you should expect:\n",
    "\n",
    "- **In-distribution accuracy**: ~95% (matching our threshold)\n",
    "- **OOD detection rates**: Variable (20-80%), depending on model and similarity\n",
    "- **Score separation**: OOD scores higher than in-dist, but distributions may overlap\n",
    "\n",
    "This reflects real-world scenarios where OOD data often shares features with training data!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab  # noqa: F401\n",
    "\n",
    "    %pip install -q dataeval torchvision\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import cast\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from maite_datasets.image_classification import CIFAR10, MNIST\n",
    "\n",
    "import dataeval\n",
    "from dataeval import Embeddings\n",
    "from dataeval.selection import ClassFilter, Limit, Select, Shuffle\n",
    "from dataeval.shift import OODKNeighbors, OODReconstruction, OODReconstructionConfig\n",
    "from dataeval.utils.models import AE, VAE, GMMDensityNet\n",
    "from dataeval.utils.preprocessing import rescale, resize, to_canonical_grayscale\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "dataeval.config.set_seed(173, all_generators=True)\n",
    "\n",
    "# Set default batch size\n",
    "dataeval.config.set_batch_size(64)\n",
    "\n",
    "# Set default torch device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Prepare the Data\n",
    "\n",
    "You'll load the MNIST dataset and split it into in-distribution (digits 0-7) and out-of-distribution (digits 8-9) samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    return x.astype(np.float32) / 255.0\n",
    "\n",
    "\n",
    "in_dist_digits = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "out_of_dist_digits = [8, 9]\n",
    "\n",
    "\n",
    "mnist_train = Select(\n",
    "    MNIST(\"./data\", image_set=\"train\", download=True, transforms=normalize),\n",
    "    selections=[Shuffle(), Limit(10000), ClassFilter(in_dist_digits)],\n",
    ")\n",
    "mnist_test_in = Select(\n",
    "    MNIST(\"./data\", image_set=\"test\", download=True, transforms=normalize),\n",
    "    selections=[Shuffle(), Limit(1000), ClassFilter(in_dist_digits)],\n",
    ")\n",
    "mnist_test_ood = Select(\n",
    "    MNIST(\"./data\", image_set=\"test\", download=True, transforms=normalize),\n",
    "    selections=[Shuffle(), Limit(1000), ClassFilter(out_of_dist_digits)],\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {len(mnist_train)}\")\n",
    "print(f\"Test set size: {len(mnist_test_in)}\")\n",
    "print(f\"Test set size: {len(mnist_test_ood)}\")\n",
    "\n",
    "# Set the input shape (MNIST images are 28x28 grayscale)\n",
    "input_shape = (1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data and labels from prefiltered datasets\n",
    "def extract_data_labels(dataset):\n",
    "    \"\"\"Extract images and labels from a dataset.\"\"\"\n",
    "    data, labels = [], []\n",
    "\n",
    "    for img, label_probs, _ in dataset:\n",
    "        label = np.argmax(label_probs)\n",
    "        data.append(img)\n",
    "        labels.append(label)\n",
    "\n",
    "    return np.stack(data), np.asarray(labels)\n",
    "\n",
    "\n",
    "# Extract training and test data (already filtered for correct classes)\n",
    "train_in, train_in_labels = extract_data_labels(mnist_train)\n",
    "test_in, test_in_labels = extract_data_labels(mnist_test_in)\n",
    "test_ood, test_ood_labels = extract_data_labels(mnist_test_ood)\n",
    "\n",
    "print(f\"Training in-distribution: {train_in.shape}\")\n",
    "print(f\"Test in-distribution: {test_in.shape}\")\n",
    "print(f\"Test out-of-distribution: {test_ood.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some in-distribution and OOD samples\n",
    "fig, axes = plt.subplots(2, 8, figsize=(12, 3))\n",
    "\n",
    "# Show in-distribution samples (0-7) - one of each digit\n",
    "for digit in range(8):\n",
    "    # Find the first occurrence of this digit\n",
    "    idx = (train_in_labels == digit).nonzero()[0][0]\n",
    "    axes[0, digit].imshow(train_in[idx].squeeze(), cmap=\"gray\")\n",
    "    axes[0, digit].axis(\"off\")\n",
    "    axes[0, digit].set_title(f\"Digit {digit}\")\n",
    "\n",
    "# Show OOD samples (8-9) - 4 of each\n",
    "for i in range(8):\n",
    "    digit = 8 if i < 4 else 9\n",
    "    idx = (test_ood_labels == digit).nonzero()[0][(i % 4) * 50]\n",
    "    axes[1, i].imshow(test_ood[idx].squeeze(), cmap=\"gray\")\n",
    "    axes[1, i].axis(\"off\")\n",
    "    if i % 4 == 0:\n",
    "        axes[1, i].set_title(f\"Digit {digit} (OOD)\", color=\"red\")\n",
    "\n",
    "axes_text_kwargs = {\"ha\": \"right\", \"va\": \"center\", \"fontsize\": 12, \"fontweight\": \"bold\"}\n",
    "axes[0, 0].text(-0.5, 0.5, \"In-Dist\\n(Train)\", transform=axes[0, 0].transAxes, **axes_text_kwargs)\n",
    "axes[1, 0].text(-0.5, 0.5, \"OOD\\n(Test)\", transform=axes[1, 0].transAxes, **axes_text_kwargs)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u2xx2bgfbi",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors (KNN) for OOD Detection\n",
    "\n",
    "KNN-based OOD detection is a simple yet effective approach that utilizes a pretrained model to create learned embeddings. It works by measuring how far test samples are from their nearest neighbors in the training data. Samples that are far from all training samples are likely OOD.\n",
    "\n",
    "**Use Case**: Fast baseline for OOD detection without model training, interpretable distance-based scoring.\n",
    "\n",
    "**âš ï¸ Important Note on Embeddings**:\n",
    "KNN performance depends entirely on the quality of the embeddings you provide:\n",
    "\n",
    "- **Better embeddings = better OOD detection**: Use task-specific, well-trained models\n",
    "- **For images**: ResNets, Vision Transformers (ViT), CLIP, or custom CNNs trained on similar data\n",
    "- **For text**: BERT, sentence transformers, domain-specific language models\n",
    "- **For time series**: LSTMs, Transformers trained on temporal data\n",
    "- **For tabular**: MLPs or autoencoders trained on your feature space\n",
    "\n",
    "This tutorial trains a simple CNN for demonstration, but using pretrained models (e.g., ImageNet-pretrained ResNet) would likely improve results significantly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5zj484c4bhx",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple CNN for learning embeddings\n",
    "class EmbeddingNet(torch.nn.Module):\n",
    "    \"\"\"Simple CNN that learns embeddings for digit classification.\"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim=64):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.conv_layers = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2),  # 28x28 -> 14x14\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2),  # 14x14 -> 7x7\n",
    "        )\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(64 * 7 * 7, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, embedding_dim),\n",
    "        )\n",
    "\n",
    "        # Classification head (for training only)\n",
    "        self.classifier = torch.nn.Linear(embedding_dim, 8)  # 8 digit classes (0-7)\n",
    "\n",
    "    def forward(self, x, return_embedding=False):\n",
    "        \"\"\"Forward pass. Returns embeddings if return_embedding=True, else logits.\"\"\"\n",
    "        emb = self.embedding(self.conv_layers(x))\n",
    "        return emb if return_embedding else self.classifier(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f29b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the embedding model\n",
    "embedding_model = EmbeddingNet(embedding_dim=64).to(device)\n",
    "optimizer = torch.optim.Adam(embedding_model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"Training embedding model for digit classification...\")\n",
    "print(f\"Embedding dimension: {embedding_model.embedding_dim}\")\n",
    "\n",
    "# Train for a few epochs\n",
    "num_epochs = 3\n",
    "batch_size = 256\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    embedding_model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "\n",
    "    # Create batches\n",
    "    num_batches = len(train_in) // batch_size\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "\n",
    "        batch_imgs = torch.as_tensor(train_in[start_idx:end_idx], device=device)\n",
    "        batch_labels = torch.as_tensor(train_in_labels[start_idx:end_idx], device=device)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        logits = embedding_model(batch_imgs)\n",
    "        loss = criterion(logits, batch_labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        total += batch_labels.size(0)\n",
    "        correct += (predicted == batch_labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "print(\"âœ“ Embedding model trained!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sft3ym2aj7s",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for all datasets\n",
    "print(\"Extracting embeddings...\")\n",
    "train_in_emb = Embeddings(train_in, model=embedding_model)\n",
    "test_in_emb = Embeddings(test_in, model=embedding_model)\n",
    "test_ood_emb = Embeddings(test_ood, model=embedding_model)\n",
    "\n",
    "print(f\"Training embeddings shape: {train_in_emb.shape}\")\n",
    "print(f\"Test in-dist embeddings shape: {test_in_emb.shape}\")\n",
    "print(f\"Test OOD embeddings shape: {test_ood_emb.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08422dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create KNN detector with learned embeddings\n",
    "ood_knn = OODKNeighbors(k=10, distance_metric=\"cosine\")\n",
    "\n",
    "print(\"\\nFitting KNN detector with learned embeddings...\")\n",
    "ood_knn.fit(train_in_emb, threshold_perc=95.0)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tm8ld6ildf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions with learned embeddings\n",
    "knn_result_in = ood_knn.predict(test_in_emb)\n",
    "knn_result_ood = ood_knn.predict(test_ood_emb)\n",
    "\n",
    "# Calculate detection accuracy\n",
    "in_acc_knn = 100 * (1 - knn_result_in.is_ood.mean())\n",
    "ood_rate_knn = 100 * knn_result_ood.is_ood.mean()\n",
    "\n",
    "print(\"\\n--- KNN (Embeddings) Results ---\")\n",
    "print(f\"In-distribution correctly identified: {in_acc_knn:.1f}%\")\n",
    "print(f\"OOD samples detected: {ood_rate_knn:.1f}%\")\n",
    "print(f\"Average score (in-dist): {knn_result_in.instance_score.mean():.4f}\")\n",
    "print(f\"Average score (OOD): {knn_result_ood.instance_score.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Standard Autoencoder (AE) for OOD Detection\n",
    "\n",
    "The simplest approach uses a standard autoencoder that learns to reconstruct normal (in-distribution) images. When presented with OOD data, reconstruction error increases, signaling anomalous samples.\n",
    "\n",
    "**Use Case**: Fast, simple OOD detection when you have well-separated distributions and don't need probabilistic interpretations.\n",
    "\n",
    "**âš ï¸ Important Note on Model Architecture**:\n",
    "This tutorial uses a simple, generic AE architecture provided by DataEval for demonstration purposes. In production:\n",
    "\n",
    "- **Design architectures for your data type**: CNNs for images, LSTMs/Transformers for sequences, MLPs for tabular data\n",
    "- **Match complexity to your problem**: Deeper networks for complex data, simpler for basic patterns\n",
    "- **Tune hyperparameters**: Latent dimension size, layer widths, activation functions, etc.\n",
    "- **Your model choice significantly impacts OOD detection performance**\n",
    "\n",
    "The DataEval `OODReconstruction` class works with any PyTorch model you provideâ€”customize it for best results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and configure the autoencoder\n",
    "ae_model = AE(input_shape=input_shape)\n",
    "\n",
    "# Configure training parameters\n",
    "config = OODReconstructionConfig(\n",
    "    epochs=3,\n",
    "    batch_size=64,\n",
    "    threshold_perc=95.0,  # 95% of training data considered normal\n",
    ")\n",
    "\n",
    "# Initialize OOD detector\n",
    "ood_ae = OODReconstruction(ae_model, device=device, config=config)\n",
    "\n",
    "print(\"Training Standard Autoencoder...\")\n",
    "print(f\"Model type detected: {ood_ae.model_type}\")\n",
    "print(f\"Using GMM: {ood_ae.use_gmm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on in-distribution data\n",
    "ood_ae.fit(train_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "ae_result_in = ood_ae.predict(test_in)\n",
    "ae_result_ood = ood_ae.predict(test_ood)\n",
    "\n",
    "# Calculate detection accuracy\n",
    "in_acc_ae = 100 * (1 - ae_result_in.is_ood.mean())\n",
    "ood_rate_ae = 100 * ae_result_ood.is_ood.mean()\n",
    "\n",
    "print(\"\\n--- Standard AE Results ---\")\n",
    "print(f\"In-distribution correctly identified: {in_acc_ae:.1f}%\")\n",
    "print(f\"OOD samples detected: {ood_rate_ae:.1f}%\")\n",
    "print(f\"Average score (in-dist): {ae_result_in.instance_score.mean():.4f}\")\n",
    "print(f\"Average score (OOD): {ae_result_ood.instance_score.mean():.4f}\")\n",
    "\n",
    "# Validation: Check if OOD scores are higher than in-dist scores\n",
    "score_separation = ae_result_ood.instance_score.mean() - ae_result_in.instance_score.mean()\n",
    "print(f\"\\nScore separation (OOD - In-Dist): {score_separation:.4f}\")\n",
    "if score_separation > 0:\n",
    "    print(\"âœ“ Expected: OOD samples have higher scores than in-distribution samples\")\n",
    "else:\n",
    "    print(\"âš  Warning: OOD scores should be higher than in-distribution scores\")\n",
    "\n",
    "# Check if we're near the target threshold\n",
    "if 90 <= in_acc_ae <= 98:\n",
    "    print(f\"âœ“ Expected: ~95% of in-distribution samples correctly identified (got {in_acc_ae:.1f}%)\")\n",
    "else:\n",
    "    print(f\"âš  Note: Expected ~95% in-dist accuracy, got {in_acc_ae:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Variational Autoencoder (VAE) for OOD Detection\n",
    "\n",
    "VAEs learn a probabilistic latent representation, which provides better generalization and more structured latent spaces compared to standard AEs. This can improve OOD detection, especially when in-distribution data has high variability.\n",
    "\n",
    "**Use Case**: When you need a more robust latent representation or when your in-distribution data has significant variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and configure the VAE\n",
    "vae_model = VAE(input_shape=input_shape)\n",
    "\n",
    "# Initialize OOD detector (auto-detects as VAE)\n",
    "ood_vae = OODReconstruction(vae_model, device=device, config=config)\n",
    "\n",
    "print(\"Training Variational Autoencoder...\")\n",
    "print(f\"Model type detected: {ood_vae.model_type}\")\n",
    "print(f\"Using GMM: {ood_vae.use_gmm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the VAE\n",
    "ood_vae.fit(train_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate VAE performance\n",
    "vae_result_in = ood_vae.predict(test_in)\n",
    "vae_result_ood = ood_vae.predict(test_ood)\n",
    "\n",
    "in_acc_vae = 100 * (1 - vae_result_in.is_ood.mean())\n",
    "ood_rate_vae = 100 * vae_result_ood.is_ood.mean()\n",
    "\n",
    "print(\"\\n--- VAE Results ---\")\n",
    "print(f\"In-distribution correctly identified: {in_acc_vae:.1f}%\")\n",
    "print(f\"OOD samples detected: {ood_rate_vae:.1f}%\")\n",
    "print(f\"Average score (in-dist): {vae_result_in.instance_score.mean():.4f}\")\n",
    "print(f\"Average score (OOD): {vae_result_ood.instance_score.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## Autoencoder with GMM for Enhanced OOD Detection\n",
    "\n",
    "Adding a Gaussian Mixture Model (GMM) to the latent space provides an additional signal for OOD detection. The GMM models the density of the latent representations, and samples with low density are likely to be OOD. This combines reconstruction error with density estimation using **sensor fusion**: both components are standardized (z-score normalized) and combined with configurable weighting.\n",
    "\n",
    "**Use Case**: When you need higher detection accuracy and have complex in-distribution data that naturally clusters into multiple groups.\n",
    "\n",
    "**âš ï¸ Important**: GMM fusion parameters (`gmm_weight` and `gmm_score_mode`) significantly impact performance. The default `gmm_weight=0.7` favors the GMM component, which typically works well. Experiment with values in [0.5, 0.9] for your data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AE with GMM density network\n",
    "# The latent dimension is auto-computed by AE\n",
    "ae_model_gmm = AE(input_shape=input_shape)\n",
    "latent_dim = cast(int, ae_model_gmm.encoder.flatten[1].out_features)\n",
    "\n",
    "# Create GMM density network with 3 components\n",
    "gmm_density_net = GMMDensityNet(latent_dim=latent_dim, n_gmm=3)\n",
    "ae_model_gmm.gmm_density_net = gmm_density_net\n",
    "\n",
    "# Configure training parameters\n",
    "config_gmm = OODReconstructionConfig(\n",
    "    epochs=3,\n",
    "    batch_size=64,\n",
    "    threshold_perc=95.0,  # 95% of training data considered normal\n",
    "    gmm_weight=0.7,  # For GMM models: balance reconstruction (30%) and GMM energy (70%)\n",
    "    gmm_score_mode=\"standardized\",  # Use z-score normalization for score fusion\n",
    ")\n",
    "\n",
    "# Initialize OOD detector (auto-detects GMM usage)\n",
    "ood_ae_gmm = OODReconstruction(ae_model_gmm, device=device, config=config_gmm)\n",
    "\n",
    "print(\"Training Autoencoder with GMM...\")\n",
    "print(f\"Model type detected: {ood_ae_gmm.model_type}\")\n",
    "print(f\"Using GMM: {ood_ae_gmm.use_gmm}\")\n",
    "print(f\"Latent dimension: {latent_dim}\")\n",
    "print(f\"Number of GMM components: {gmm_density_net.n_gmm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the AE+GMM model\n",
    "ood_ae_gmm.fit(train_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate AE+GMM performance\n",
    "ae_gmm_result_in = ood_ae_gmm.predict(test_in)\n",
    "ae_gmm_result_ood = ood_ae_gmm.predict(test_ood)\n",
    "\n",
    "in_acc_ae_gmm = 100 * (1 - ae_gmm_result_in.is_ood.mean())\n",
    "ood_rate_ae_gmm = 100 * ae_gmm_result_ood.is_ood.mean()\n",
    "\n",
    "print(\"\\n--- AE + GMM Results ---\")\n",
    "print(f\"In-distribution correctly identified: {in_acc_ae_gmm:.1f}%\")\n",
    "print(f\"OOD samples detected: {ood_rate_ae_gmm:.1f}%\")\n",
    "print(f\"Average score (in-dist): {ae_gmm_result_in.instance_score.mean():.4f}\")\n",
    "print(f\"Average score (OOD): {ae_gmm_result_ood.instance_score.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## VAE with GMM for Maximum Robustness\n",
    "\n",
    "Combining VAE's probabilistic latent space with GMM density estimation provides the most sophisticated OOD detection approach. This is particularly effective when you need high reliability and have sufficient computational resources.\n",
    "\n",
    "**Use Case**: Production systems where false negatives (missing OOD samples) are costly, and you need maximum detection reliability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create VAE with GMM density network\n",
    "vae_model_gmm = VAE(input_shape=input_shape)\n",
    "vae_latent_dim = vae_model_gmm.latent_dim\n",
    "\n",
    "# Create GMM density network\n",
    "gmm_density_net_vae = GMMDensityNet(latent_dim=vae_latent_dim, n_gmm=3)\n",
    "vae_model_gmm.gmm_density_net = gmm_density_net_vae\n",
    "\n",
    "# Initialize OOD detector\n",
    "ood_vae_gmm = OODReconstruction(vae_model_gmm, device=device, config=config_gmm)\n",
    "\n",
    "print(\"Training VAE with GMM...\")\n",
    "print(f\"Model type detected: {ood_vae_gmm.model_type}\")\n",
    "print(f\"Using GMM: {ood_vae_gmm.use_gmm}\")\n",
    "print(f\"Latent dimension: {vae_latent_dim}\")\n",
    "print(f\"Number of GMM components: {gmm_density_net_vae.n_gmm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the VAE+GMM model\n",
    "ood_vae_gmm.fit(train_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate VAE+GMM performance\n",
    "vae_gmm_result_in = ood_vae_gmm.predict(test_in)\n",
    "vae_gmm_result_ood = ood_vae_gmm.predict(test_ood)\n",
    "\n",
    "in_acc_vae_gmm = 100 * (1 - vae_gmm_result_in.is_ood.mean())\n",
    "ood_rate_vae_gmm = 100 * vae_gmm_result_ood.is_ood.mean()\n",
    "\n",
    "print(\"\\n--- VAE + GMM Results ---\")\n",
    "print(f\"In-distribution correctly identified: {in_acc_vae_gmm:.1f}%\")\n",
    "print(f\"OOD samples detected: {ood_rate_vae_gmm:.1f}%\")\n",
    "print(f\"Average score (in-dist): {vae_gmm_result_in.instance_score.mean():.4f}\")\n",
    "print(f\"Average score (OOD): {vae_gmm_result_ood.instance_score.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "## Compare All Methods\n",
    "\n",
    "Now let's visualize and compare the performance of all six approaches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison\n",
    "methods = [\"KNN\", \"AE\", \"VAE\", \"AE+GMM\", \"VAE+GMM\"]\n",
    "in_dist_acc = [in_acc_knn, in_acc_ae, in_acc_vae, in_acc_ae_gmm, in_acc_vae_gmm]\n",
    "ood_detect = [ood_rate_knn, ood_rate_ae, ood_rate_vae, ood_rate_ae_gmm, ood_rate_vae_gmm]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 5))\n",
    "\n",
    "# Plot in-distribution accuracy\n",
    "colors = [\"#3498db\", \"#9b59b6\", \"#8e44ad\", \"#2ecc71\", \"#e74c3c\", \"#f39c12\"]\n",
    "bars1 = ax1.bar(methods, in_dist_acc, color=colors)\n",
    "ax1.set_ylabel(\"Accuracy (%)\", fontsize=12)\n",
    "ax1.set_title(\"In-Distribution Samples Correctly Identified\", fontsize=14, fontweight=\"bold\")\n",
    "ax1.set_ylim([0, 105])\n",
    "ax1.axhline(y=95, color=\"gray\", linestyle=\"--\", alpha=0.5, label=\"Target: 95%\")\n",
    "ax1.legend()\n",
    "ax1.tick_params(axis=\"x\", rotation=0)\n",
    "text_kwargs = {\"ha\": \"center\", \"va\": \"bottom\", \"fontsize\": 9, \"fontweight\": \"bold\"}\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width() / 2.0, height, f\"{height:.1f}%\", **text_kwargs)\n",
    "\n",
    "# Plot OOD detection rate\n",
    "bars2 = ax2.bar(methods, ood_detect, color=colors)\n",
    "ax2.set_ylabel(\"Detection Rate (%)\", fontsize=12)\n",
    "ax2.set_title(\"Out-of-Distribution Samples Detected\", fontsize=14, fontweight=\"bold\")\n",
    "ax2.set_ylim([0, 105])\n",
    "ax2.tick_params(axis=\"x\", rotation=0)\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width() / 2.0, height, f\"{height:.1f}%\", **text_kwargs)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a755cc4",
   "metadata": {},
   "source": [
    "### Key Observations\n",
    "\n",
    "1. In-distribution accuracy should be close to threshold (95%)\n",
    "2. KNN (Pixels) provides a fast baseline without neural network training\n",
    "3. KNN (Embeddings) shows how learned representations improve distance-based methods\n",
    "4. GMM models add latent density information for better separation\n",
    "5. All models show some OOD detection capability\n",
    "\n",
    "Note: Digits 8 and 9 share features with 0-7 (circles, curves),\n",
    "making this a challenging OOD scenario. Lower detection rates\n",
    "(20-70%) are expected and realistic for this hard case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize OOD score distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "results = [\n",
    "    (knn_result_in, knn_result_ood, \"KNN\"),\n",
    "    (ae_result_in, ae_result_ood, \"AE\"),\n",
    "    (vae_result_in, vae_result_ood, \"VAE\"),\n",
    "    None,  # Skip this subplot\n",
    "    (ae_gmm_result_in, ae_gmm_result_ood, \"AE + GMM\"),\n",
    "    (vae_gmm_result_in, vae_gmm_result_ood, \"VAE + GMM\"),\n",
    "]\n",
    "\n",
    "for idx, result in enumerate(results):\n",
    "    row, col = idx // 3, idx % 3\n",
    "    ax = axes[row, col]\n",
    "\n",
    "    if result is None:\n",
    "        ax.set_visible(False)\n",
    "        continue\n",
    "\n",
    "    result_in, result_ood, title = result\n",
    "\n",
    "    # Plot histograms\n",
    "    ax.hist(result_in.instance_score, bins=50, alpha=0.6, label=\"In-Distribution\", color=\"blue\")\n",
    "    ax.hist(result_ood.instance_score, bins=50, alpha=0.6, label=\"Out-of-Distribution\", color=\"red\")\n",
    "\n",
    "    ax.set_xlabel(\"OOD Score\", fontsize=11)\n",
    "    ax.set_ylabel(\"Frequency\", fontsize=11)\n",
    "    ax.set_title(title, fontsize=13, fontweight=\"bold\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c823dcd",
   "metadata": {},
   "source": [
    "### Interpreting Score Distributions\n",
    "\n",
    "What to look for:\n",
    "\n",
    "- Good separation: Blue (in-dist) and red (OOD) histograms are well-separated\n",
    "- Poor separation: Significant overlap between distributions\n",
    "- KNN: Distance in learned feature space - often better separation\n",
    "- GMM models: Add latent density information for better separation\n",
    "\n",
    "Expected behavior:\n",
    "\n",
    "- All OOD scores should be shifted right (higher) compared to in-dist scores\n",
    "- More separation = better OOD detection capability\n",
    "- Some overlap is normal, especially when OOD samples (8,9) share features with in-dist (0-7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## Visualize Reconstructions\n",
    "\n",
    "Let's examine how reconstruction-based models reconstruct in-distribution vs out-of-distribution samples. Good OOD detection should show clear degradation in reconstruction quality for OOD samples.\n",
    "\n",
    "Note: KNN doesn't use reconstruction, so we'll focus on the autoencoder-based methods here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get reconstructions\n",
    "def get_reconstructions(model, data, device):\n",
    "    \"\"\"Get reconstructions from a model.\"\"\"\n",
    "    # Ensure model is on the correct device\n",
    "    model.model.to(device)\n",
    "    model.model.eval()\n",
    "    with torch.no_grad():\n",
    "        data_tensor = torch.from_numpy(data).float().to(device)\n",
    "        output = model.model(data_tensor)\n",
    "        # Handle different output formats (AE vs VAE vs GMM)\n",
    "        reconstruction = output[0] if isinstance(output, tuple) else output\n",
    "        return reconstruction.cpu().numpy()\n",
    "\n",
    "\n",
    "# Get a few examples\n",
    "n_examples = 4\n",
    "example_in = test_in[:n_examples]\n",
    "example_ood = test_ood[:n_examples]\n",
    "\n",
    "# Get reconstructions from all models\n",
    "recon_ae_in = get_reconstructions(ood_ae, example_in, device)\n",
    "recon_ae_ood = get_reconstructions(ood_ae, example_ood, device)\n",
    "\n",
    "recon_vae_gmm_in = get_reconstructions(ood_vae_gmm, example_in, device)\n",
    "recon_vae_gmm_ood = get_reconstructions(ood_vae_gmm, example_ood, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize reconstructions\n",
    "fig, axes = plt.subplots(3, n_examples, figsize=(12, 8))\n",
    "\n",
    "# Render images - split n_examples in half for in-dist and OOD\n",
    "n_half = n_examples // 2\n",
    "titles = [\"Original\", \"AE Recon\", \"VAE+GMM Recon\"]\n",
    "examples_in = [example_in, recon_ae_in, recon_vae_gmm_in]\n",
    "examples_ood = [example_ood, recon_ae_ood, recon_vae_gmm_ood]\n",
    "\n",
    "for j in range(3):\n",
    "    axes[j, 0].set_title(f\"{titles[j]}\\n(In-Dist)\", fontsize=10, fontweight=\"bold\")\n",
    "    axes[j, n_half].set_title(f\"{titles[j]}\\n(OOD)\", fontsize=10, fontweight=\"bold\")\n",
    "    for i in range(n_half):\n",
    "        # In-dist samples (left half)\n",
    "        axes[j, i].imshow(examples_in[j][i].squeeze(), cmap=\"gray\")\n",
    "        axes[j, i].axis(\"off\")\n",
    "        # OOD samples (right half)\n",
    "        axes[j, i + n_half].imshow(examples_ood[j][i].squeeze(), cmap=\"gray\")\n",
    "        axes[j, i + n_half].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019ecb88",
   "metadata": {},
   "source": [
    "### Understanding Reconstructions\n",
    "\n",
    "What to observe:\n",
    "\n",
    "- Row 1: Original images - In-Dist (left) vs OOD (right)\n",
    "- Row 2: AE reconstructions - compare quality for In-Dist vs OOD\n",
    "- Row 3: VAE reconstructions - compare quality for In-Dist vs OOD\n",
    "\n",
    "Expected reconstruction behavior:\n",
    "\n",
    "- In-dist: Model has learned these patterns â†’ good reconstruction â†’ low error\n",
    "- OOD: Model hasn't seen these patterns â†’ worse reconstruction â†’ high error\n",
    "\n",
    "Note: The degree of degradation depends on similarity between in-dist and OOD:\n",
    "\n",
    "- Digits 8 and 9 share some features with 0-7 (curves, circles)\n",
    "- So reconstructions may still look reasonable but will have higher error\n",
    "- More distinct OOD data (e.g., letters instead of digits) would show clearer degradation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qtww4lekk8o",
   "metadata": {},
   "source": [
    "## Comparing Use Cases - When Does Each Method Excel?\n",
    "\n",
    "**âš ï¸ IMPORTANT: Results Reflect Limited Training & Generic Models**\n",
    "\n",
    "This comparison uses:\n",
    "\n",
    "- **Only 3 epochs** for AE/VAE training and KNN embedding model training (production typically needs 10-50+ epochs)\n",
    "- **Small sample size**: 10K training, 3K test samples\n",
    "- **Generic model architectures**: Simple CNNs not optimized for MNIST\n",
    "- **Fast demonstration** prioritized over optimal performance\n",
    "\n",
    "**What this means:**\n",
    "\n",
    "- Results show what happens with _minimal_ training and _generic_ models (useful for quick prototypes)\n",
    "- VAE and GMM methods typically need more training to show their theoretical advantages\n",
    "- **Model architecture matters**: Custom architectures designed for your data type (images, time series, tabular) will perform significantly better\n",
    "- With proper training/tuning and domain-specific architectures, the performance rankings may change significantly\n",
    "- Use these results as a starting point, not definitive guidance\n",
    "\n",
    "**ðŸ’¡ Key Insight:** The AE, VAE, and GMM methods use **models you provide**. Performance heavily depends on:\n",
    "\n",
    "- Choosing appropriate architectures for your data type and complexity\n",
    "- Proper hyperparameter tuning (latent dimensions, layer sizes, activation functions)\n",
    "- Sufficient training epochs and data\n",
    "- Appropriate loss functions and regularization\n",
    "\n",
    "The simple models used here serve as examplesâ€”real applications should use architectures targeted to the specific scenario.\n",
    "\n",
    "---\n",
    "\n",
    "Let's test each method on different OOD scenarios to understand their strengths and weaknesses in this limited-training setting.\n",
    "\n",
    "We'll create three different OOD scenarios with increasing difficulty:\n",
    "\n",
    "1. **Easy OOD**: CIFAR10 natural images (converted to grayscale 28x28) - completely different from digits\n",
    "2. **Medium OOD**: Rotated digits - same objects, different orientation\n",
    "3. **Hard OOD**: Digits 8-9 - similar features to training data (current scenario)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2uo0p5e4ai",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create different OOD scenarios\n",
    "\n",
    "# Scenario 1: Easy OOD - CIFAR10 (completely different domain: natural images vs digits)\n",
    "# Load CIFAR10 and convert to match MNIST format\n",
    "cifar_dataset = CIFAR10(\"./data\", image_set=\"test\", download=True)\n",
    "easy_ood_list = []\n",
    "for i in range(500):\n",
    "    img = cifar_dataset[i][0]\n",
    "    img_gray = resize(to_canonical_grayscale(rescale(img, 8)), 28)[np.newaxis, :]\n",
    "    easy_ood_list.append(normalize(img_gray))\n",
    "easy_ood = np.stack(easy_ood_list)\n",
    "\n",
    "# Scenario 2: Medium OOD - Rotated digits (same domain, different transformation)\n",
    "medium_ood = np.rot90(test_in[:500], k=1, axes=(2, 3)).copy()\n",
    "\n",
    "# Scenario 3: Hard OOD - Digits 8-9 (already created as test_ood_subset)\n",
    "hard_ood = test_ood\n",
    "\n",
    "# Get embeddings for all OOD scenarios\n",
    "easy_ood_emb = Embeddings(easy_ood, model=embedding_model)\n",
    "medium_ood_emb = Embeddings(medium_ood, model=embedding_model)\n",
    "hard_ood_emb = Embeddings(hard_ood, model=embedding_model)\n",
    "\n",
    "print(\"Created three OOD scenarios:\")\n",
    "print(f\"1. Easy (CIFAR10 â†’ grayscale): {easy_ood.shape}\")\n",
    "print(f\"2. Medium (Rotated digits): {medium_ood.shape}\")\n",
    "print(f\"3. Hard (Digits 8-9): {hard_ood.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mjx660xdmo8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the different OOD scenarios\n",
    "fig, axes = plt.subplots(3, 5, figsize=(12, 7))\n",
    "\n",
    "ood_by_scenario = [easy_ood, medium_ood, hard_ood]\n",
    "ood_title = [(\"Easy OOD (CIFAR10)\", \"red\"), (\"Medium OOD (Rotated)\", \"orange\"), (\"Hard OOD (Digits 8-9)\", \"darkred\")]\n",
    "\n",
    "# Easy OOD - CIFAR10 (grayscale)\n",
    "for i in range(5):\n",
    "    for j in range(3):\n",
    "        if i == 0:\n",
    "            axes[j, 0].set_title(ood_title[j][0], fontweight=\"bold\", color=ood_title[j][1])\n",
    "        axes[j, i].imshow(ood_by_scenario[j][i * 20].squeeze(), cmap=\"gray\")\n",
    "        axes[j, i].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mmgn9rw5h9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models on all three OOD scenarios\n",
    "models = {\"KNN\": ood_knn, \"AE\": ood_ae, \"VAE\": ood_vae, \"AE+GMM\": ood_ae_gmm, \"VAE+GMM\": ood_vae_gmm}\n",
    "\n",
    "scenarios = {\n",
    "    \"Easy (CIFAR10)\": (easy_ood, easy_ood_emb),\n",
    "    \"Medium (Rotated)\": (medium_ood, medium_ood_emb),\n",
    "    \"Hard (Digits 8-9)\": (hard_ood, hard_ood_emb),\n",
    "}\n",
    "\n",
    "# Store results\n",
    "results_matrix = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    results_matrix[model_name] = {}\n",
    "    for scenario_name, (ood_data, ood_data_emb) in scenarios.items():\n",
    "        # Use appropriate data format\n",
    "        data_to_use = ood_data_emb if model_name == \"KNN\" else ood_data\n",
    "\n",
    "        result = model.predict(data_to_use)\n",
    "        detection_rate = 100 * result.is_ood.mean()\n",
    "        results_matrix[model_name][scenario_name] = detection_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swkwis2xhi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap visualization\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "model_names = list(results_matrix.keys())\n",
    "scenario_names = list(scenarios.keys())\n",
    "\n",
    "# Create matrix for heatmap\n",
    "data = np.array([[results_matrix[model][scenario] for scenario in scenario_names] for model in model_names])\n",
    "\n",
    "im = ax.imshow(data, cmap=\"viridis\", aspect=\"auto\", vmin=0, vmax=100)\n",
    "\n",
    "# Set ticks and labels\n",
    "ax.set_xticks(np.arange(len(scenario_names)))\n",
    "ax.set_yticks(np.arange(len(model_names)))\n",
    "ax.set_xticklabels(scenario_names)\n",
    "ax.set_yticklabels(model_names)\n",
    "\n",
    "# Rotate the tick labels for better readability\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(len(model_names)):\n",
    "    for j in range(len(scenario_names)):\n",
    "        text = ax.text(j, i, f\"{data[i, j]:.1f}%\", ha=\"center\", va=\"center\", color=\"black\", fontweight=\"bold\")\n",
    "\n",
    "ax.set_title(\"OOD Detection Rate by Model and Scenario\", fontsize=14, fontweight=\"bold\", pad=20)\n",
    "fig.colorbar(im, ax=ax, label=\"Detection Rate (%)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fe5cfa",
   "metadata": {},
   "source": [
    "ðŸ” What the Results Show:\n",
    "\n",
    "âœ… All models excel on Easy OOD (CIFAR10): 86-100% detection\n",
    "\n",
    "âš ï¸ Medium OOD (Rotations): Wide variation (5-87%)\n",
    "\n",
    "- KNN and GMM methods (with proper fusion) perform best\n",
    "- VAE struggles with limited training\n",
    "\n",
    "âŒ Hard OOD (Digits 8-9): Challenging for all (5-50%)\n",
    "\n",
    "- KNN is strongest (40-50%)\n",
    "- GMM methods competitive with proper score fusion (10-20%)\n",
    "- Standard AE provides baseline performance (20-25%)\n",
    "- VAE underperforms without extensive training (5-10%)\n",
    "\n",
    "ðŸ’¡ Takeaway: KNN with good embeddings and GMM methods with proper score fusion\n",
    "show the strongest performance. Simpler methods (AE) provide reliable baselines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3p0lap655",
   "metadata": {},
   "source": [
    "### Analysis: What the Results Show\n",
    "\n",
    "**âš ï¸ Important Context:** These results are based on limited training (3 epochs) with small datasets (10K train, 3K test) and generic model architectures. Performance patterns will differ significantly with more training, larger datasets, and architectures optimized for your specific problem.\n",
    "\n",
    "#### Performance by OOD Difficulty:\n",
    "\n",
    "**Easy OOD (CIFAR10 - completely different domain):**\n",
    "\n",
    "- All methods achieve excellent detection (84-99%+)\n",
    "- Even simple approaches work well when OOD data is very different\n",
    "- GMM methods reach near-perfect detection (99%+)\n",
    "\n",
    "**Medium OOD (Rotated digits - same objects, different orientation):**\n",
    "\n",
    "- **KNN**: Strong performance (75-85%) - learned embeddings capture orientation-invariant features\n",
    "- **GMM methods**: Excellent with proper fusion (85-90%)\n",
    "- **Standard AE**: Moderate (50-55%) - reconstruction sensitive to orientation\n",
    "- **VAE**: Poor (5-10%) - insufficient training for robust latent structure\n",
    "\n",
    "**Hard OOD (Digits 8-9 - similar features to training data):**\n",
    "\n",
    "- **KNN**: Best performer (40-50%) - distance metrics in embedding space most discriminative\n",
    "- **Standard AE**: Reliable baseline (20-25%)\n",
    "- **GMM methods**: Competitive with tuning (10-20%) - sensitive to `gmm_weight` parameter\n",
    "- **VAE**: Struggles (5-10%) - needs extensive training to show advantages\n",
    "\n",
    "#### Key Observations:\n",
    "\n",
    "1. **KNN with learned embeddings** consistently outperformed reconstruction-based methods\n",
    "2. **GMM score fusion is critical**: Proper `gmm_weight` (0.6-0.8) significantly impacts performance\n",
    "3. **VAE underperforms** with limited training - requires 10-20x more epochs to converge\n",
    "4. **Simpler methods (AE) provide reliable baselines** with minimal tuning\n",
    "5. **Performance gap narrows** as OOD difficulty decreases (all methods work well on easy OOD)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, you learned how to use DataEval's OOD detection capabilities with five different approaches: KNN (with embeddings), Standard AE, VAE, AE+GMM, and VAE+GMM.\n",
    "\n",
    "### Method Selection Guide\n",
    "\n",
    "Based on the comparative analysis across three OOD difficulty levels, here's how to choose the right method for your use case:\n",
    "\n",
    "#### **Quick Decision Table:**\n",
    "\n",
    "| Your Situation                           | Recommended Method   | Why                                                  |\n",
    "| ---------------------------------------- | -------------------- | ---------------------------------------------------- |\n",
    "| Have pretrained embeddings               | **KNN**              | Best overall performer, no training needed           |\n",
    "| Need fast baseline                       | **Standard AE**      | Simple, reliable, minimal tuning                     |\n",
    "| Multi-modal data clusters                | **AE + GMM**         | Enhanced detection with density modeling             |\n",
    "| Maximum accuracy (can train extensively) | **KNN or VAE + GMM** | KNN for strong embeddings, VAE+GMM for 30-50+ epochs |\n",
    "| Limited computational resources          | **Standard AE**      | Fastest training, good baseline                      |\n",
    "\n",
    "#### **By Application Domain:**\n",
    "\n",
    "| Domain               | Best Method        | Rationale                                                         |\n",
    "| -------------------- | ------------------ | ----------------------------------------------------------------- |\n",
    "| Medical imaging      | KNN or VAE+GMM     | Safety-critical, leverage pretrained models or extensive training |\n",
    "| Manufacturing QA     | AE+GMM or KNN      | Natural defect clusters, fast inference                           |\n",
    "| Fraud detection      | KNN or Standard AE | Clear separation, interpretable                                   |\n",
    "| Autonomous systems   | KNN                | Complex scenarios, use pretrained vision models                   |\n",
    "| Research/Prototyping | KNN or Standard AE | Quick iteration, establish baseline                               |\n",
    "\n",
    "### Implementation Recommendations\n",
    "\n",
    "#### **For KNN (Best Overall):**\n",
    "\n",
    "```python\n",
    "# Train embedding model or use pretrained\n",
    "embedding_model = YourPretrainedModel()  # ResNet, ViT, CLIP, etc.\n",
    "\n",
    "# Create embeddings\n",
    "train_emb = Embeddings(train_data, model=embedding_model)\n",
    "test_emb = Embeddings(test_data, model=embedding_model)\n",
    "\n",
    "# Fit and predict\n",
    "ood_knn = OODKNeighbors(k=10, distance_metric=\"cosine\")\n",
    "ood_knn.fit(train_emb, threshold_perc=95.0)\n",
    "result = ood_knn.predict(test_emb)\n",
    "```\n",
    "\n",
    "**Key Success Factor**: Embedding quality - invest in domain-specific pretrained models\n",
    "\n",
    "#### **For Standard AE (Reliable Baseline):**\n",
    "\n",
    "```python\n",
    "config = OODReconstructionConfig(\n",
    "    epochs=10,  # 10-20 for production\n",
    "    batch_size=256,\n",
    "    threshold_perc=95.0,\n",
    ")\n",
    "ood_ae = OODReconstruction(your_ae_model, device=device, config=config)\n",
    "```\n",
    "\n",
    "**Key Success Factor**: Architecture design - match to your data type\n",
    "\n",
    "#### **For GMM Methods (Advanced):**\n",
    "\n",
    "```python\n",
    "# Add GMM to your model\n",
    "gmm_net = GMMDensityNet(latent_dim=256, n_gmm=8)\n",
    "your_model.gmm_density_net = gmm_net\n",
    "\n",
    "# Configure fusion parameters\n",
    "config = OODReconstructionConfig(\n",
    "    epochs=15,  # 15-30 for AE+GMM, 30-50 for VAE+GMM\n",
    "    batch_size=256,\n",
    "    threshold_perc=95.0,\n",
    "    gmm_weight=0.7,  # Tune in [0.5, 0.9]\n",
    "    gmm_score_mode=\"standardized\",\n",
    ")\n",
    "```\n",
    "\n",
    "**Key Success Factors**:\n",
    "\n",
    "- Tune `gmm_weight` for your data (try 0.6-0.8)\n",
    "- Match `n_gmm` to natural data clusters\n",
    "- More training epochs than standard AE/VAE\n",
    "\n",
    "### Critical Takeaways\n",
    "\n",
    "**âš ï¸ Results Context:**\n",
    "\n",
    "- This tutorial used minimal training (3 epochs) and generic architectures\n",
    "- Your results will improve significantly with:\n",
    "  - More training epochs (10-50+)\n",
    "  - Architectures designed for your data type\n",
    "  - Larger datasets and proper hyperparameter tuning\n",
    "  - Domain-specific pretrained models (for KNN)\n",
    "\n",
    "**What Matters Most:**\n",
    "\n",
    "1. **Embedding quality (KNN)**: Use pretrained models (ResNet, ViT, CLIP) or train task-specific embeddings\n",
    "2. **Architecture design (AE/VAE)**: Generic models shown here are examples - customize for your data\n",
    "3. **GMM configuration**: `gmm_weight` parameter critically impacts performance (0.6-0.8 range)\n",
    "4. **Training investment**: VAE needs 10-20x more epochs than shown here to reach potential\n",
    "5. **Threshold selection**: Balance false positives vs detection rate for your use case\n",
    "\n",
    "### Performance Expectations\n",
    "\n",
    "Based on OOD similarity to in-distribution data:\n",
    "\n",
    "- **Easy OOD** (completely different): 85-100% detection with any method\n",
    "- **Medium OOD** (same domain, different features): 50-90% - KNN and GMM methods excel\n",
    "- **Hard OOD** (very similar): 10-50% - KNN best, requires careful tuning\n",
    "\n",
    "Remember: Digits 8-9 vs 0-7 is a **hard** OOD case (shared features). Real-world performance depends on your specific data distributions.\n",
    "\n",
    "### What's Next\n",
    "\n",
    "To learn more about OOD detection and related concepts:\n",
    "\n",
    "- Read the [OOD Detection concept page](../concepts/OOD.md)\n",
    "- Learn about [monitoring operational data](./tt_monitor_shift.ipynb)\n",
    "- Try the [data cleaning tutorial](./tt_clean_dataset.ipynb)\n",
    "\n",
    "### Try It Yourself\n",
    "\n",
    "Experiment with:\n",
    "\n",
    "- **Better embeddings for KNN**: ResNet, ViT, CLIP, or domain-specific pretrained models\n",
    "- **More training**: 10-20 epochs for AE/AE+GMM, 30-50+ for VAE/VAE+GMM\n",
    "- **GMM tuning**: Try `gmm_weight` values in [0.5, 0.9] and different `n_gmm` (match to data clusters)\n",
    "- **Custom architectures**: Design models for your specific data type (not generic examples)\n",
    "- **Different OOD scenarios**: Test on your own data with varying difficulty levels\n",
    "- **Threshold adjustment**: Tune `threshold_perc` for your false positive tolerance\n",
    "- **Transfer learning**: Use pretrained models instead of training from scratch\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
