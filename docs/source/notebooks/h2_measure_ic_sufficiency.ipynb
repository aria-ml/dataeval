{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to measure dataset sufficiency for image classification\n",
    "\n",
    "This guide provides a beginner friendly how-to guide to analyze an image classification model's hypothetical performance.\n",
    "\n",
    "Estimated time to complete: 10 minutes\n",
    "\n",
    "Relevant ML stages: [Model Development](../concepts/users/ML_Lifecycle.md#model-development)\n",
    "\n",
    "Relevant personas: ML Engineer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _What you'll do_\n",
    "\n",
    "- Evaluate an image classification model's performance with the MNIST dataset\n",
    "- Define a custom evaluation function with metrics of interest\n",
    "- Project the model's performance over increasing sample sizes\n",
    "\n",
    "## _What you'll learn_\n",
    "\n",
    "- Learn to evaluate a model's limits for different metrics with the MNIST dataset\n",
    "- Learn to determine how many samples are required to reach specific performance thresholds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Problem Statement_\n",
    "\n",
    "For machine learning tasks, often we would like to evaluate the performance of a model on a small, preliminary dataset. In situations where data collection is expensive, we would like to extrapolate hypothetical performance out to a larger dataset.\n",
    "\n",
    "DataEval has introduced a method projecting performance via _sufficiency curves_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _When to use_\n",
    "\n",
    "The {class}`.Sufficiency` class should be used when you would like to extrapolate hypothetical performance. For example, if you have a small dataset, and would like to know if it is worthwhile to collect more data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _What you will need_\n",
    "\n",
    "1. A particular model architecture.\n",
    "2. Metric(s) that we would like to evaluate.\n",
    "3. A dataset of interest.\n",
    "4. A Python environment with the following packages installed:\n",
    "   - `tabulate`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Setting up_\n",
    "\n",
    "Let's import the required libraries needed to set up a minimal working example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Google Colab Only\n",
    "try:\n",
    "    import google.colab  # noqa: F401\n",
    "\n",
    "    # specify the version of DataEval (==X.XX.X) for versions other than the latest\n",
    "    %pip install -q dataeval maite-datasets\n",
    "    !export LC_ALL=\"en_US.UTF-8\"\n",
    "    !export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
    "    !export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
    "    !ldconfig /usr/lib64-nvidia\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections.abc import Sequence\n",
    "from typing import Any, cast\n",
    "\n",
    "import dataeval_plots as dep\n",
    "import numpy as np\n",
    "import plotly.io as pio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchmetrics\n",
    "from IPython.display import display  # noqa: A004\n",
    "from maite_datasets.image_classification import MNIST\n",
    "from numpy.typing import NDArray\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "\n",
    "from dataeval import config\n",
    "from dataeval.performance import Sufficiency\n",
    "from dataeval.protocols import Dataset, DatumMetadata\n",
    "from dataeval.selection import Limit, Select\n",
    "\n",
    "DatumType = tuple[NDArray[np.number[Any]], NDArray[np.number[Any]], DatumMetadata]\n",
    "\n",
    "# Set seed for reproducibility\n",
    "config.set_seed(0, all_generators=True)\n",
    "\n",
    "# Set hardware based on system\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "config.set_device(device=device)\n",
    "\n",
    "# Additional reproducibility and printing options\n",
    "np.set_printoptions(formatter={\"float\": lambda x: f\"{x:0.4f}\"})\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "torch.use_deterministic_algorithms(True)\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "\n",
    "# Use plotly to render plots\n",
    "dep.set_default_backend(\"plotly\")\n",
    "\n",
    "# Use the notebook renderer so JS is embedded\n",
    "pio.renderers.default = \"notebook\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and create model\n",
    "\n",
    "Before calculating the sufficiency of a dataset, the dataset must be loaded and the model architecture defined. We will walk through these in the following steps.\n",
    "\n",
    "### Loading MNIST data\n",
    "\n",
    "Load the MNIST data and split it into training and test datasets.\n",
    "For this notebook, we will use subsets of the training (2500) and test (500) data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the dataset transforms\n",
    "\n",
    "transforms = [\n",
    "    lambda x: x / 255.0,  # scale to [0, 1]\n",
    "    lambda x: x.astype(np.float32),  # convert to float32\n",
    "]\n",
    "\n",
    "# Download the mnist dataset and apply the transforms and subset the data\n",
    "train_ds = Select(MNIST(root=\"./data\", image_set=\"train\", transforms=transforms,download=True),selections=[Limit(2500)])  # fmt: skip # noqa: E501\n",
    "test_ds = Select(MNIST(root=\"./data\", image_set=\"test\", transforms=transforms, download=True), selections=[Limit(500)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a PyTorch model\n",
    "\n",
    "Next, we define the network architecture that will be trained and then evaluated throughout the sufficiency calculation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(6400, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Compile the model (cast sets the type to Net as compile returns an Unknown)\n",
    "model: Net = cast(Net, torch.compile(Net().to(device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategy Protocols\n",
    "\n",
    "Training and evaluation functions are heavily dependent on the hyperparameters defined by a user. These can include metrics, loss functions, optimizers, model architectures, input sizes, etc.\n",
    "\n",
    "To allow the Sufficiency class to handle this situation, DataEval uses [_Protocols_](https://typing.python.org/en/latest/spec/protocol.html). Sufficiency requires two specific protocols called {class}`.TrainingStrategy` and {class}`.EvaluationStrategy`.  \n",
    "Below we will define the strategies that align with this notebook and combine them into a {class}`.Sufficiency.Config` that can be given to the `Sufficiency` class.\n",
    "\n",
    "### Training strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTTrainingStrategy:\n",
    "    def train(self, model: nn.Module, dataset: Dataset[DatumType], indices: Sequence[int]):\n",
    "        # Defined only for this testing scenario\n",
    "        criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "        epochs = 10\n",
    "\n",
    "        # Define the dataloader for training\n",
    "        dataloader = DataLoader(Subset(cast(TorchDataset, dataset), indices), batch_size=8)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch in dataloader:\n",
    "                # Load data/images to device\n",
    "                X = torch.Tensor(batch[0]).to(device)\n",
    "                # Load one-hot encoded targets/labels to device\n",
    "                y = torch.argmax(torch.asarray(batch[1], dtype=torch.int).to(device), dim=1)\n",
    "                # Zero out gradients\n",
    "                optimizer.zero_grad()\n",
    "                # Forward propagation\n",
    "                outputs = model(X)\n",
    "                # Compute loss\n",
    "                loss = criterion(outputs, y)\n",
    "                # Back prop\n",
    "                loss.backward()\n",
    "                # Update weights/parameters\n",
    "                optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation strategy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTEvaluationStrategy:\n",
    "    def evaluate(self, model: nn.Module, dataset: Dataset[DatumType]) -> dict[str, float]:\n",
    "        # Metrics of interest\n",
    "        metrics = {\n",
    "            \"Accuracy\": torchmetrics.Accuracy(task=\"multiclass\", num_classes=10).to(device),\n",
    "            \"AUROC\": torchmetrics.AUROC(task=\"multiclass\", num_classes=10).to(device),\n",
    "            \"TPR at 0.5 Fixed FPR\": torchmetrics.ROC(task=\"multiclass\", average=\"macro\", num_classes=10).to(device),\n",
    "        }\n",
    "        result = {}\n",
    "        # Set model layers into evaluation mode\n",
    "        model.eval()\n",
    "        dataloader = DataLoader(cast(TorchDataset, dataset), batch_size=8)\n",
    "        # Tell PyTorch to not track gradients, greatly speeds up processing\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                # Load data/images to device\n",
    "                X = torch.Tensor(batch[0]).to(device)\n",
    "                # Load one-hot encoded targets/labels to device\n",
    "                y = torch.argmax(torch.asarray(batch[1], dtype=torch.int).to(device), dim=1)\n",
    "                preds = model(X)\n",
    "                for metric in metrics.values():\n",
    "                    metric.update(preds, y)\n",
    "            # Compute ROC curve\n",
    "            false_positive_rate, true_positive_rate, _ = metrics[\"TPR at 0.5 Fixed FPR\"].compute()\n",
    "            # determine interval to examine\n",
    "            desired_rate = 0.5\n",
    "            closest_desired_index = torch.argmin(torch.abs(false_positive_rate - desired_rate)).item()\n",
    "            # return corresponding tpr value\n",
    "            result[\"TPR at 0.5 Fixed FPR\"] = true_positive_rate[closest_desired_index].cpu()\n",
    "            result[\"Accuracy\"] = metrics[\"Accuracy\"].compute().cpu()\n",
    "            result[\"AUROC\"] = metrics[\"AUROC\"].compute().cpu()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reset strategy\n",
    "\n",
    "The `Sufficiency` class requires a `reset_strategy` that resets the model's parameters between runs. This ensures each run starts from a fresh initialization. Here's a simple implementation for PyTorch models:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_model(model: nn.Module) -> nn.Module:\n",
    "    \"\"\"Reset all parameters in a PyTorch model.\"\"\"\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def weight_reset(m: nn.Module) -> None:\n",
    "        reset_fn = getattr(m, \"reset_parameters\", None)\n",
    "        if callable(reset_fn):\n",
    "            reset_fn()\n",
    "\n",
    "    return model.apply(fn=weight_reset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sufficiency config\n",
    "\n",
    "Do not forget to initialize your strategy classes!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_config = Sufficiency.Config(\n",
    "    training_strategy=MNISTTrainingStrategy(),\n",
    "    evaluation_strategy=MNISTEvaluationStrategy(),\n",
    "    reset_strategy=reset_model,\n",
    "    runs=5,\n",
    "    substeps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize sufficiency metric\n",
    "\n",
    "Attach the custom training and evaluation functions to the Sufficiency metric and define the number of models to train in parallel (stability), as well as the number of steps along the learning curve to evaluate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate sufficiency metric\n",
    "suff = Sufficiency(\n",
    "    model=model,\n",
    "    train_ds=train_ds,\n",
    "    test_ds=test_ds,\n",
    "    config=mnist_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Sufficiency\n",
    "\n",
    "Now we can evaluate the metric to train the models and produce the learning curve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train & test model\n",
    "output = suff.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out sufficiency output in a table format\n",
    "output.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out projected output values\n",
    "output.project([1000, 2500, 5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the output using dataeval-plots library\n",
    "for plot in dep.plot(output, backend=\"plotly\"):\n",
    "    display(plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Using these learning curves, we can project performance under much larger datasets (with the same models).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting sample requirements\n",
    "\n",
    "We can also predict the amount of training samples required to achieve specific performance thresholds.\n",
    "\n",
    "Let's say we wanted to see how many samples are needed to hit 90%, 93%, and 99% accuracy, area under the receiver operating characteristic, and true positive rate at a fixed false positive rate of 0.5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the array of desired thresholds to apply to all metrics\n",
    "output.inv_project([0.90, 0.93, 0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a value of \"-1\" samples, the projection shows that given the current model, hitting an accuracy of 99% is improbable.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
