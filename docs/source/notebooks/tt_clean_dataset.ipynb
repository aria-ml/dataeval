{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Cleaning\n",
    "\n",
    "Part 1 of our introduction to exploratory data analysis guide\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What you'll do\n",
    "\n",
    "- You will use DataEval's linters to assess the 2011 VOC dataset.\n",
    "- You will analyze the results through various plots and tables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What you'll learn\n",
    "\n",
    "- You'll learn how to assess a dataset for extreme and/or redundant data points.\n",
    "- You'll learn helpful questions to determine when to remove or collect additional data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What you'll need\n",
    "\n",
    "- Environment Requirements\n",
    "  - `dataeval` or `dataeval[all]`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Exploratory Data Analysis (EDA) is an approach to analyzing data sets to summarize the main characteristics and identify incongruencies in the data.\n",
    "Before diving into machine learning or statistical modeling, it is crucial to understand the data you are working with.\n",
    "EDA helps in understanding the patterns, detecting anomalies, checking assumptions, and determining relationships in the data.\n",
    "\n",
    "One of the most important aspects of EDA is data cleaning.\n",
    "A portion of DataEval is dedicated to being able to identify duplicates and outliers as well as data points that have missing or too many extreme values.\n",
    "These techniques help ensure that you only include high quality data for your projects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-by-Step Guide\n",
    "\n",
    "This guide will walk through how to use DataEval to perform basic data cleaning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup\n",
    "\n",
    "You'll begin by importing the necessary libraries to walk through this guide.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab  # noqa: F401\n",
    "\n",
    "    # specify the version of DataEval (==X.XX.X) for versions other than the latest\n",
    "    %pip install -q dataeval\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will need matplotlib for visualing our dataset and\n",
    "# numpy to be able to handle the data.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from maite_datasets.object_detection import VOCDetection\n",
    "\n",
    "# Load the classes from DataEval that are helpful for EDA\n",
    "from dataeval.detectors.linters import Duplicates, Outliers\n",
    "from dataeval.metrics.stats import hashstats, labelstats\n",
    "\n",
    "# Set the random value\n",
    "rng = np.random.default_rng(213)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper method to plot images of interest\n",
    "def plot_sample_images(\n",
    "    dataset, outlier_class, outlier_result, metric: str, metric_dict: dict[str, list[int]], layout: tuple[int, int]\n",
    ") -> None:\n",
    "    _, axs = plt.subplots(*layout, figsize=(10, layout[0] * 4))\n",
    "    selected_index = rng.choice(metric_dict[metric], min(int(np.prod(layout)), len(metric_dict[metric])), replace=False)\n",
    "\n",
    "    for i, ax in enumerate(axs.flat):\n",
    "        ax.imshow(dataset[selected_index[i]][0].transpose(1, 2, 0))\n",
    "        ax.set_title(f\"{metric}={np.round(outlier_result.issues[selected_index[i]][metric], 2)}\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    print(f\"metric={metric}\")\n",
    "    print(f\"quantiles={np.round(np.quantile(outlier_class.stats.data()[metric], [0, 0.25, 0.5, 0.75, 1]), 2)}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Understand the Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data\n",
    "\n",
    "You are going to work with the PASCAL VOC 2011 dataset.\n",
    "This dataset is a small curated dataset that was used for a computer vision competition.\n",
    "The images were used for classification, object detection, and segmentation.\n",
    "This dataset was chosen because it has multiple classes and images with a variety of sizes and objects.\n",
    "\n",
    "If this data is already on your computer you can change the file location from `\"./data\"` to wherever the data is stored.\n",
    "Just remember to also change the download value from `True` to `False`.\n",
    "\n",
    "For the sake of ensuring that this tutorial runs quickly on most computers, you are going to analyze only the training set of the data, which is a little under 6000 images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data and then load it as a torch Tensor\n",
    "ds = VOCDetection(\"./data\", image_set=\"train\", year=\"2011\", download=True)\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the Data\n",
    "\n",
    "As this data was used for a computer vision competition, it will most likely have very few issues, but it is always worth it to check.\n",
    "Many of the large webscraped datasets available for use do contain image issues.\n",
    "Verifying in the beginning that you have a high quality dataset is always easier than finding out later that you trained a model on a dataset with erroneous images or a set of splits with leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate basic label statistics from the dataset\n",
    "lstats = labelstats(ds)\n",
    "\n",
    "# Display label stats\n",
    "print(lstats.to_table())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above table shows that this dataset has a total of 20 classes.\n",
    "\n",
    "Of the classes, `person` is the class with the highest total object count followed by `chair` and `car`, while `person`, `chair` and `dog` are the classes with the highest number of images.\n",
    "\n",
    "`cow`, `sheep`, and `bus` are the classes with least number of objects, while `bus`, `train` and `cow` are the classes with the least number of images.\n",
    "\n",
    "This table helps point out the wide variation in\n",
    "\n",
    "- the number of classes per image,\n",
    "- the number of objects per image,\n",
    "- and the number of objects of each class per image.\n",
    "\n",
    "This highlights an important concept - class balance.\n",
    "A dataset that is imbalanced can result in a model that chooses the more prominent class more often just because there are more samples in that class.\n",
    "To explore this concept further, see the bias tutorial in the [What's Next](#whats-next) section at the end of this tutorial.\n",
    "\n",
    "Now that the metadata has been examined, it's important to inspect random images to get an idea of the variety of backgrounds, the range of colors, the locations of objects in images,\n",
    "and how often an image is seen with a single object versus multiple objects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot random images from each category\n",
    "_, axs = plt.subplots(5, 4, figsize=(8, 10))\n",
    "\n",
    "for ax, (category, indices) in zip(axs.flat, lstats.image_indices_per_class.items()):\n",
    "    # Randomly select an index from the list of indices\n",
    "    ax.imshow(ds[rng.choice(indices)][0].transpose(1, 2, 0))\n",
    "    ax.set_title(lstats.class_names[category])\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the images displays the variety in the images, including image sizes, image brightness, object sizes, backgrounds, number of objects in the image, and even the lack of color in a few images which are black and white.\n",
    "\n",
    "This is where DataEval comes in. It's designed to help you make sense of the many different aspects that affect building representative datasets and robust models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to making sure that you understand the structure of the labels and have visualized some of the images from the dataset,\n",
    "you can also visualize the data distribution across different statistics such as the image size or the pixel mean.\n",
    "In order to view these distributions, you have to use DataEval's stat functions and plot the results.\n",
    "For more information, see :module:`dataeval.metrics.stats`.\n",
    "\n",
    "Now, you can move on to identifying which images have a statistical difference from the rest of the images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Identify any Outlying Data Points\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extreme/Missing Values\n",
    "\n",
    "Here you will detect and identify the images associated with the extreme values from DataEval's stat functions.\n",
    "To detect these extreme values, you will use the :class:`.Outliers` class.\n",
    "The `Outliers` class has multiple methods to determine the extreme values, which are discussed in the [Data Cleaning explanation](../concepts/DataCleaning.md).\n",
    "For this guide, you will use the \"zscore\" as the Z score defines outliers in a normal distribution.\n",
    "\n",
    "The output of the `Outliers` class contains a dictionary where the image number is the key and the value is a dictionary containing the flagged metrics and their value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell takes about 1-5 minutes to run depending on your hardware\n",
    "\n",
    "# Initialize the Outliers class\n",
    "outliers = Outliers(outlier_method=\"zscore\")\n",
    "\n",
    "# Find the extreme images\n",
    "outlier_imgs = outliers.evaluate(ds)\n",
    "\n",
    "# View the number of extreme images\n",
    "print(f\"Number of images with extreme values: {len(outlier_imgs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class can flag a lot of images, depending on how varied the dataset is and which method you use to define extreme values.\n",
    "Using the zscore, it flagged 480 images across 15 metrics out of the 5717 images in the dataset.\n",
    "However, switching the method can give different results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the metrics with an extreme value\n",
    "metrics = {}\n",
    "for img, group in outlier_imgs.issues.items():\n",
    "    for extreme in group:\n",
    "        if extreme in metrics:\n",
    "            metrics[extreme].append(img)\n",
    "        else:\n",
    "            metrics[extreme] = [img]\n",
    "print(f\"Number of metrics with extremes: {len(metrics)}\")\n",
    "\n",
    "# Show the total number of extreme values for each metric\n",
    "for group, imgs in sorted(metrics.items(), key=lambda item: len(item[1]), reverse=True):\n",
    "    print(f\"  {group} - {len(imgs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digging into the flagged images and organizing them by category shows that the metric with the most extreme values is \"size\" while \"sharpness\" has the least number of extreme values.\n",
    "\n",
    "`Outliers` is designed to flag any images on the edge of each metric's data distribution.\n",
    "Some images will get flagged as an outlier by multiple metrics, while others will get flagged by only a single metric.\n",
    "It is then up to you, the user, to shift through the information provided by the result from `Outliers`.\n",
    "\n",
    "Part of exploring the results includes displaying how the flagged images are spread across the 20 classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the table\n",
    "print(outlier_imgs.to_table(lstats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the trends to note from the table above which splits the issues by class and metric:\n",
    "\n",
    "- An image with an unusual aspect ratio is most likely to contain a boat or aeroplane.\n",
    "- An image with an issue in brightness is most likely to contain an aeroplane.\n",
    "- An image with an issue in darkness is most likely to be a person.\n",
    "- Images with high contrast are likely to fall within 1 of 4 classes: bottle, cat, chair, person.\n",
    "- Images with low entropy (think image with constant pixels) are likely to fall within 1 of 4 classes: aeroplane, bird, bottle, person.\n",
    "- Unusual skew and kurtosis images follow a similar trend as entropy.\n",
    "- Every class has images with size issues.\n",
    "\n",
    "Something to remember is that there are different number of images for each class and that effective use of this tool requires understanding the dataset in question. For example, 36 low entropy images out of the 2000 for person might be outliers while 28 low entropy images out of 300 for aeroplane might not be;\n",
    "low entropy might be an inherent characteristic of the aeroplane class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to understand the above table, you will plot sample images from a few of the metrics, specifically:\n",
    "\n",
    "- entropy\n",
    "- size\n",
    "- zeros\n",
    "- sharpness\n",
    "\n",
    "Entropy, variance, standard deviation, kurtosis, and skew all measure (in different ways) how much change there is across the pixels in the image, and entropy will be the easiest to understand.\n",
    "\n",
    "Size, width, height and aspect ratio are all interrelated and size has the most extreme images from those.\n",
    "\n",
    "Zeros is a category unto itself but it is closely related to brightness, contrast, darkness, and mean. Zeros measures the percentage of pixels with a zero value compared to the average image.\n",
    "\n",
    "Sharpness is also in it's own category and it measures the perceived edges in an image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(questions)=\n",
    "\n",
    "#### Questions\n",
    "\n",
    "When looking at these images, you want to think about the following questions:\n",
    "\n",
    "- Does this image represent something that would be expected in operation?\n",
    "- Is there commonality to the objects in the images?\n",
    "- Is there commonality to the backgrounds of the images?\n",
    "- Is there commonality to the class of objects in the images?\n",
    "\n",
    "Asking these questions will help you notice things like all objects being located on the leftside of the image or all the images of a specific class have a specific background.\n",
    "Training a model with data that has commonalities can cause your model to develop biases or limit your model's ability to generalize to non-training data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot images flagged for \"entropy\"\n",
    "plot_sample_images(ds, outliers, outlier_imgs, \"entropy\", metrics, (2, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you examine the flagged images for entropy, look for patterns in the content of the images. Many of these images may feature backgrounds with very little variation, such as water or sky. Others might have darker backgrounds than usual.\n",
    "\n",
    "For example, in an operational setting, water or sky backgrounds may or may not appear frequently, depending on the expected use case. Similarly, darker images may indicate low-light conditions, which could suggest either operational relevance (e.g., night operations) or anomalies that need to be addressed.\n",
    "\n",
    "To refine your dataset, decide whether these flagged images represent scenarios that align with your goals. If they do, consider collecting more data with similar characteristics to balance your dataset. If not, these images may be excluded as outliers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot images flagged for \"size\"\n",
    "plot_sample_images(ds, outliers, outlier_imgs, \"size\", metrics, (2, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flagged images for size often include examples where the objects in the image are unusually large or small relative to the rest of the dataset. For instance, animal images might have a wide range of sizes depending on how the photographs were taken.\n",
    "\n",
    "If your workflow involves preprocessing images to a uniform size, verify that resizing does not distort important details. For example, cropping could remove key parts of the image, while resizing could stretch or compress objects. Alternatively, if you plan to filter images based on size, ensure this doesn’t introduce bias—for example, by disproportionately excluding images of certain classes or contexts.\n",
    "\n",
    "After evaluating the flagged images, you may notice that size discrepancies are common across multiple classes, as shown in the earlier table. This observation suggests that these issues are a general feature of the dataset, and dropping all size outliers might be an appropriate step. However, be cautious and verify whether this action creates any imbalances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zeros\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot images flagged for \"zeros\"\n",
    "plot_sample_images(ds, outliers, outlier_imgs, \"zeros\", metrics, (2, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images flagged for zeros typically feature large regions of completely black or gray pixels. Some of these may also appear in grayscale. These characteristics could indicate issues like underexposed photos, scanning errors, or specific use cases.\n",
    "\n",
    "Grayscale images, in particular, might stand out if the rest of your dataset is primarily in color. Check whether grayscale images are relevant to your operational scenario or whether they are artifacts of the data collection process.\n",
    "\n",
    "For instance, if grayscale images are operationally irrelevant, consider removing them. However, if grayscale scenarios are possible, ensure that you have sufficient representation of these types of images to train a robust model. Similarly, dark images with many zero-value pixels may indicate rare but valid scenarios (e.g., nighttime operations) or irrelevant anomalies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sharpness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot images flagged for \"sharpness\"\n",
    "plot_sample_images(ds, outliers, outlier_imgs, \"sharpness\", metrics, (1, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sharpness measures the clarity of edges in an image. Flagged images often include those with unusually crisp or blurry details. For instance, you might notice a close-up shot of leaves or grass, where the texture stands out significantly compared to other images in the dataset.\n",
    "\n",
    "Evaluate whether these highly detailed images are typical of your use case. If they are uncommon in your operational scenario, they might skew your model's ability to generalize. In such cases, consider excluding these images. Conversely, if they are operationally relevant, ensure that similar images are sufficiently represented in your dataset to prevent biases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linting Summary\n",
    "\n",
    "The Outliers class identifies images that deviate significantly from the dataset's overall distribution. While it cannot determine operational relevance, it highlights patterns that may require further investigation.\n",
    "\n",
    "For example, flagged images might reflect real-world scenarios underrepresented in your dataset, such as night operations or objects photographed from unusual angles. Alternatively, they may reveal anomalies, such as artifacts from the data collection process.\n",
    "\n",
    "By reviewing flagged images for multiple metrics and plotting examples, you can better understand how the Outliers class identifies extremes. This hands-on exploration helps you decide whether to include or exclude specific images based on your dataset's intended use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Identify duplicate data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicates\n",
    "\n",
    "Now that you know how to identify poor quality images in your dataset, another important aspect of data cleaning is detecting and removing any duplicates.\n",
    "\n",
    "The `Duplicates` class identifies both exact duplicates and potential (near) duplicates.\n",
    "Potential duplicates can occur in a variety of ways:\n",
    "\n",
    "- Intentional perturbations\n",
    "  - Images with varying brightness\n",
    "  - Translating the image\n",
    "  - Padding the image\n",
    "  - Cropping the image\n",
    "- Unintentional changes\n",
    "  - Copying the image from one format to another (png->jpeg)\n",
    "  - Using the same image with two different filenames\n",
    "  - Duplicate frames from video extraction\n",
    "  - Oversight in the data collection process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Duplicates class\n",
    "dups = Duplicates()\n",
    "\n",
    "# Find the duplicates\n",
    "dups.evaluate(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected there are no duplicates in this dataset, since it was curated for a specific competition.\n",
    "\n",
    "However, to highlight the abilities of the `Duplicates` class, you will add some duplicates to the dataset and then rerun the `Duplicates` class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create exact and duplicate images\n",
    "\n",
    "# Copy images 23 and 46 to create exact duplicates\n",
    "# Copy and crop images 5 and 4376 to create near duplicates\n",
    "dupes = [\n",
    "    ds[23][0],\n",
    "    ds[46][0],\n",
    "    ds[5][0][:, 5:-5, 5:-5],\n",
    "    ds[4376][0][:, :-5, 5:],\n",
    "]\n",
    "\n",
    "dupes_stats = hashstats(dupes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the duplicates appended to the dataset\n",
    "duplicates = dups.from_stats([dups.stats, dupes_stats])\n",
    "print(f\"exact: {duplicates.exact}\")\n",
    "print(f\"near: {duplicates.near}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, the `Duplicates` class identified all images from the second dataset as exact or near duplicates.\n",
    "\n",
    "Images 0 and 1 from dataset 1 are identified as exact duplicates of images 23 and 46, respectively from the original dataset (dataset 0). Images 2 and 3 from dataset 1 are identified as near duplicates of images 5 and 4376, respectively, which were cropped from the original dataset (dataset 0).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through this process, you've learned how to use DataEval's `Outliers` class to identify and analyze images that deviate from the overall distribution of your dataset and DataEval's `Duplicates` class to identify exact and near duplicates. By examining the images flagged by the different metrics, you gained a deeper understanding of potential issues within your dataset. In this tutorial, the following were covered:\n",
    "\n",
    "- **Underrepresented classes** that may require additional data collection.\n",
    "- **Inconsistencies in image characteristics**, such as brightness, sharpness, or size, which could affect model performance.\n",
    "- **Duplicate data** that can affect model performance.\n",
    "\n",
    "This work has provided a clearer picture of your dataset's strengths and limitations. You are now equipped to make informed decisions about which data points to keep, remove, or augment. For example, you may decide to exclude irrelevant outliers, collect more data for underrepresented scenarios, or address biases that could impact your model's generalizability.\n",
    "\n",
    "By using DataEval, you are not just refining your dataset—you are laying the groundwork for creating a more representative, balanced, and reliable dataset. These insights ultimately enable the development of models that perform robustly in real-world operational settings.\n",
    "\n",
    "DataEval’s tools empower you to move from raw data to actionable insights, ensuring your dataset is not only comprehensive but also aligned with your specific goals and requirements.\n",
    "\n",
    "Good luck with your data!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next\n",
    "\n",
    "Learn how to do the following:\n",
    "\n",
    "- [Assess the data space](./tt_assess_data_space.ipynb)\n",
    "- [Identify bias and correlations](./tt_identify_bias.ipynb)\n",
    "- [Monitor shifting operational data](./tt_monitor_shift.ipynb)\n",
    "\n",
    "To learn more about specific functions or classes, see the [API Reference](../reference/autoapi/dataeval/index.rst) section.\n",
    "To learn more about data cleaning, see the [Data Cleaning](../concepts/DataCleaning.md) explanation page.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On your own\n",
    "\n",
    "Now that you've gone through a tutorial on exploring a dataset, try going through the tutorial again with the test set, full dataset, or even your own dataset.\n",
    "One thing to look for when checking other sets of data is to observe how the stats of each grouping of data changes or doesn't change.\n",
    "\n",
    "You can also play around with the different statistical methods that the `Outlier` class employs to see how the method affects the number and type of issues detected.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
