{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab  # noqa: F401\n",
    "\n",
    "    %pip install -q dataeval\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torchvision.transforms.v2 import GaussianNoise\n",
    "\n",
    "from dataeval.data import Embeddings, Metadata\n",
    "from dataeval.detectors.drift import DriftCVM, DriftKS, DriftMMD\n",
    "from dataeval.metrics.bias import label_parity\n",
    "from dataeval.utils.datasets import VOCDetectionTorch\n",
    "\n",
    "# Set a random seed\n",
    "rng = np.random.default_rng(213)\n",
    "\n",
    "# Set default torch device for notebook\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the embedding network\n",
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Load in pretrained resnet18 model\n",
    "        self.model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        # Add an additional fully connected layer with an embedding dimension of 128\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run input data through the model\"\"\"\n",
    "\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /home/dataeval/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0% 0.00/44.7M [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 14% 6.25M/44.7M [00:00<00:00, 64.6MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 28% 12.5M/44.7M [00:00<00:00, 54.0MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 43% 19.0M/44.7M [00:00<00:00, 59.4MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 56% 24.9M/44.7M [00:00<00:00, 57.8MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 69% 30.8M/44.7M [00:00<00:00, 58.9MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 83% 37.0M/44.7M [00:00<00:00, 60.7MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 96% 42.9M/44.7M [00:00<00:00, 56.0MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100% 44.7M/44.7M [00:00<00:00, 58.1MB/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embedding_net = EmbeddingNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pretrained model transformations\n",
    "transforms = models.ResNet18_Weights.DEFAULT.transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCDetectionTorch Dataset\n",
      "-------------------------\n",
      "    Year: 2011\n",
      "    Transforms: [ImageClassification(\n",
      "    crop_size=[224]\n",
      "    resize_size=[256]\n",
      "    mean=[0.485, 0.456, 0.406]\n",
      "    std=[0.229, 0.224, 0.225]\n",
      "    interpolation=InterpolationMode.BILINEAR\n",
      ")]\n",
      "    Image_set: train\n",
      "    Metadata: {'id': 'VOCDetectionTorch_train', 'index2label': {0: 'aeroplane', 1: 'bicycle', 2: 'bird', 3: 'boat', 4: 'bottle', 5: 'bus', 6: 'car', 7: 'cat', 8: 'chair', 9: 'cow', 10: 'diningtable', 11: 'dog', 12: 'horse', 13: 'motorbike', 14: 'person', 15: 'pottedplant', 16: 'sheep', 17: 'sofa', 18: 'train', 19: 'tvmonitor'}, 'split': 'train'}\n",
      "    Path: /dataeval/docs/source/tutorials/data/VOC2011\n",
      "    Size: 5717\n"
     ]
    }
   ],
   "source": [
    "# Load the training dataset\n",
    "train_ds = VOCDetectionTorch(\"./data\", year=\"2011\", image_set=\"train\", download=False, transforms=transforms)\n",
    "print(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCDetectionTorch Dataset\n",
      "-------------------------\n",
      "    Year: 2011\n",
      "    Transforms: [ImageClassification(\n",
      "    crop_size=[224]\n",
      "    resize_size=[256]\n",
      "    mean=[0.485, 0.456, 0.406]\n",
      "    std=[0.229, 0.224, 0.225]\n",
      "    interpolation=InterpolationMode.BILINEAR\n",
      ")]\n",
      "    Image_set: val\n",
      "    Metadata: {'id': 'VOCDetectionTorch_val', 'index2label': {0: 'aeroplane', 1: 'bicycle', 2: 'bird', 3: 'boat', 4: 'bottle', 5: 'bus', 6: 'car', 7: 'cat', 8: 'chair', 9: 'cow', 10: 'diningtable', 11: 'dog', 12: 'horse', 13: 'motorbike', 14: 'person', 15: 'pottedplant', 16: 'sheep', 17: 'sofa', 18: 'train', 19: 'tvmonitor'}, 'split': 'val'}\n",
      "    Path: /dataeval/docs/source/tutorials/data/VOC2011\n",
      "    Size: 5823\n"
     ]
    }
   ],
   "source": [
    "# Load the \"operational\" dataset\n",
    "operational_ds = VOCDetectionTorch(\"./data\", year=\"2011\", image_set=\"val\", download=False, transforms=transforms)\n",
    "print(operational_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training batches and targets\n",
    "train_embs = Embeddings(train_ds, batch_size=64, model=embedding_net, cache=True)\n",
    "\n",
    "# Create operational batches and targets\n",
    "operational_embs = Embeddings(operational_ds, batch_size=64, model=embedding_net, cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5717, torch.Size([128]))\n",
      "(5823, torch.Size([128]))\n"
     ]
    }
   ],
   "source": [
    "print(f\"({len(train_embs)}, {train_embs[0].shape})\")  # (5717, shape)\n",
    "print(f\"({len(operational_embs)}, {operational_embs[0].shape})\")  # (5823, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A type alias for all of the drift detectors\n",
    "DriftDetector = DriftMMD | DriftCVM | DriftKS\n",
    "\n",
    "# Create a mapping for the detectors to iterate over\n",
    "detectors: dict[str, DriftDetector] = {\n",
    "    \"MMD\": DriftMMD(train_embs),\n",
    "    \"CVM\": DriftCVM(train_embs),\n",
    "    \"KS\": DriftKS(train_embs),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0118, -0.4469,  0.7662,  ..., -0.7061, -0.6653, -0.7943],\n",
       "        [-0.1112,  0.2925,  0.7559,  ..., -0.6217, -0.4802, -0.0090],\n",
       "        [-0.7563, -0.3675, -0.5289,  ...,  0.2602,  0.3514, -0.7790],\n",
       "        ...,\n",
       "        [ 0.6948, -0.2368,  1.0821,  ..., -1.5768, -0.1185, -0.1115],\n",
       "        [ 1.0290,  0.1698,  1.1292,  ...,  0.5667,  0.0767, -0.4378],\n",
       "        [ 0.4550,  0.2376,  0.1331,  ..., -1.3488, -0.2151, -1.2545]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embs.to_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMD detected drift? False\n",
      "CVM detected drift? False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KS detected drift? False\n"
     ]
    }
   ],
   "source": [
    "# Iterate and print the name of the detector class and its boolean drift prediction\n",
    "for name, detector in detectors.items():\n",
    "    print(f\"{name} detected drift? {detector.predict(operational_embs).drifted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applies gaussian noise to images before processing\n",
    "noisy_embs = Embeddings(operational_ds, batch_size=64, model=embedding_net, transforms=GaussianNoise(), cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MMD detected drift? True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CVM detected drift? True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KS detected drift? True\n"
     ]
    }
   ],
   "source": [
    "# Iterate and print the name of the detector class and its boolean drift prediction\n",
    "for name, detector in detectors.items():\n",
    "    print(f\"{name} detected drift? {detector.predict(noisy_embs).drifted}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.949856067521638)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The VOC dataset has 20 classes\n",
    "label_parity(Metadata(train_ds).targets.labels, Metadata(operational_ds).targets.labels, num_classes=20).p_value"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}