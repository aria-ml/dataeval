{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitor shifts in operational data\n",
    "\n",
    "This guide provides a beginner friendly introduction on monitoring post deployment data shifts.\n",
    "\n",
    "Estimated time to complete: 5 minutes\n",
    "\n",
    "Relevant ML stages: [Monitoring](../concepts/workflows/ML_Lifecycle.md#monitoring)\n",
    "\n",
    "Relevant personas: Machine Learning Engineer, T&E Engineer\n",
    "\n",
    "## What you'll do\n",
    "\n",
    "- Construct embeddings by training a simple neural network\n",
    "- Compare the embeddings between a training and operational set\n",
    "- Compare the label distributions between a training and operational set\n",
    "\n",
    "## What you'll learn\n",
    "\n",
    "- Learn how to analyze embeddings for operational drift\n",
    "- Learn how to analyze label distributions\n",
    "\n",
    "## What you'll need\n",
    "\n",
    "- Knowledge of Python\n",
    "- Beginner knowledge of PyTorch or neural networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Monitoring is a critical step in the [AI/ML lifecycle](../concepts/workflows/ML_Lifecycle.md). When a model is deployed, data can, and generally will, drift from the distribution on which the model was originally trained.\n",
    "One critical step in AI T&E is the detection of changes in the operational distribution so that they may be proactively addressed. While some change might not affect performance, significant deviation is often associated with model degradation.\n",
    "\n",
    "For this tutorial, you will use the popular [2011 VOC](http://host.robots.ox.ac.uk/pascal/VOC/voc2011/index.html) computer vision dataset to detect drift between the image distribution of the `train` split and the `val` split, which will represent an operational dataset in this guide. You will then determine if the labels within these two datasets has high parity, or equivalent label distributions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "You'll begin by importing the necessary libraries for this tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab  # noqa: F401\n",
    "\n",
    "    %pip install -q dataeval\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# Drift\n",
    "from dataeval.detectors.drift import DriftCVM, DriftKS, DriftMMD\n",
    "from dataeval.metrics.bias import label_parity\n",
    "from dataeval.utils.data import Embeddings, Metadata\n",
    "from dataeval.utils.data.datasets import VOCDetection\n",
    "\n",
    "# Set a random seed\n",
    "rng = np.random.default_rng(213)\n",
    "\n",
    "# Set default torch device for notebook\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More on device**\\\n",
    "The device is set above as it will be used in subsequent steps. The device is the piece of hardware where the model, data, and other related objects are stored in memory. If a GPU is available, this notebook will use that hardware rather than the CPU. To force running only on the CPU, change `device` to `\"cpu\"` For more information, see the [PyTorch device page](https://pytorch.org/tutorials/recipes/recipes/changing_default_device.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Constructing Embeddings\n",
    "\n",
    "A common first step in many aspects of data monitoring is reducing images down to a smaller dimension. While this step is not always necessary, it is good practice to use embeddings over raw images to improve\n",
    "the speed and memory efficiency of many workflows without sacrificing downstream performance.\n",
    "\n",
    "In this step, you will use a [pretrained ResNet18 model](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html) to reduce the dimensionality of the VOC dataset.\n",
    "\n",
    "### Define model architecture\n",
    "\n",
    "Below is a simple [PyTorch nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) that wraps the pre-trained ResNet18 referred to above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the embedding network\n",
    "class EmbeddingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Load in pretrained resnet18 model\n",
    "        self.model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        # Add an additional fully connected layer with an embedding dimension of 128\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run input data through the model\"\"\"\n",
    "\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can now be instantiated in the code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_net = EmbeddingNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download VOC dataset\n",
    "\n",
    "With the model created on the device set at the beginning, you will download the train and validation splits of the 2011 VOC Dataset. Afterwards, you will use the defined `custom_batch` function to chunk the data into batches to make the model run more efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pretrained model transformations\n",
    "preprocess = models.ResNet18_Weights.DEFAULT.transforms()\n",
    "\n",
    "# Load the training dataset\n",
    "train_ds = VOCDetection(\"./data\", year=\"2011\", image_set=\"train\", download=False, transform=preprocess)\n",
    "# Load the \"operational\" dataset\n",
    "operational_ds = VOCDetection(\"./data\", year=\"2011\", image_set=\"val\", download=False, transform=preprocess)\n",
    "\n",
    "print(train_ds.info())\n",
    "print(operational_ds.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is good to notice a few points about each dataset:\n",
    "\n",
    "- Number of datapoints\n",
    "- Resize size\n",
    "\n",
    "These two values give an estimate of the memory impact that each dataset has. The following step will modify the resize size by creating model embeddings for each image to reduce this impact.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Embeddings\n",
    "\n",
    "Now it is time to process the datasets through your model. Aggregating the model outputs gives you the embeddings of the data. This will be helpful in determining drift between the training and operational splits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you will call the helper function and create embeddings for both the train and operational splits. The labels will also be saved so they can be used in a later step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training batches and targets\n",
    "train_embs = Embeddings(train_ds, batch_size=64, model=embedding_net).to_tensor()\n",
    "\n",
    "# Create operational batches and targets\n",
    "operational_embs = Embeddings(operational_ds, batch_size=64, model=embedding_net).to_tensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the shape of embeddings is different than before.\n",
    "\n",
    "**Previously**\n",
    "\n",
    "Training shape - (5717, 256)\\\n",
    "Operational shape - (5823, 256)\n",
    "\n",
    "**After embeddings**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_embs.shape)\n",
    "print(operational_embs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reduced shape of both the training and operational datasets will improve the performance of the upcoming drift algorithms without impacting the accuracy of the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Monitor drift\n",
    "\n",
    "In this step, you will be checking for drift between the training embeddings and the operational embeddings from before. If drift is detected, a model trained on this training data should be retrained with new operational data. This can help mitigate performance degradation in a deployed model. Visit our [About Drift](../concepts/Drift.md) page to learn more.\n",
    "\n",
    "### Drift detectors\n",
    "\n",
    "DataEval offers a few drift detectors: {class}`.DriftMMD`, {class}`.DriftCVM`, {class}`.DriftKS`\n",
    "\n",
    "Since each detector outputs a binary decision on whether drift is detected, a **majority vote** will be used to make the determination of drift.\\\n",
    "To learn more about these algorithms, see the [theory behind drift detection](../concepts/Drift.md#theory-behind-drift-detection) concept page.\n",
    "\n",
    "### Fit the detectors\n",
    "\n",
    "Each drift detector needs a reference set that the operational set will be compared against. In the following code, you will set the reference data to the training embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A type alias for all of the drift detectors\n",
    "DriftDetector = DriftMMD | DriftCVM | DriftKS\n",
    "\n",
    "# Create a mapping for the detectors to iterate over\n",
    "detectors: dict[str, DriftDetector] = {\n",
    "    \"MMD\": DriftMMD(train_embs),\n",
    "    \"CVM\": DriftCVM(train_embs),\n",
    "    \"KS\": DriftKS(train_embs),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions\n",
    "\n",
    "Now that the detectors are setup, predictions can be made against the operational embeddings you made earlier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate and print the name of the detector class and its boolean drift prediction\n",
    "for name, detector in detectors.items():\n",
    "    print(f\"{name} detected drift? {detector.predict(operational_embs).drifted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you expect these results?\n",
    "\n",
    "There is no drift detected between the train and operational embeddings because they come from very similar distributions.\\\n",
    "Ideally, your training data and your validation data, which we used as operational, come from the same distribution. This is the purpose of [data splitters](https://scikit-learn.org/stable/api/sklearn.model_selection.html#splitters).\n",
    "\n",
    "So how do we know if the detectors can detect drift?\n",
    "\n",
    "Well, add some random Gaussian noise to the operational embeddings and find out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a normal distribution around the operational embeddings\n",
    "noisy_embs = torch.normal(mean=operational_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate and print the name of the detector class and its boolean drift prediction\n",
    "for name, detector in detectors.items():\n",
    "    print(f\"{name} detected drift? {detector.predict(noisy_embs).drifted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now drift is detected!\n",
    "\n",
    "Adding Gaussian noise was enough to cause a noticeable change in the drift detectors, but this is not always the case. There are many [types of drift](../concepts/Drift.md#formal-definition-and-types-of-drift) that data can and will experience.\n",
    "\n",
    "In this step, you learned how to take your generated embeddings and detect drift between the training and operational image data. While there was no drift originally, you were able to add small perturbations to the data that did affect the data distributions and cause drift.\n",
    "\n",
    "Next you will look at the labels' distributions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Parity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of looking at the images, you can compare the distributions of the labels using a method called [label parity](../concepts/LabelParity.md).\\\n",
    "There is parity between two sets of labels if the label frequencies are approximately equal.\n",
    "\n",
    "You will now compare the label distributions using the `label_parity` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The VOC dataset has 20 classes\n",
    "label_parity(Metadata(train_ds).targets.labels, Metadata(operational_ds).targets.labels, num_classes=20).p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the {class}`.ParityOutput` class, you can see that it calculated a p_value of ~**0.95**. Since this is close to 1.0, it can be said that the two distributions **have** parity, or similar distributions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, you have learned to create embeddings from the VOC dataset, look for drift between two sets of data, and calculate the parity of two label distributions. These are important steps when monitoring data as drift and lack of parity can affect a model's ability to achieve performance recorded during model training. When data drift is detected or the label distributions lack parity, it is a good idea to consider retraining the model and incorporating operational data into the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## What's next\n",
    "\n",
    "DataEval plays a small, but impactful role in data monitoring as a metrics library.\\\n",
    "Visit these additional resources for more information on other aspects:\n",
    "\n",
    "- Read about the entire [monitoring in AI/ML](../concepts/workflows/ML_Lifecycle.md#monitoring) stage\n",
    "- Explore DataEval's [API reference](../reference/autoapi/dataeval/index.rst) for drift and other monitoring tools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
