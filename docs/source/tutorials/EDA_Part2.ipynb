{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assess an unlabeled data space\n",
    "\n",
    "This guide provides a beginner friendly introduction to exploratory data analysis when labels are not available.\n",
    "\n",
    "Estimated time to complete: 10 minutes\n",
    "\n",
    "Relevant ML stages: [Data Engineering](../concepts/workflows/ML_Lifecycle.md#data-engineering)\n",
    "\n",
    "Relevant personas: Data Engineer, Data Scientist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What you'll do\n",
    "\n",
    "- Construct embeddings by inferencing the PASCAL VOC 2011 through a model\n",
    "- Analyze clustered embeddings to find outliers\n",
    "- Run additional analysis to find gaps in data coverage\n",
    "\n",
    "## What you'll learn\n",
    "\n",
    "- Learn to use cluster based algorithms to measure dataset quality without labels\n",
    "- Learn to distinguish if an image is an outlier or is underrepresented\n",
    "- Learn to determine if flagged images should be added or removed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What you'll need\n",
    "\n",
    "- Basic familiarity with Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Before building predictive models, it's essential to identify outliers and ensure that data groups accurately reflect the underlying distribution.\n",
    "When labels are unavailable do to things like unsupervised learning, initial data curration, or expensive annotations, analysis can still be done on the images alone.\n",
    "We do this by considering images to be geometric points in a high-dimensional abstract space called _feature space_.\n",
    "By understanding how the data examples are distributed in feature space, you will become able to make the assessments you need.\n",
    "Specifically by grouping data points (i.e. images) into clusters, you can explore the natural structure of the dataset to reveal hidden patterns and potential anomalies.\n",
    "Measuring coverage goes a step further by quantifying how well the clusters represent the entire dataset to ensure that no significant portion of the feature space is being overlooked.\n",
    "\n",
    "These techniques are critical for evaluating the quality and representativeness of your data, helping to avoid biases and missing information or overfitting issues in your models.\n",
    "By understanding the space your data occupies and how it groups, you can build more robust and reliable models that generalize well in real-world applications.\n",
    "\n",
    "## Setup\n",
    "\n",
    "You'll begin by importing the necessary libraries for this tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab  # noqa: F401\n",
    "\n",
    "    # specify the version of DataEval (==X.XX.X) for versions other than the latest\n",
    "    %pip install -q dataeval\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision.transforms.v2 as v2\n",
    "\n",
    "# Coverage\n",
    "from dataeval.metrics.bias import coverage\n",
    "\n",
    "# Clustering\n",
    "from dataeval.metrics.estimators import clusterer\n",
    "\n",
    "# Dataset and model\n",
    "from dataeval.utils.data import Embeddings\n",
    "from dataeval.utils.data.datasets import VOCDetectionTorch\n",
    "from dataeval.utils.torch.models import ResNet18\n",
    "\n",
    "# Set default torch device for notebook\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "\n",
    "The device is the piece of hardware where the model, data, and other related objects are stored in memory. If a GPU is available, this notebook will use that hardware rather than the CPU. To force running only on the CPU, change `device` to `\"cpu\"` For more information, see the [PyTorch device page](https://pytorch.org/tutorials/recipes/recipes/changing_default_device.html).\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing Embeddings\n",
    "\n",
    "An important concept in many aspects of machine learning is {term}`Dimensionality Reduction`. While this step is not always necessary, it is good practice to use embeddings over raw images to improve the speed and memory efficiency of many workflows without sacrificing downstream performance.\n",
    "\n",
    "In this section, you will use a [pretrained ResNet18 model](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html) to reduce the dimensionality of the VOC dataset.\n",
    "\n",
    "### Define model architecture\n",
    "\n",
    "In the cell below, you will instantiate a wrapper class for the ResNet18 model that automatically sets common parameters. Since the model has many layers, visit the {class}`.ResNet18` reference page for more information on its architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = ResNet18()\n",
    "\n",
    "# Uncomment the line below to see the model architecture in detail\n",
    "# print(resnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download VOC dataset\n",
    "\n",
    "Now you will download the train split using the {class}`.VOCDetection` class. The train split contains 5717 images with a varying number of targets in each image. This makes it useful for determining gaps in coverage.\n",
    "\n",
    "To run this tutorial with your own dataset, swap dataset with a [torch.utils.data.Dataset](https://pytorch.org/vision/stable/datasets.html) subclass and remove the `print(dataset.info())` call below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training dataset using model specific transforms\n",
    "transforms = v2.Compose([v2.Resize((256, 256)), v2.ToDtype(torch.float32, scale=True)])\n",
    "\n",
    "dataset = VOCDetectionTorch(root=\"./data\", year=\"2011\", image_set=\"train\", download=True, transforms=transforms)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is good to notice a few points about each dataset:\n",
    "\n",
    "- Number of datapoints\n",
    "- Resize size\n",
    "\n",
    "These two values give an estimate of the memory impact that the dataset has. The following step will modify the resize size by creating model embeddings for each image to reduce this impact.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract embeddings\n",
    "\n",
    "Now it is time to process the datasets through your model. Aggregating the model outputs gives you the embeddings of the data. This will be helpful in determining clusters that are representative of your dataset\n",
    "\n",
    "The {func}`.batch_voc` function is specifically designed to handle specific behaviors when using a PyTorch Dataloader on the VOC dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings from the dataset using the ResNet18 model\n",
    "embeddings = Embeddings(dataset=dataset, batch_size=64, model=resnet).to_tensor()\n",
    "\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape has been reduced from (256, 256) to (128,). This will greatly improve the performance of the upcoming algorithms without impacting the accuracy of the results. Notice that the original number of images, 5717, is the same as before you created image embeddings.\n",
    "\n",
    "Embeddings create a more performant representation of the data than simply resizing but are more computational expensive. In most cases, this step is not the bottleneck of a pipeline and is generally considered worth the improvement in performance of downstream tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster the Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will use the embeddings you generated to create clusters. A cluster is a group of images that have similar characteristics. Based on this information, images can be found that are either too similar or too dissimilar. Both cases can lead to performance degradation of downstream tasks such as data analysis or model training.\n",
    "\n",
    "### Normalize embeddings\n",
    "\n",
    "Before clustering can be done, the embeddings need to be normalized between 0 and 1 for the {func}`.clusterer` to properly group them. Let's look at the current spread of the embeddings values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Max value: {embeddings.max()}\")\n",
    "print(f\"Min value: {embeddings.min()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are certainly not between 0 and 1 but the next step will fix that by doing min-max normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Puts the embeddings onto the unit interval [0-1]\n",
    "normalized_embs = (embeddings - embeddings.min()) / (embeddings.max() - embeddings.min())\n",
    "\n",
    "print(f\"Max value: {normalized_embs.max()}\")\n",
    "print(f\"Min value: {normalized_embs.min()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your embeddings are between 0 and 1!\n",
    "\n",
    "### Create clusters\n",
    "\n",
    "The data is now ready to be clustered. Run the normalized embeddings through the {func}`.clusterer` function to generate the {class}`.ClustererOutput` class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = clusterer(normalized_embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have successfully clustered your image embeddings! The `ClustererOutput` contains several attributes related to clusters such as the minimum spanning tree and linkage array. For this tutorial, you can focus on the {meth}`.ClustererOutput.find_outliers` method to analyze the quality of the dataset. For additional use cases, additional benefits, and even alternative methods, read more [about clustering](../concepts/Clustering.md) in DataEval's documentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the outputs\n",
    "\n",
    "While generating the clusters, individual samples were flagged by the {func}`.clusterer` for potentially being outliers or duplicates. In this section, you will look at the flagged images and decide on corrective measures to improve the quality of the data.\n",
    "\n",
    "<!-- This is now deprecated -->\n",
    "\n",
    "Let's take a quick look at the categories of flagged images:\n",
    "\n",
    "- duplicates\n",
    "- outliers\n",
    "\n",
    "Let's take a look at each of these in turn.\n",
    "\n",
    "### Duplicates\n",
    "\n",
    "When evaluating the clusters, if two or more data points are within a certain distances from each other, the index of each data point will be marked as a duplicate.\n",
    "\n",
    "To access the duplicates, the {meth}`.ClustererOutput.find_duplicates` function will return a list of indices it has flagged.\n",
    "\n",
    "For this tutorial, we will not focus on duplicates as no images are flagged in the PASCAL 2011 VOC dataset.\n",
    "\n",
    "### Outliers\n",
    "\n",
    "Outliers are individual data points that did not fit within the radius of any cluster. Potential outliers are outliers who fall within a small distance from the radius and may become part of the cluster if more data is acquired.\n",
    "\n",
    "It is important to identify outliers as they can skew the training distribution away from the operational distribution, which can lead to performance degradation.\n",
    "\n",
    "Let's see how many images were flagged by calling {meth}`.ClustererOutput.find_outliers`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = output.find_outliers()\n",
    "\n",
    "print(f\"Number of outliers: {len(outliers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is a significant number of outliers. A quick calculation of the percent of outliers in the dataset can be done.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Percent of outliers: {(100 * len(outliers) / len(dataset)):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always a good idea to view a few of these images to get a better understand of why they were flagged.\n",
    "\n",
    "#### Plot some samples\n",
    "\n",
    "The details of the plotting function are unimportant to this tutorial but it plots two rows of images with `count` number of images per row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_two_rows(indices: list[int], count=3):\n",
    "    _, axs = plt.subplots(2, count, figsize=(8, 8))\n",
    "\n",
    "    for i, ax in enumerate(axs.flat):\n",
    "        curr_image = dataset[indices[i]][0]\n",
    "        norm_image = (curr_image - curr_image.min()) / (curr_image.max() - curr_image.min())\n",
    "\n",
    "        # Permutes image from torch shape CHW to matplotlib shape HWC\n",
    "        ax.imshow(norm_image.permute(1, 2, 0).numpy(force=True))\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are free to choose which set of images you would like to examine. Take a moment to swap out the different indices when calling the plot function by uncommenting your preferred indices calculation. You can also adjust the `count` parameter if you would like to see more examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To use the flagged outliers, uncomment the line below\n",
    "indices = outliers\n",
    "\n",
    "# To use unflagged images, uncomment the line below\n",
    "# indices = list(set(range(len(dataset))) - set(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run your chosen set of indices through the plot function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_two_rows(indices.tolist(), count=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the flagged images\n",
    "\n",
    "Visualization is a great way to catch initial differences between images in a dataset. However, it is only a preliminary step into identifying the root cause of an outlier. Several initial questions can be asked to ensure outliers are handled properly and represent the underlying distribution of the data.\n",
    "\n",
    "### Initial questions\n",
    "\n",
    "When looking at the above samples, here is a shortlist of questions you should use to determine the appropriate data cleaning step:\n",
    "\n",
    "1. Are there unexpected artifacts in the image?\n",
    "   - **Examples**: Data collection, processing, and visualization can cause discoloration, blurriness, etc\n",
    "   - **Solution**: Ensure images are normalized with the right values, channel order is correct, color scheme is consistent\n",
    "   - See our in-depth [Data Cleaning](EDA_Part1.ipynb) tutorial to explore these types of outliers\n",
    "1. Do you have enough data?\n",
    "   - **Examples**: Less data points in each cluster can lead to higher deviation and more outliers\n",
    "   - **Solution**: Adding more data can help make the clusters more robust to the variations in images\n",
    "1. Do the images provide useful information?\n",
    "   - **Examples**: Image attributes like number of duplicates, pixel intensity, etc can cause the clusterer to become biased to the wrong information\n",
    "   - **Solution**: Determine if the data represents the whole operational distribution\n",
    "   - This solution will be explored in the following section\n",
    "\n",
    "This is only a shortlist of potential causes for outliers based on pixel values. Over time you will add more questions and solutions that suit your specific needs.\n",
    "\n",
    "### Outliers and edge cases\n",
    "\n",
    "The most common way to clean data is to remove the outliers entirely. This provides an easy and efficient way to stabilize the training distribution. However, this can lead to shortages of data, class imbalances, and a loss of potentially useful information. It is also important to remember that outliers and edge cases are different. Edge cases are statistically rare events that _are_ relevant to the downstream task. This is why it is important to carefully look at outliers to ensure they are irrelevant.\n",
    "\n",
    "One way to determine if an image is relevant even when the total distribution considers it an outlier is to measure its coverage.\n",
    "\n",
    "## Measure image coverage\n",
    "\n",
    "{term}`Coverage` is the measurement of the representation of an image's variations in a feature's space. When an image does not have enough variations, it is _underrepresented_. In this section, you will test for any gaps in the coverage of the dataset to find underrepresented images.\n",
    "\n",
    "### Calculate coverage\n",
    "\n",
    "The {func}`.coverage` function will return a list of image indices that it finds to be underrepresented. This means it does not have enough similar images around it. This should sound familiar as outliers have a very similar situation. However, underrepresented images should be handled different than outliers. More on this in the next section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = coverage(embeddings=normalized_embs)\n",
    "\n",
    "print(f\"Number of uncovered images: {len(cov.uncovered_indices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can plot a few images using the same function as before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_two_rows(indices=cov.uncovered_indices.tolist(), count=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "\n",
    "The {class}`.CoverageOutput` class has its own plot functionality that is used when you have an array of images instead of a `torch.utils.data.Dataset`.\n",
    "\n",
    ":::\n",
    "\n",
    "Now you have seen a few examples of images that were flagged as uncovered. You will compare this information with the outliers found earlier to determine which actions should be taken.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncovered_outliers = set(cov.uncovered_indices.tolist()).intersection(set(outliers))\n",
    "\n",
    "print(f\"Number of outliers found as uncovered images: {len(uncovered_outliers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling flagged images\n",
    "\n",
    "You now have enough information to determine what should be done with these images. If they are both flagged as outliers and underrepresented, handle them first as missing coverage and then as outliers if solutions cannot be acted on.\n",
    "\n",
    "### Missing coverage\n",
    "\n",
    "For the images with a gap in coverage, the characteristics of the individual images need additional samples. This can be done by collecting more images of similar information (pose, scene, color, labels if available) or through additional augmentations.\n",
    "\n",
    "### Outliers\n",
    "\n",
    "Outliers can be handled by additional sampling but typically are removed as they either do not contain enough relevant information or would cause a shift in the underlying distribution if similar items were added.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this tutorial you have learned to create image embeddings for more efficient calculations, to use cluster based algorithms to find outliers, to check for gaps in coverage, and to make decisions on possible solutions.\n",
    "\n",
    "Good luck with your data!\n",
    "\n",
    "---\n",
    "\n",
    "## What's next\n",
    "\n",
    "In addition to exploring a dataset in its feature space, DataEval offers additional tutorials on exploratory data analysis:\n",
    "\n",
    "- Clean a dataset with the labels in the [Data Cleaning Guide](EDA_Part1.ipynb)\n",
    "- [Identify Bias and Correlations](EDA_Part3.ipynb) in your metadata\n",
    "\n",
    "Explore deeper explanations on topics such as [clustering](../concepts/Clustering.md), [coverage](../concepts/Coverage.md), and [outliers](../concepts/Outliers.md) in the [Concept pages](../concepts/index.md).\n",
    "\n",
    "## On your own\n",
    "\n",
    "Once you are familiar with DataEval and data analysis, run this analysis on your own dataset.\n",
    "When you do, make sure that you analyze all of your data and not just the training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
