{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessing the Data Space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What you'll do\n",
    "\n",
    "You will use DataEval's `coverage` function and `Clusterer` class to identify coverage gaps and outliers in the 2011 VOC dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What you'll learn\n",
    "\n",
    "- You'll learn how to use DataEval's `coverage` function and `Clusterer` class to assess a dataset's data space for coverage gaps and outliers.\n",
    "- You'll learn about the kinds of questions to ask to help you determine if a data point should be removed or additional data collected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What you'll need\n",
    "\n",
    "- Environment Requirements\n",
    "  - `dataeval[torch]`\n",
    "  - `torchvision`\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Understanding how the data lies within its feature space is a crucial step in evaluating the structure and completeness of your dataset.\n",
    "Before building predictive models, it's essential to assess how well the data is represented, identify any outliers,\n",
    "and ensure that data groups accurately reflect the underlying distribution.\n",
    "\n",
    "DataEval has two dedicated methods for identifying and understand the grouping of data, the `Clusterer` class and the `coverage` function.\n",
    "By grouping data points into clusters, you can explore the natural structure of the dataset, revealing hidden patterns and potential anomalies.\n",
    "The coverage function goes a step further by quantifying how well the clusters represent the entire dataset, ensuring that no significant portion of the feature space is being overlooked.\n",
    "\n",
    "These techniques are critical for evaluating the quality and representativeness of your data, helping to avoid biases, missing information, or overfitting issues in your models.\n",
    "By focusing on understanding the space your data occupies and how it groups, you can build more robust and reliable models that generalize well in real-world applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll begin by installing the necessary libraries to walk through this guide.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab  # noqa: F401\n",
    "\n",
    "    # specify the version of DataEval (==X.XX.X) for versions other than the latest\n",
    "    %pip install -q dataeval[torch]\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will need matplotlib for visualing our dataset and numpy to be able to handle the data.\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# You are importing torch in order to create image embeddings.\n",
    "# You are only using torchvision to load in the dataset.\n",
    "# If you already have the data stored on your computer in a numpy friendly manner,\n",
    "# then feel free to load it directly into numpy arrays.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.v2 as v2\n",
    "from torchvision import datasets, models\n",
    "\n",
    "# Load the classes from DataEval that are helpful for EDA\n",
    "from dataeval.detectors.linters import Clusterer\n",
    "from dataeval.metrics.bias import coverage\n",
    "\n",
    "# Set the random value\n",
    "rng = np.random.default_rng(213)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load the Data\n",
    "\n",
    "You are going to work with the PASCAL VOC 2011 dataset.\n",
    "This dataset is a small curated dataset that was used for a computer vision competition.\n",
    "The images were used for classification, object detection, and segmentation.\n",
    "This dataset was chosen because it has multiple classes and images with a variety of sizes and objects.\n",
    "\n",
    "If this data is already on your computer you can change the file location from `\"./data\"` to wherever the data is stored.\n",
    "Just remember to also change the download value from `True` to `False`.\n",
    "\n",
    "For the sake of ensuring that this tutorial runs quickly on most computers, you are going to analyze only the training set of the data, which is a little under 6000 images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data and then load it as a torch Tensor.\n",
    "to_tensor = v2.ToImage()\n",
    "ds = datasets.VOCDetection(\"./data\", year=\"2011\", image_set=\"train\", download=True, transform=to_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the size of the loaded dataset\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on, verify that the above code cell printed out 5717 for the size of the [dataset](http://host.robots.ox.ac.uk/pascal/VOC/voc2011/dbstats.html).\n",
    "\n",
    "This ensures that everything is working as needed for the tutorial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract Image Embeddings\n",
    "\n",
    "Both the Clusterer class and the coverage function expect 2D arrays or in other words a set of 1D images.\n",
    "To flatten our set of 3D images, you will use a neural network to translate the images into 1D embeddings.\n",
    "This will allow you to flatten the dimensions of the image as well as shrink the size of the 1D array,\n",
    "as the images themselves are too large for the Clusterer to handle efficiently.\n",
    "The Clusterer works best when the feature dimension is around 250 or less.\n",
    "\n",
    "For this guide, you will use a pretrained ResNet18 model and adjust the last layer to be our desired dimension of 128.\n",
    "Also, pretrained torchvision models come with all the necessary information for preprocessing your images correctly for that model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the embedding network\n",
    "class EmbeddingNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple CNN that repurposes a pretrained ResNet18 model by overwriting the last linear layer.\n",
    "    Results in a last layer dimension of 128.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, 128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Initialize the network\n",
    "embedding_net = EmbeddingNet()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embedding_net.to(device)\n",
    "\n",
    "\n",
    "# Extract embeddings\n",
    "def extract_embeddings(dataset, model):\n",
    "    \"\"\"Helper function to stack image embeddings from a model\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    embeddings = torch.empty(size=(0, 128)).to(device)\n",
    "    with torch.no_grad():\n",
    "        images = []\n",
    "        for i, (img, _) in enumerate(dataset):\n",
    "            images.append(img)\n",
    "            if (i + 1) % 64 == 0:\n",
    "                inputs = torch.stack(images, dim=0).to(device)\n",
    "                outputs = model(inputs)\n",
    "                embeddings = torch.vstack((embeddings, outputs))\n",
    "                images = []\n",
    "        inputs = torch.stack(images, dim=0).to(device)\n",
    "        outputs = model(inputs)\n",
    "        embeddings = torch.vstack((embeddings, outputs))\n",
    "    return embeddings.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is defined and initialized, you will reload the dataset with the desired preprocessing for the chosen resnet model.\n",
    "Then you will run the images through the model to get the image embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pretrained model transformations\n",
    "preprocess = models.ResNet18_Weights.DEFAULT.transforms()\n",
    "\n",
    "# Load the dataset\n",
    "dataset = datasets.VOCDetection(\"./data\", year=\"2011\", image_set=\"train\", download=False, transform=preprocess)\n",
    "\n",
    "# Create image embeddings\n",
    "embeddings = extract_embeddings(dataset, embedding_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for the coverage function to work properly, the embeddings have to be on the unit interval (between 0 and 1).\n",
    "Below normalizes the embeddings to ensure they are on the unit interval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize image embeddings\n",
    "norm_embeddings = (embeddings - embeddings.min()) / (embeddings.max() - embeddings.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Cluster the Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the images translated into image embeddings and normalized to be on the unit interval,\n",
    "you can now run the `Clusterer` to create clusters and identify outliers.\n",
    "\n",
    "The `Clusterer` output has 4 keys:\n",
    "\n",
    "- outliers,\n",
    "- potential_outliers,\n",
    "- duplicates,\n",
    "- and near_duplicates.\n",
    "\n",
    "Outliers are the images which did not fit into a cluster.\n",
    "Potential outliers are images which are on the edge of the cluster, but were not far enough away from the cluster to be considered an outlier.\n",
    "These are good images to compare with the outliers in order to get a sense of what was grouped versus what was not.\n",
    "\n",
    "In addition to finding outliers and potential outlierts, the Clusterer class can identify duplicates or near duplicates in the dataset.\n",
    "Duplicates are groups of images which are exactly the same.\n",
    "While near duplicates are images which are not exactly the same but very similar, such as the same scene from a slightly different viewpoint or a slightly cropped version of the same image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell takes about 5-10 minutes to run depending on your hardware\n",
    "\n",
    "# Initialize the Clusterer class (with the embedded images)\n",
    "cluster = Clusterer(norm_embeddings)\n",
    "\n",
    "# Find the outlier images\n",
    "results = cluster.evaluate()\n",
    "\n",
    "# View the number of outliers\n",
    "print(f\"Number of outliers: {len(results.outliers)}\")\n",
    "print(f\"Number of potential outliers: {len(results.potential_outliers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of what the Clusterer class considered an outlier, plot the first 16 images along with their labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot random images from each category\n",
    "fig, axs = plt.subplots(4, 4, figsize=(12, 12))\n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    # Selected image\n",
    "    selected_index = results.outliers[i]\n",
    "\n",
    "    # Grabbing the object names\n",
    "    names = []\n",
    "    objects = ds[selected_index][1][\"annotation\"][\"object\"]\n",
    "    for each in objects:\n",
    "        names.append(each[\"name\"])\n",
    "\n",
    "    # Plot the corresponding image - need to permute to get channels last for matplotlib\n",
    "    ax.imshow(np.moveaxis(ds[selected_index][0].numpy(), 0, -1))\n",
    "    ax.set_title(\"-\".join(set(names)))\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at these images, you want to think about the following questions:\n",
    "\n",
    "- Does this image represent something that would be expected in operation?\n",
    "- Is there commonality to the objects in the images? Such as all the objects are found on the leftside of the images.\n",
    "- Is there commonality to the backgrounds of the images? Such as similar colors, darkness/brightness, places, things (like water or snow).\n",
    "- Is there commonality to the class of objects in the images? Such as a specific pose for person or specific pot color for pottedplant.\n",
    "\n",
    "You want to address these outliers from the `Clusterer` with the questions above in mind to determine if they represent actual outliers or just underrepresented samples.\n",
    "In specific context to the `Clusterer`, you want to focus on these in a class by class manner,\n",
    "so thinking about the person class images only in context of the person class, not the dataset as a whole.\n",
    "\n",
    "A few of the images from above have been analyzed in the context of their classes to help you get an idea of how to process the results. \n",
    "The first two horse images have a horse with water in the background.\n",
    "There are 238 total horse images and only 5 of them have water in the background.\n",
    "So while these images would be operationally relevant if you were trying to detect horses, they are underrepresented in the dataset. \n",
    "The same goes for the third horse image.\n",
    "It is one of 4 images that are a close up picture of a horse standing against a fence or railing.\n",
    "It is most likely flagged as an outlier because it is underrepresented in the dataset. \n",
    "Likewise with the potted plant, there are only about 4 images with a potted plant up against a solid background out of the 289 potted plant images.\n",
    "Likely this is also just an underrepresented image. \n",
    "With the dog image, there are 13 dog images wearing an outfit out of 636 dog images and this is the only one in which the dog is sitting while wearing something, likely an underrepresented image.\n",
    "\n",
    "With the last two people images that you see, the person is mostly occluded in the last one and they are really small and off to the side in the second to last one. \n",
    "With the second to last person image, you have to determine how operationally relevant it is.\n",
    "Are you trying to detect people far away or are you focusing on closer images?\n",
    "Also, what is the scale at which an object is too small for detection?\n",
    "With the last one, it is likely that the image could be dropped unless you will often have occlusion when detecting people.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Measure the dataset coverage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the discussion of the results of the Clusterer class, it was noted that many of the outliers could be underrepresented images in the dataset.\n",
    "To address this concern, you will use the `coverage` function to evaluate how well the dataset covers the feature space.\n",
    "The coverage function identifies data points that are in undercovered regions.\n",
    "\n",
    "The coverage function output has 3 keys:\n",
    "\n",
    "- indices,\n",
    "- radii, and\n",
    "- critical_value.\n",
    "\n",
    "Indices contains an array with all of the data points it considers to be uncovered, listing first the most uncovered data point.\n",
    "Radii is an array that contains the defined radius for each data point.\n",
    "Critical value is the calculated threshold value above which the values in radii are considered uncovered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure the coverage of the embeddings\n",
    "embedding_coverage = coverage(norm_embeddings)\n",
    "\n",
    "print(f\"Number of uncovered data points: {len(embedding_coverage.indices)}\\n{embedding_coverage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the coverage function, there are 57 images which are uncovered.\n",
    "You'll plot the first 16 images along with their labels to see what kind of images are considered uncovered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot random images from each category\n",
    "fig, axs = plt.subplots(4, 4, figsize=(12, 12))\n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    # Selected image\n",
    "    selected_index = embedding_coverage.indices[i]\n",
    "\n",
    "    # Grabbing the object names\n",
    "    names = []\n",
    "    objects = ds[selected_index][1][\"annotation\"][\"object\"]\n",
    "    for each in objects:\n",
    "        names.append(each[\"name\"])\n",
    "\n",
    "    # Plot the corresponding image - need to permute to get channels last for matplotlib\n",
    "    ax.imshow(np.moveaxis(ds[selected_index][0].numpy(), 0, -1))\n",
    "    ax.set_title(\"-\".join(set(names)))\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With coverage, you want to ask yourself the same questions that you did with the outliers above:\n",
    "\n",
    "- Does this image represent something that would be expected in operation?\n",
    "- Is there commonality to the objects in the images? Such as all the objects are found on the leftside of the images.\n",
    "- Is there commonality to the backgrounds of the images? Such as similar colors, darkness/brightness, places, things (like water or snow).\n",
    "- Is there commonality to the class of objects in the images? Such as a specific pose for person or specific pot color for pottedplant.\n",
    "\n",
    "Again, answers to these questions will help you determine if the image is an outlier or an underrepresented region of the dataset.\n",
    "Determining whether an image is an outlier or underrepresented, depends on the task that the data is being used for.\n",
    "The better defined the task, the easier it is to determine whether an image should be thrown out or if additional images of a similar nature need to be collected.\n",
    "\n",
    "Now that both the Clusterer and the coverage function have been run on this dataset, you'll compare the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncovered_outliers = [x for x in embedding_coverage.indices if x in results.outliers]\n",
    "print(\n",
    "    f\"Number of images identified by both functions: {len(uncovered_outliers)} \\\n",
    "        out of {len(embedding_coverage.indices)} possible\"\n",
    ")\n",
    "print(uncovered_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "54 of the 57 uncovered images were also identified by the Clusterer, meaning that they must be analyzed to ensure that they are not outliers first.\n",
    "You can claim the remaining 3 images as uncovered because they were not identified by the Clusterer.\n",
    "Below, you'll plot the 3 images just to get a look at them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncovered_only = [x for x in embedding_coverage.indices if x not in uncovered_outliers]\n",
    "\n",
    "# Plot random images from each category\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    # Selected image\n",
    "    selected_index = uncovered_outliers[i]\n",
    "\n",
    "    # Grabbing the object names\n",
    "    names = []\n",
    "    objects = ds[selected_index][1][\"annotation\"][\"object\"]\n",
    "    for each in objects:\n",
    "        names.append(each[\"name\"])\n",
    "\n",
    "    # Plot the corresponding image - need to permute to get channels last for matplotlib\n",
    "    ax.imshow(np.moveaxis(ds[selected_index][0].numpy(), 0, -1))\n",
    "    ax.set_title(\"-\".join(set(names)))\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether the other 54 images are uncovered or outliers, depend on the task at hand and the answers to the questions above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes the fun part, determining what data points are supposed to be in the data set, what points need to be removed, and whether or not you need to collect more data points for a given class or style of image.\n",
    "\n",
    "The images identified by the `Clusterer` and `coverage` stand out from the other images in some way.\n",
    "DataEval isn't able to tell you exactly why they stand out, but it highlights the images that you need to check.\n",
    "You will want to compare each image with other images in that same class to determine whether it is an under-represented image or an image that contains some error and needs to be removed.\n",
    "\n",
    "As you can see, the DataEval methods are here to help you gain a deep understanding of your dataset and all of it's strengths and limitations.\n",
    "It is designed to help you create representative and reliable datasets.\n",
    "\n",
    "Good luck with your data!\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Next\n",
    "\n",
    "In addition to exploring a dataset in it's feature space, DataEval offers additional tutorials to help you learn about:\n",
    "\n",
    "- cleaning a dataset with the [Data Cleaning Guide](EDA_Part1.ipynb),\n",
    "- identifying bias or other factors in a dataset that may influence model performance with the **Identifying Bias and Correlations Guide**,\n",
    "- and monitoring data for shifts during operation with the [Data Monitoring Guide](Data_Monitoring.ipynb).\n",
    "\n",
    "To learn more about specific functions or classes, see the [Cocept pages](../concepts/index.md).\n",
    "\n",
    "## On your own\n",
    "\n",
    "Once you are familiar with DataEval and data analysis, you will want to run this analysis on your own dataset.\n",
    "When you do, make sure that you analyze all of your data and not just the training set.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
