{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to detect undersampled data subsets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Problem Statement_\n",
    "\n",
    "For most computer vision tasks like **image classification** and **object detection**, we often have a lot of images, but certain subsets of the images can be undersampled, such as label, style within a label, etc. A way to detect this regional sparsity is through coverage analysis.\n",
    "\n",
    "To help with this, DataEval has introduced a {class}`.Coverage` class, that provides a user with example images which have few similar instances within the provided dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _When to use_\n",
    "\n",
    "The `Coverage` class should be used when you have lots of images, but only a small fraction from certain regimes/labels.\n",
    "\n",
    "### _What you will need_\n",
    "\n",
    "1. Image classification dataset.\n",
    "2. Autoencoder trained on image classification dataset for dimension reduction (e.g. through the `AETrainer` class).\n",
    "3. A Python environment with the following packages installed:\n",
    "   - `dataeval` or `dataeval[all]`\n",
    "   - `tabulate`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _Setting up_\n",
    "\n",
    "Let's import the required libraries needed to set up a minimal working example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Google Colab Only\n",
    "try:\n",
    "    import base64\n",
    "    import io\n",
    "    import json\n",
    "\n",
    "    import google.colab  # noqa: F401\n",
    "    import torch\n",
    "\n",
    "    # specify the version of DataEval (==X.XX.X) for versions other than the latest\n",
    "    %pip install -q dataeval\n",
    "    !export LC_ALL=\"en_US.UTF-8\"\n",
    "    !export LD_LIBRARY_PATH=\"/usr/lib64-nvidia\"\n",
    "    !export LIBRARY_PATH=\"/usr/local/cuda/lib64/stubs\"\n",
    "    !ldconfig /usr/lib64-nvidia\n",
    "\n",
    "    # Code below is to download the pretrained model weights stored on github\n",
    "    !mkdir models\n",
    "    !curl -o gitlfsbinary https://api.github.com/repos/aria-ml/dataeval/git/blobs/ad520d5589fdc49830f98d28aa5eaed0bbdfe5cb\n",
    "\n",
    "    with open(\"gitlfsbinary\") as f:\n",
    "        rawfile = json.load(f)\n",
    "\n",
    "    binaryfile = base64.b64decode(rawfile[\"content\"])\n",
    "    buffer = io.BytesIO(binaryfile)\n",
    "\n",
    "    temp = torch.load(buffer, weights_only=False)\n",
    "    torch.save(temp, \"models/ae\")\n",
    "\n",
    "    del rawfile\n",
    "    del binaryfile\n",
    "    del buffer\n",
    "    del temp\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "%pip install -q tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from dataeval.metrics.bias import coverage\n",
    "from dataeval.utils.data import Embeddings, Metadata\n",
    "from dataeval.utils.data.datasets import MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "Load the MNIST data and create the training dataset.\n",
    "For the purposes of this example, we will use subsets of the training (2000) data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds\n",
    "torch.manual_seed(14)\n",
    "\n",
    "# MNIST with mean 0 unit variance\n",
    "train_ds = MNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    size=2000,\n",
    "    unit_interval=True,\n",
    "    dtype=np.float32,\n",
    "    channels=\"channels_first\",\n",
    "    normalize=(0.1307, 0.3081),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will use an autoencoder to reduce the dimension of the MNIST images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            # 28 x 28\n",
    "            nn.Conv2d(1, 4, kernel_size=5),\n",
    "            # 4 x 24 x 24\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(4, 8, kernel_size=5),\n",
    "            nn.ReLU(True),\n",
    "            # 8 x 20 x 20 = 3200\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3200, 10),\n",
    "            # 10\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            # 10\n",
    "            nn.Linear(10, 400),\n",
    "            # 400\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(400, 4000),\n",
    "            # 4000\n",
    "            nn.ReLU(True),\n",
    "            nn.Unflatten(1, (10, 20, 20)),\n",
    "            # 10 x 20 x 20\n",
    "            nn.ConvTranspose2d(10, 10, kernel_size=5),\n",
    "            # 24 x 24\n",
    "            nn.ConvTranspose2d(10, 1, kernel_size=5),\n",
    "            # 28 x 28\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "    def encode(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For computational reasons, we will simply load the trained autoencoder. See the how-to [How to create image embeddings with an autoencoder](AETrainerTutorial.ipynb) for more information on how to train an autoencoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The trained autoencoder was trained for 1000 epochs\n",
    "sd = torch.load(\"models/ae\", weights_only=True)\n",
    "model = Autoencoder()\n",
    "model.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the embeddings and extract the labels from the dataset\n",
    "embeddings = Embeddings(train_ds, batch_size=64, model=model).to_tensor()\n",
    "labels = Metadata(train_ds).targets.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize the encodings, we will use TSNE on them to view separation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 10d as 2d with TSNE\n",
    "tsne = TSNE(n_components=2)\n",
    "red_dim = tsne.fit_transform(embeddings.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results with color being label\n",
    "fig, ax = plt.subplots()\n",
    "scatter = ax.scatter(\n",
    "    x=red_dim[:, 0],\n",
    "    y=red_dim[:, 1],\n",
    "    c=labels,\n",
    "    label=labels,\n",
    ")\n",
    "ax.legend(*scatter.legend_elements(), loc=\"upper right\", ncols=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some good separation, but you can see a few images in the \"gaps\". This could be an artifact of dimension reduction, or suggest that we have poor coverage for some covariates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use data adaptive cutoff\n",
    "cvrg = coverage(embeddings, radius_type=\"adaptive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the least covered 0.5%\n",
    "f, axs = plt.subplots(4, 5, figsize=(5, 5))\n",
    "axs = axs.flatten()\n",
    "for count, i in enumerate(axs):\n",
    "    idx = cvrg.uncovered_indices[count]\n",
    "    i.imshow(np.squeeze(train_ds[idx][0]), cmap=\"gray\")\n",
    "    i.set_axis_off()\n",
    "    i.title.set_text(int(labels[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Coverage tool identified that in this set of 2000 images, there is potential under-coverage when it comes to wonky 2s and 7s.\n",
    "Other digits have some undercovered instances, but could be they are just outliers.\n",
    "More investigation into outlier status is needed, see [How to identify outliers and/or anomalies in a dataset](ClustererTutorial.ipynb) for more info.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "### TEST ASSERTION CELL ###\n",
    "wonky = sum(labels[i] == 7 or labels[i] == 2 for idx, i in enumerate(cvrg.uncovered_indices) if idx < 16)\n",
    "assert (wonky / 16) > 0.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
