{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Linting Tutorial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "Exploratory data analysis (EDA) can be overwhelming. There are so many things to check.\n",
    "Duplicates in your dataset, bad/corrupted images in the set, blurred or bright/dark images, the list goes on.\n",
    "\n",
    "DataEval created a Linting class to assist you with your EDA so you can start training your models on high quality data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _When to use_\n",
    "\n",
    "The Linting class should be used during the initial EDA process or if you are trying to verify that you have the right data in your dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _What you will need_\n",
    "\n",
    "1. A dataset to analyze\n",
    "2. A Python environment with the following packages installed:\n",
    "   - `dataeval` or `dataeval[all]`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Getting Started_\n",
    "\n",
    "Let's import the required libraries needed to set up a minimal working example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab  # noqa: F401\n",
    "\n",
    "    # specify the version of DataEval (==X.XX.X) for versions other than the latest\n",
    "    %pip install -q dataeval\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms.v2 as v2\n",
    "\n",
    "from dataeval.detectors.linters import Outliers\n",
    "from dataeval.utils.data.datasets import CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in the data\n",
    "\n",
    "We are going to start by loading in torchvision's CIFAR-10 dataset.\n",
    "\n",
    "The CIFAR-10 dataset contains 60,000 images - 50,000 in the train set and 10,000 in the test set.\n",
    "For the purposes of this demonstration, we are just going to use the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the cifar-10 dataset from torchvision\n",
    "to_tensor = v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])\n",
    "testing_dataset = CIFAR10(\"./data\", train=False, download=True, transform=to_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linting the Dataset\n",
    "\n",
    "Now we can begin finding those images which are significantly different from the rest of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Duplicates class\n",
    "outliers = Outliers(outlier_method=\"zscore\", outlier_threshold=3)\n",
    "\n",
    "# Evaluate the data\n",
    "results = outliers.evaluate(datum[0] for datum in testing_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are a dictionary with the keys being the image that has an issue in one of the listed properties below:\n",
    "\n",
    "- Brightness\n",
    "- Blurriness\n",
    "- Missing\n",
    "- Zero\n",
    "- Width\n",
    "- Height\n",
    "- Size\n",
    "- Aspect Ratio\n",
    "- Channels\n",
    "- Depth\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of images with an issue: {len(results.issues)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a count of issues by type\n",
    "issue_count_by_type = {}\n",
    "for index, issue in results.issues.items():\n",
    "    for k, v in issue.items():\n",
    "        issue_count_by_type[k] = issue_count_by_type.setdefault(k, 0) + 1\n",
    "for issue, count in issue_count_by_type.items():\n",
    "    print(f\"{issue:>10}: {count:<5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "### TEST ASSERTION CELL ###\n",
    "assert len(results.issues) == 501\n",
    "assert {\n",
    "    \"var\",\n",
    "    \"mean\",\n",
    "    \"skew\",\n",
    "    \"kurtosis\",\n",
    "    \"entropy\",\n",
    "    \"brightness\",\n",
    "    \"contrast\",\n",
    "    \"zeros\",\n",
    "    \"sharpness\",\n",
    "    \"std\",\n",
    "    \"darkness\",\n",
    "} == set(issue_count_by_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
